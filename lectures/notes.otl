!preamble \input std-macros.tex
!preamble \input macros.tex
!preamble \usepackage[round]{natbib}
!escape false
!format T
CS229T/STAT231: Statistical Learning Theory (Winter 2016)
{\normalsize Percy Liang}
!ruby '\begin{center} Last updated ' + Time.new.strftime("%a %b %d %Y %H:%M") + '\end{center}'

!format N
These lecture notes will be updated periodically as the course goes on.
 The Appendix describes the basic notation, definitions, and theorems.

!verbatim \tableofcontents

!format SSI
!verbatim \lecture{1}
Overview
	What is this course about? \currlecture
		Machine learning has become an indispensible part of many application areas,
		 in both science (biology, neuroscience, psychology, astronomy, etc.)
		 and engineering (natural language processing, computer vision, robotics, etc.).
		 But machine learning is not a single approach;
		 rather, it consists of a dazzling array of
		 seemingly disparate frameworks and paradigms spanning
		 classification, regression, clustering, matrix factorization,
		 Bayesian networks, Markov random fields, etc.
		 This course aims to uncover the common \word{statistical principles}
		 underlying this diverse array of techniques.
		This class is about the theoretical analysis of learning algorithms.
		 Many of the analysis techniques introduced in this class---which involve
		 a beautiful blend of probability, linear algebra, and optimization---are
		 worth studying in their own right and are useful outside machine learning.
		 For example, we will provide generic tools to bound the supremum of stochastic processes.
		 We will show how to optimize an arbitrary sequence of convex functions
		 and do as well on average compared to an expert that sees all the functions in advance.
		Meanwhile, the practitioner of machine learning is hunkered down trying to get things to work.
		 Suppose we want to build a classifier to predict the topic of a document
		 (e.g., sports, politics, technology, etc.).
		 We train a logistic regression with bag-of-words features and obtain
		 8\% training error on 1000 training documents,
		 test error is 13\% on 1000 documents.
		 There are many questions we could ask that could help us move forward.
			How reliable are these numbers?
			 If we reshuffled the data, would we get the same answer?
			How much should we expect the test error to change
			 if we double the number of examples?
			What if we double the number of features?
			 What if our features or parameters are sparse?
			What if we double the regularization?
			 Maybe use $L_1$ regularization?
			Should we change the model and use an SVM with a polynomial kernel or a neural network?
		 In this class, we develop tools to tackle some of these questions.
		 Our goal isn't to give precise quantitative answers
		 (just like analyses of algorithms doesn't tell you how
		 exactly many hours a particular algorithm will run).
		 Rather, the analyses will reveal the relevant quantities
		 (e.g., dimension, regularization strength, number of training
		 examples), and reveal how they influence the final test error.
		While a deeper theoretical understanding can offer a new perspective
		 and can aid in troubleshooting existing algorithms,
		 it can also suggest new algorithms which might have been non-obvious without
		 the conceptual scaffolding that theory provides.
			A famous example is boosting.
			 The following question was posed by Kearns and Valiant in the late 1980s:
			 is it possible to combine weak classifiers (that get 51\% accuracy)
			 into a strong classifier (that get 99\% accuracy)?
			 This theoretical challenge eventually led to the development of AdaBoost in the mid 1990s,
			 a simple and practical algorithm with strong theoretical guarantees.
			In a more recent example, Google's latest
			 \href{http://arxiv.org/pdf/1409.4842v1.pdf}{22-layer convolutional neural network}
			 that won the 2014 ImageNet Visual Recognition Challenge
			 was initially inspired by
			 \href{http://jmlr.org/proceedings/papers/v32/arora14.pdf}{a theoretically-motivated algorithm
			 for learning deep neural networks with sparsity structure}.
		 There is obviously a large gap between theory and practice;
		 theory relies on assumptions can be simultaneously too strong
		 (e.g., data are i.i.d.) and too weak (e.g., any distribution).
		 The philosophy of this class is that the the purpose of theory here not
		 to churn out formulas that you simply plug numbers into.
		 Rather, theory should \emph{change the way you think}.
		!comment Error decomposition
			How do we even begin to analyze or even formalize the machine learning enterprise,
			 which seems to have so many moving parts?
			It will be conceptually useful to break the error of the classifier
			 (more generally, predictor\footnote{Note that in statistics, \emph{predictor} is synonymous with \emph{feature}.
			 In this class, we use \emph{predictor} in the machine learning sense.})
			 returned by the learning algorithm into two parts:
			 \word{approximation error} and \word{estimation error}:
			 \Fig{figures.slides/errorDecomp}{0.4}{errorDecomp}{Cartoon showing error decomposition into approximation and estimation errors.}
			 A learning algorithm can be viewed as operating on a \word{hypothesis class} $\sH$,
			 which governs what kind of predictions it can make.
			Approximation error is error made by the best predictor in $\sH$.
			 If the approximation error is large, then the hypothesis class is probably too small.
			Estimation error is the difference between the error of the learner and the error of the best predictor.
			 If the estimation error is large, then the hypothesis class is probably too large
			 for the given amount of data.
			We should strive to balance these two errors so that the sum is minimized.
			 Of course, this is not a one-dimensional problem, for there is a lot of wiggle room in choosing $\sH$
			 based on knowledge about the learning task at hand.
		This class is structured into four sections:
		 asymptotics, uniform convergence, kernel methods, and online learning.
		 We will move from very strong assumptions (assuming the data are Gaussian, in asymptotics)
		 to very weak assumptions (assuming the data can be generated by an adversary, in online learning).
		 Kernel methods is a bit of an outlier in this regard;
		 it is more about representational power rather than statistical learning.
	Asymptotics \currlecture
		The setting is thus: Given data drawn based on some unknown parameter vector $\theta^*$,
		 we compute a parameter estimate $\hat\theta$ from the data.
		 How close is $\hat\theta$ to $\theta^*$?
		 \Fig{figures.slides/asymptoticNormality}{0.4}{asymptoticNormality}{
		 In asymptotic analysis, we study how a parameter estimate $\hat\theta$
		 behaves when it is close to the true parameters $\theta^*$.}
		For simple models such as Gaussian mean estimation and fixed design linear regression,
		 we can compute $\hat\theta - \theta^*$ in closed form.
		For most models, e.g., logistic regression, we can't.
		 But we can appeal to asymptotics, a common tool used in statistics \citep{vaart98asymptotic}.
		 The basic idea is to take Taylor expansions
		 and show \word{asymptotic normality}:
		 that is, the distribution of $\sqrt{n}(\hat\theta - \theta^*)$
		 tends to a Gaussian as the number of examples $n$ grows.
		 The appeal of asymptotics is that we can get this simple result
		 even if $\hat\theta$ is very complicated!
		Most of our analyses will be done with maximum likelihood estimators,
		 which have nice statistical properties
		 (they have the smallest possible asymptotic variance out of all estimators).
		 But maximum likelihood is computationally intractable for most latent-variable models
		 and requires non-convex optimization.
		 These optimization problems
		 are typically solved by EM, which is only guaranteed to converge to local optima.
		 We will show how the \word{method of moments},
		 an old approach to parameter estimation dating back to \citet{pearson1894contributions}
		 can be brought to bear on this problem,
		 yielding efficient algorithms that produce globally optimal solutions
		 \citep{anandkumar12moments}.
	Uniform convergence \currlecture
		Asymptotics provides a nice first cut analysis,
		 and is adequate for many scenarios,
		 but it has two drawbacks:
			It requires a smoothness assumption, which means you can't analyze the hinge loss or $L_1$ regularization.
			It doesn't tell you how large $n$ has to be before the asymptotics kick in.
		 \Fig{figures.slides/uniformConvergence}{0.5}{uniformConvergence}{
		 We want to minimize the expected risk $L$ to get $h^*$,
		 but we can only minimize the empirical risk $\hat L$ to get $\hat h$.}
		Uniform convergence provides another perspective on the problem.
		 To start, consider standard supervised learning:
		 given a training set of input-output $(x,y)$ pairs,
		 the learning algorithm chooses a predictor $h : \sX \to \sY$
		 from a hypothesis class $\sH$ and we evaluate it based on unseen test data.
		 Here's a simple question:
		 how do the training error $\hat L(h)$ and test error $L(h)$
		 relate to each other?
		For a fixed $h \in \sH$,
		 the training error $\hat L(h)$ is an average of i.i.d.~random variables
		 (loss on each example),
		 which converges to test error $L(h)$ at a rate governed by
		 Hoeffding's inequality or the central limit theorem.
		But the point in learning is that we're supposed to choose a hypothesis
		 based on the training data, not simply used a fixed $h$.
		 Specifically, consider the \word{empirical risk minimizer} (ERM),
		 which minimizes the training error:
		 \eqn{\hERM \in \arg\min_{h \in \sH} \hat L(h).}
		 Now what can we say about the relationship between $\hat L(\hERM)$ and $L(\hERM)$?
		 The key point is that $\hERM$ depends on $\hat L$,
		 so $\hat L(\hERM)$ is no longer an average of i.i.d.~random variables;
		 in fact, it's quite a narly beast.
		 Intuitively, the training error should be smaller than the test error
		 and hence less reliable, but how do we formalize this?
		 Using \word{uniform convergence}, we will show that:
		 \eqn{\underbrace{L(\hERM)}_\text{test error}
		 \le \underbrace{\hat L(\hERM)}_\text{training error} + \, O_p\p{\sqrt{\frac{\Complexity(\sH)}{n}}}.}
		The rate of convergence is governed by the complexity of $\sH$.
		 We will devote a good deal of time computing the complexity of various function classes $\sH$.
		 VC dimension, covering numbers, and Rademacher complexity are different ways of measuring
		 how big a set of functions is.
		 For example, we will see that if $\sH$ contains $d$-dimensional linear classifiers
		 that are $s$-sparse,
		 then $\Complexity(\sH) = O(s \log d)$,
		 which means we can tolerate exponentially more irrelevant features!
		 All of these results are \word{distribution-free},
		 makes no assumptions on the underlying data-generating distribution!
		These generalization bounds are in some sense the heart of statistical learning theory.
		 But along the way, we will develop generally useful \word{concentration inequalities}
		 whose applicability extends beyond machine learning
		 (e.g., showing convergence of eigenvalues in random matrix theory).
	Kernel methods \currlecture
		Let us take a detour away from analyzing the error of learning algorithms
		 and thinking more about what models we should be learning.
		 Real-world data is complex, so we need expressive models.
		 Kernels provide a rigorous mathematical framework to build complex, non-linear models
		 based on the machinery of linear models.
		For concreteness, suppose we're trying to solve a regression task:
		 predicting $y \in \R$ from $x \in \sX$.
		The usual way of approaching machine learning is to define functions via a
		 linear combination of features: $f(x) = w \cdot \phi(x)$.
		 Richer features yield richer functions:
			$\phi(x) = [1, x]$ yields linear functions
			$\phi(x) = [1, x, x^2]$ yields quadratic functions
		Kernels provide another way to define functions.
		 We define a \word{positive semidefinite kernel} $k(x,x')$,
		 which captures the ``similarity'' between $x$ and $x'$,
		 and the define a function by comparing with a set of exemplars:
		 $f(x) = \sum_{i=1}^n \alpha_i k(x_i, x)$.
			Kernels allow us to construct complex non-linear functions (e.g.,
			 Gaussian kernel $k(x,x') = \exp(\frac{-\|x-x'\|^2}{2\sigma^2})$)
			 that are \word{universal}, in that it can approximate \emph{any} continuous function.
			For strings, trees, graphs, etc.,
			 can define kernels that exploit dynamic programming for efficient computation.
		Finally, we operate on the functions themselves, which is at the end of the day
		 all that matters.
		 We will define a space of functions called an \word{reproducing kernel Hilbert space} (RKHS),
		 which allows us to treat the functions like vectors and perform linear algebra.
		It turns out that all three perspectives are getting at the same thing:
		 \Fig{figures.slides/kernelViews}{0.4}{kernelViewsPreview}{
		 The three key mathematical concepts in kernel methods.}
		There are in fact many feature maps that yield the same kernel (and thus RKHS),
		 so one can choose the one based on computational convenience.
		 Kernel methods generally require $O(n^2)$ time ($n$ is the number of training points)
		 to even compute the kernel matrix.
		 Approximations based on sampling offer efficient solutions.
		 For example, by generating lots of random features of the form $\cos(\omega \cdot x + b)$,
		 we can approximate any shift-invariant kernel.
		 Random features also provides some intuition into why neural networks work.
	Online learning \currlecture
		The world is a dynamic place,
		 something that's not captured well by our earlier analyses based
		 on asymptotics and uniform convergence.
		 Online learning tries to do address this in two ways:
			So far, in order to analyze the error of a learning algorithm,
			 we had to assume that the training examples were i.i.d.
			 However in practice, data points might be dependent, and worse,
			 they might be generated by an adversary (think spam classification).
			In addition, we have so far considered the batch learning setting,
			 where we get a training set, learn a model, and then deploy that model.
			 But in practice, data might be arriving in a stream,
			 and we might want to interleave learning and prediction.
		The online learning setting is a game between a learner and nature:
		 \Fig{figures.slides/onlineLearningGame}{0.3}{onlineLearningGame}{Online learning game.}
			Iterate $t = 1, \dots, T$:
				Learner receives input $x_t \in \sX$ (e.g., system gets email)
				Learner outputs prediction $p_t \in \sY$ (e.g., system labels email)
				Learner receives true label $y_t \in \sY$ (e.g., user relabels email if necessary)
				(Learner updates model parameters)
		How do we evaluate?
			Loss function: $\ell(y_t, p_t) \in \R$ (e.g., 1 if wrong, 0 if correct)
			Let $\sH$ be a set of \emph{fixed} expert predictors (e.g., heuristic rules)
			\word{Regret}: cumulative difference between learner's loss and best fixed expert's loss:
			 in other words, how well did the learner do compared
			 to if we had seen all the data and chose the best single expert in advance?
		Our primary aim is to upper bound the regret.
		 For example, for finite $\sH$, we will show:
		 \eqn{\Regret \le O(\sqrt{T \log |\sH|}).}
		 Things to note:
			The average regret $\frac{\Regret}{T}$ goes to zero
			 (in the long run, we're doing as well as the best fixed expert).
			The bound only depends logarithmically on the number of experts,
			 which means that the number of experts $|\sH|$
			 can be exponentially larger than the number of data points $T$.
			There is no probability here; the data is generated adversarially.
			 What we rely on is the (possibly hidden) \word{convexity} in the problem.
		Online learning naturally leads to the \word{multi-armed bandit setting},
		 where the learner only receives partial feedback.
		 Suppose you're trying to assign treatments $p_t$ to patients $x_t$.
		 In this case, you only observe the outcome associated with treatment $p_t$,
		 not the other treatments you could have chosen (in the standard online learning setting,
		 you receive the true label and hence feedback about every possible output).
		 Online learning techniques can be adapted to work in the multi-armed bandit setting.
		Overall, online learning has many things going for it:
		 it leads to many simple algorithms (online gradient descent)
		 which are used heavily in practice.  At the same time, the
		 theory is remarkably clean and results are quite tight.
		!comment Ideas
			\textbf{Online convex optimization}:
			 The theoretical foundation for online learning is \word{convexity}.
			 We will consider a more general framework known as online convex optimization.
			 %\word{Convexity}: much of the theory works only for convex objectives.  Properties like Lipschitz constants and strong convexity constants come into the bounds.
			 In this framework, we will develop learners which are 
			 simple and scalable (sub)gradient descent algorithms of a weight vector $w_t \in \R^d$, for example:
			 \eqn{w_{t+1} \leftarrow w_t - \eta \nabla \ell(y_t, w_t \cdot x_t),}
			 where $\eta$ is the step size and $\nabla \ell$ is the gradient of the loss with respect to $w_t$.
			\textbf{Online-to-batch conversion}:
			 in batch learning, we care about generalization ability of an algorithm.
				Learner takes in $n$ i.i.d.\ training examples $(x,y)$ and outputs a predictor $h$.
				Evaluate $h$ on test examples drawn from the same distribution.
			 While the online and batch scenarios on the surface look different,
			 they are closely connected.
			 In fact, we can take an online learner with \textbf{low regret} and convert it into a
			 batch learner that has \textbf{low expected risk} and vice-versa.
			\textbf{Multi-armed bandits}:
			 here, the learner does not see what the true label is,
			 but only sees the loss (this is partial feedback).
				Bandits have many applications such as advertisement placement and packet routing.
				Need to deal with exploration/exploitation, but not with state
				 (so easier than reinforcement learning).
	!comment
		In this course, we will assume knowledge of the following
		 (you don't need to remember the deep theorems,
		 just be able to work proficiently with the concepts):
			Linear algebra: eigendecomposition, pseudoinverses, matrix derivatives, etc.
			Probability/statistics: conditional expectation, variance, conditional independence, common distributions
			 (Gaussian, Dirichlet, etc.)
			Convex optimization: convex functions, duality
			Machine learning: Perceptron algorithm, SVM, Naive Bayes, logistic regression, least squares regression, HMM, EM
		 Perhaps most importantly, having mathematical maturity and being able to do proofs is key.
		 If you have a strong mathematical foundation but don't much about machine learning,
		 you will probably be fine technically in the class (after getting pass some jargon),
		 but you might not be able to fully appreciate the significance of the results.

Asymptotics
	Overview \currlecture
		Here's the basic question:
		 Suppose we have data points drawn from an unknown distribution with parameters $\theta^*$:
		 \eqn{x^{(1)}, \dots, x^{(n)} \sim p_{\theta^*}.}
		 Can we come up with an estimate $\hat\theta$ (some function of the data)
		 that gets close to $\theta^*$?
		We begin with perhaps the simplest statistical problem:~estimating the mean
		 of a Gaussian distribution (\refsec{gaussianMeanEstimation}).
		 This allows us to introduce the concept of parameter error,
		 and provides intuition for how this error scales with dimensionality and number of examples.
		 We then consider estimating multinomial distributions (\refsec{multinomialMeanEstimation}),
		 where we get our first taste of asymptotics in order to cope with the non-Gaussianity,
		 appealing to the central limit theorem and the continuous mapping theorem.
		 Finally, we generalize to exponential families (\refsec{exponentialFamilies}),
		 a hugely important class of models.
		 Here, our parameter estimate is no longer an empirical average,
		 so we lean on the delta method.
		We then introduce the method of moments,
		 and show an equivalence with exponential families via the maximum entropy principle (\refsec{maximumEntropy}).
		 But the real benefit of method of moments is in latent-variable models (\refsec{mixtureModels}).
		 Here, we show that we can obtain an efficient estimator for three-view mixture models
		 when maximum likelihood would otherwise require non-convex optimization.
		Next, we turn from parameter estimation to prediction,
		 starting with fixed design linear regression (\refsec{fixedDesignLinearRegression}).
		 The analysis is largely similar to mean estimation,
		 and we get exact closed form expressions.
		 For the random design setting and general loss functions (to handle logistic regression),
		 we need to again appeal to asymptotics (\refsec{fullAsymptotics}).
		So far, we've only considered unregularized estimators,
		 which make sense when the number of data points is large relative to the model complexity.
		 Otherwise, regularization is necessary to prevent overfitting.
		 As a first attempt to study regularization,
		 we analyze the regularized least squares estimator (ridge regression)
		 for fixed design linear regression,
		 and show how to tune the regularization strength
		 to balance the bias-variance tradeoff (\refsec{regularizedLinearRegression}).
	Gaussian mean estimation \currlecture \label{sec:gaussianMeanEstimation}
		We warmup with a simple classic example from statistics.
		 The goal is to estimate the mean of a Gaussian distribution.
		 Suppose we have $n$ i.i.d.~samples from a Gaussian distribution with unknown mean $\theta^* \in \R^d$
		 and known variance $\sigma^2 I$ (so that each of the $d$ components are independent):
		 \eqn{x^{(1)}, \dots, x^{(n)} \sim \sN(\theta^*, \sigma^2 I)}
		Define $\hat\theta$ to be parameter estimate equal to the sample mean:
		 \eqnl{gaussianSampleMean}{\hat\theta = \inv{n} \sum_{i=1}^n x^{(i)}.}
		Let us study how this estimate $\hat\theta$ deviates from the true parameter $\theta$:
		 \eqn{\hat\theta - \theta^* \in \R^d.}
		 This deviation is a random vector which should depend
		 on the number of points $n$ and the variance $\sigma^2$.
		 What do you think the dependence will be?
		\lemmaHeading{gaussianMeanParameterDeviation}{parameter deviation for Gaussian mean}
			The sample mean estimate $\hat\theta$ deviates from the true parameter $\theta^*$
			 according to a Gaussian:
			 \eqn{\hat\theta - \theta^* \sim \sN\p{0, \blue{\frac{\sigma^2 I}{n}}}.}
			FIGURE: [$\hat\theta$ in a ball around $\theta^*$ for $d=2$]
		Proof of \reflem{gaussianMeanParameterDeviation}
			We need the following basic properties of Gaussians:
				Fact 1 (sums of independent Gaussians): if $v_1 \sim \sN(0, \Sigma_1)$ and $v_2 \sim \sN(0, \Sigma_2)$ are independent,
				 then $v_1 + v_2 \sim \sN(0, \Sigma_1 + \Sigma_2)$.
				Fact 2 (constant times Gaussian): if $v \sim \sN(0, \Sigma)$, then $A v \sim \sN(0, A \Sigma A^\top)$ for any matrix $A$.
			The rest is just algebra:
				First, note that $x^{(i)} - \theta^* \sim \sN(0, \sigma^2 I)$.
				Next, we have that $S_n \eqdef \sum_{i=1}^n x^{(i)} \sim \sN(0, n \sigma^2 I)$ by Fact 1.
				Finally, we have $\hat\theta - \theta^* = \frac{S_n}{n} \sim \sN\p{0, \frac{\sigma^2 I}{n}}$ by Fact 2.
		\reflem{gaussianMeanParameterDeviation} tells us how the entire estimated vector behaves,
		 but sometimes it's convenient to just get one number that tells us how we're doing.
		 Let's define the \word{mean-squared parameter error},
		 which is the squared distance between the estimate $\hat\theta$ and the true parameter $\theta^*$:
		 \eqn{\|\hat\theta - \theta^*\|_2^2.}
		 How do you think the parameter error behaves?
		\lemmaHeading{gaussianMeanParameterError}{parameter error for Gaussian mean}
			The mean-squared parameter error is:
			 \eqnl{gaussianMeanParameterErrorOne}{\|\hat\theta - \theta^*\|_2^2 \sim \frac{\sigma^2}{n} \chi^2_d,}
			 and has expected value:
			 \eqnl{gaussianMeanParameterErrorTwo}{\boxed{\E\pb{\|\hat\theta - \theta\|_2^2} = \blue{\frac{d \sigma^2}{n}}.}}
		Proof of \reflem{gaussianMeanParameterError}
			Standard properties of chi-squared distributions:
				If $v_1, \dots, v_d \sim \sN(0, 1)$, then $\sum_{j=1}^d v_j^2 \sim \chi^2_d$.
				If $z \sim \chi^2_d$, then $\E[z] = d$ and $\var[z] = 2d$.
			By \reflem{gaussianMeanParameterDeviation},
			 we know that $v = \sqrt{\frac{n}{\sigma^2}} (\hat\theta - \theta^*) \sim \sN(0, I)$.
			The squared 2-norm of this vector ($\|v\|_2^2 = \sum_{j=1}^d v_j^2$) is therefore distributed as $\chi^2_d$.
			Multiplying both sides by $\frac{\sigma^2}{n}$ yields \refeqn{gaussianMeanParameterErrorOne}.
			Taking expectations on both sides yields \refeqn{gaussianMeanParameterErrorTwo}.
		A Gaussian random vector has fluctuations on the order of its standard deviation,
		 we can think of $\hat\theta$ having typical deviations on the order of $\sqrt{\frac{d \sigma^2}{n}}$.
		FIGURE: [relate to previous figure, density of a chi-squared for $\|\hat\theta - \theta^*\|_2^2$]
		Takeaways
			Life is easy when everything's Gaussian, since we can compute things in closed form.
			Think geometrically about shrinking balls around $\theta^*$.
	Multinomial estimation \currlecture \label{sec:multinomialMeanEstimation}
		In the above, things worked out nicely because everything was Gaussian.
		 What if are data are not Gaussian?
		Suppose we have an unknown multinomial distribution over $d$ choices:
		 $\theta^* \in \Delta_d$
		 (that is, $\theta = [\theta_1, \dots, \theta_d]$, $\theta_j \ge 0$ for each $j$
		 and $\sum_{j=1}^d \theta_j = 1$).
		 Suppose we have $n$ i.i.d.~samples from this unknown multinomial distribution
		 \eqn{x^{(1)}, \dots, x^{(n)} \sim \text{Multinomial}(\theta^*),}
		 where each $x^{(i)} \in \{ e_1, \dots, e_d \}$
		 and $e_j \in \{0,1\}^d$ is an indicator vector that is $1$ at position $j$ and $0$ elsewhere.
		Consider the empirical distribution as the estimator (same form as the sample mean):
		 \eqn{\hat\theta = \inv{n} \sum_{i=1}^n x^{(i)}.}
		Example:
			$\theta^* = [0.2, 0.5, 0.3]$
			$x^{(1)} = [0, 1, 0], x^{(2)} = [1, 0, 0], x^{(3)} = [0, 1, 0]$
			$\hat\theta = [\frac13, \frac23, 0]$
		We can now ask the same question as we did for Gaussian estimation:
		 what is the parameter error $\|\hat\theta - \theta^*\|_2^2$?
		 Before we had gotten a chi-squared distribution because we had Gaussians and could work out
		 everything in closed form.
		 But now, we have multinomial distributions,
		 which makes things more difficult to analyze.
		 How do you think the parameter error will behave?
		Fortunately, not all hope is lost.
		 Our strategy will be to study the \word{asymptotic behavior} of the estimators.
		 By the \textbf{central limit theorem}, we have
		 \eqn{\sqrt{n}(\hat\theta - \theta^*) \cvd \sN(0, V),}
		 where $V \eqdef \diag(\theta^*) - \theta^* (\theta^*)^\top$ is the
		 the asymptotic variance of a single multinomial draw.
		 Written out, the elements of $V$ are:
		 \eqn{V_{jk} = \begin{cases} \theta^*_j (1-\theta^*_j) & \text{if $j = k$} \\ -\theta^*_j \theta^*_k & \text{if $j \neq k$.} \end{cases}}
		 It is easy to check this by noticing that $\E[x_j] = \E[x_j^2] = \theta^*_j$, $\E[x_j x_k] = 0$ for $j \neq k$.
		 Further sanity check:
			For a single component $j$, when $\theta^*_j$ is close to $0$ or $1$, the variance is small.
			The covariance between components $j$ and $k$ is negative since the probabilities sum to $1$ (there is competition among components).
		Next, we can apply the \word{continuous mapping theorem} on the function $\|\cdot\|_2^2$ (see Appendix),
		 which lets us transfer convergence results through continuous functions:
		 \eqn{n \|\hat\theta - \theta^*\|_2^2 \cvd \tr(\sW(V, 1)),}
		 where $\sW(V, k)$ is the Wishart distribution (multivariate generalization of the chi-squared)
		 with mean matrix $V$ and $k$ degrees of freedom.
		 This follows because $z \sim \sN(0, V)$,
		 then $z z^\top \sim \sW(V, 1)$,
		 and $\|z\|_2^2 = \tr(z z^\top)$.
		Taking expectations\footnote{
		 This step actually requires additional justification.
		 In particular, if we have $Y_n \cvd Y$,
		 in order to conclude $\E[Y_n] \cv \E[Y]$,
		 we need $(Y_n)$ to be uniformly integrable.
		 A sufficient condition is that $Y_n$ has finite second-order moments,
		 which means that a data point needs to have finite fourth-order moments.
		 This is clearly the case for the multinomial distribution (which is bounded),
		 and will be true for exponential families (for which all moments exist), discussed in the next section.
		 Showing this result formally is non-trivial and is out of the scope of this class.
		 } and dividing by $n$ on both sides yields:
		 \eqn{\E\pb{\|\hat\theta - \theta^*\|_2^2}
		 &= \p{\sum_{j=1}^d \theta^*_j (1 - \theta^*_j)} \inv{n} + o\p{\frac{1}{n}} \\
		 &\le \boxed{\blue{\inv{n}} + o\p{\frac{1}{n}}.}}
		Note that the $\frac{1}{n}$ term is independent of $d$, unlike in the Gaussian case,
		 where the result was $\frac{d \sigma^2}{n}$.
		 The difference is that in the Gaussian case,
		 each dimension had standard deviation $\sigma$,
		 whereas in the multinomial case here, it is about $\sqrt{\frac{1}{d}}$ for the uniform distribution.
	!verbatim \lecture{2}
	Exponential families \currlecture \label{sec:exponentialFamilies}
		So far, we've analyzed the Gaussian and the multinomial.
		 Now let us delve into exponential families,
		 a much more general family of distributions
		 which include the Gaussian, multinomial, gamma, beta, etc.,
		 but not things like the uniform distribution or mixture models.
		Exponential families are also the basis of undirected graphical models.
		 You probably first saw exponential families in a univariate continuous setting,
		 but we will introduce them here in the multivariable discrete setting
		 in order to build some different intuitions
		 (and to simply the math).
		\definitionHeading{exponentialFamily}{exponential family}
			Let $\sX$ be a discrete set.
				Example: $\sX = \{ -1, +1 \}^3$
			Let $\phi : \sX \to \R^d$ be a feature function.
				Example (unary and binary factors):
				 \eqnl{ising}{\phi(x) = [x_1, x_2, x_3, x_1 x_2, x_2 x_3] \in \R^5.}
			Define a family of distributions $\sP$, where each $p_\theta \in \sP$
			 assigns each $x \in \sX$ a probability that depends on $x$ only through $\phi(x)$:
			 \eqnl{expfamily}{\boxed{\sP \eqdef \{ p_\theta : \theta \in \R^d \}, \quad p_\theta(x) \eqdef \exp\{ \theta \cdot \phi(x) - A(\theta) \},}}
			 where the \word{log-partition function}
			 \eqn{A(\theta) \eqdef \log \sum_{x \in \sX} \exp\{ \theta \cdot \phi(x) \}}
			 ensures the distribution is normalized.
			Example: if $\theta = [0, 0, 0, 0, 0]$, then $p_\theta$ is the uniform distribution.
			Example: if $\theta = [0, 0, 0, \log 2, 0]$ (which favors $x_1$ and $x_2$ having the same sign),
			 then $p_\theta$ assigns probability:
			 \begin{center}
			 \begin{tabular}{lll|ll}
			 $x_1$ & $x_2$ & $x_3$ & $\exp\{\theta \cdot \phi(x)\}$ & $p_\theta(x)$ \\ \hline
			 $-1$ & $-1$ & $-1$ & 2   & 0.2 \\
			 $-1$ & $-1$ & $+1$ & 2   & 0.2 \\
			 $-1$ & $+1$ & $-1$ & 0.5 & 0.05 \\
			 $-1$ & $+1$ & $+1$ & 0.5 & 0.05 \\
			 $+1$ & $-1$ & $-1$ & 0.5 & 0.05 \\
			 $+1$ & $-1$ & $+1$ & 0.5 & 0.05 \\
			 $+1$ & $+1$ & $-1$ & 2   & 0.2 \\
			 $+1$ & $+1$ & $+1$ & 2   & 0.2 \\
			 \end{tabular}
			 \end{center}
			Note that unlike multinomial distributions, $\sP$ does not contain all possible distributions.
			 Varying one parameter $\theta_j$ has the effect of moving all points $x$
			 with the same feature $\phi(x)_j$ the same way.
			FIGURE: [Venn diagram over outcome $x$, indicator features correspond to regions]
		A very useful property of exponential families is that derivatives of the log-partition function correspond
		 to moments of the distribution (a short algebra calculation that you should do once,
		 but we're going to skip):
			The mean is the gradient of the log-partition function:
			 \eqn{\nabla A(\theta) = \E_\theta[\phi(x)] \eqdef \sum_{x \in \sX} p_\theta(x) \phi(x).}
			The covariance is the Hessian of the log-partition function:
			 \eqn{\nabla^2 A(\theta) = \text{Cov}_\theta[\phi(x)] \eqdef \E_\theta[(\phi(x) - \E_\theta[\phi(x)]) (\phi(x) - \E_\theta[\phi(x)])^\top].}
			Note that since $\nabla^2 A(\theta)$ is a covariance matrix, it is necessarily positive semidefinite,
			 which means that $A$ is convex.
			If $\nabla^2 A(\theta) \succ 0$, then $A$ is strongly convex,
			 and $\nabla A$ is invertible (for intuition, consider the 1D case where $\nabla A$ would be an increasing function).
			 In this case, the exponential family is said to be \word{minimal}.
			 For simplicity, we will assume we are working with minimal exponential families henceforth.
			One important consequence of minimality is that there is a one-to-one mapping
			 between the \word{canonical parameters} $\theta$
			 and the \word{mean parameters} $\mu$:
			 \eqn{\theta = (\nabla A)^{-1}(\mu) \quad\quad \mu = \nabla A(\theta).}
			FIGURE: [moment mapping diagram from $\theta$ to $\mu$]
			An example of a non-minimal exponential family is the standard representation of a multinomial distribution:
			 $\sX = \{ \text{A}, \text{B}, \text{C} \}$
			 and $\phi(x) = [\1[x = \text{A}], \1[x = \text{B}], \1[x = \text{C}]]$.
			 There are three parameters but only two actual degrees of freedom in the model family $\sP$.
			 To make this exponential family minimal, just remove one of the features.
		Now let us turn to parameter estimation.
		 Assume as usual that we get $n$ i.i.d.~points drawn from some member of the exponential family with parameters $\theta^*$:
		 \eqn{x^{(1)}, \dots, x^{(n)} \sim p_{\theta^*}.}
		 The classic way to estimate this distribution is \word{maximum likelihood}:
		 \eqn{\hat p = p_{\hat\theta}, \quad \hat\theta = \arg\max_{\theta \in \R^d} \sum_{i=1}^n \log p_\theta(x^{(i)}).}
		 For exponential families,
		 we can write this as succinctly as:
		 \eqnl{expfammle}{\hat\theta = \arg\max_{\theta \in \R^d} \{ \hat\mu \cdot \theta - A(\theta) \},}
		 where
		 \eqnl{expfamhatmu}{\hat\mu \eqdef \inv{n} \sum_{i=1}^n \phi(x^{(i)})}
		 are the sufficient statistics of the data.
		 At the end of the day, estimation in exponential families is still about forming empirical averages.
		 Let's now try to get a closed form expression for $\hat\theta$ as a function of $\hat\mu$.
		 Since $A$ is convex and differentiable,
		 we can differentiate the expression in \refeqn{expfammle} to get the necessary and sufficient conditions for optimality:
		 \eqn{\hat\mu - \nabla A(\hat\theta) = 0.}
		 Inverting, we get that $\hat\theta$ and $\hat\mu$ are canonical and mean parameters:
		 \eqnl{expfammleClosed}{\boxed{\hat\theta = (\nabla A)^{-1}(\hat\mu).}}
		 In other words, the maximum likelihood estimator $\hat\theta$ for exponential families is just a non-linear
		 transformation of the sufficient statistics $\hat\mu$.
		Asymptotic analysis of maximum likelihood in exponential families
			By the definition of $\hat\mu$ \refeqn{expfamhatmu} and the central limit theorem, we have:
			 \eqn{\sqrt{n} (\hat\mu - \mu^*) \cvd \sN(0, \text{Cov}_\theta(\phi(x))),}
			 where $\mu^* = \E_{\theta^*}[\phi(x)]$.
			By the connection between $\hat\theta$ and $\hat\mu$ \refeqn{expfammleClosed} and redefining $f = (\nabla A)^{-1}$,
			 we can rewrite the quantity of interest as:
			 \eqn{\sqrt{n} (\hat\theta - \theta^*) = \sqrt{n} (f(\hat\mu) - f(\mu^*)).}
			 What do we do with the RHS?
			 The \word{delta method} allows us to transfer asymptotic normality results on one quantity
			 ($\hat\mu$ in this case)
			 to a non-linear transformation of another quantity ($f(\hat\mu)$ in this case).
			 The asymptotic variance is just multiplied by a linearization of the non-linear transformation.
			 Specifically:
			 \eqn{\sqrt{n} (f(\hat\mu) - f(\mu^*)) \cvd \sN(0, \nabla f(\mu^*)^\top \text{Cov}_\theta(\phi(x)) \nabla f(\mu^*)).}
			 Conveniently, $\nabla f(\mu) = \nabla^2 A(\mu)^{-1} = \text{Cov}_\theta(\phi(x))^{-1}$ for $\theta = f(\mu)$,
			 so we get that:
			 \eqn{\boxed{\sqrt{n} (\hat\theta - \theta^*) \cvd \sN(0, \text{Cov}_\theta(\phi(x))^{-1}).}}
			Notice that the asymptotic variance is the inverse of the covariance of the features $\phi(x)$.
			 Intuitively, if the features vary more, we can estimate the parameters better
			 (smaller asymptotic variance).
			The parameter error $\|\hat\theta - \theta\|_2^2$ can be computed exactly the same way
			 as for multinomial mean estimation, so we will skip this.
		!comment A major disadvantage with maximum likelihood is that it can be \textbf{computationally inefficient} for certain models.
			For example, consider an exponential family with $d$ binary variables ($\sX = \{ -1, +1 \}^d$).
			Define a graph with vertices $V = \{ 1, \dots, d \}$ and edges $E \subseteq V \times V$.
			Let $\phi(x) \in \R^{|E|}$, where each component corresponds to an edge $(i,j) \in E$
			 with $\phi_{(i,j)}(x) = x_i x_j$.
			 This corresponds to a binary Markov network with structure $(V,E)$.
			The difficulty of working with this exponential family is computing the log-partition function
			 \eqn{A(\theta) = \log \sum_{x \in \sX} \exp(\theta \cdot \phi(x)).}
			 For some special graph structures (chains and trees), this computation takes $O(d)$ time,
			 but in the worst-case, it takes time exponential in $d$.
			Even just computing the loss function $-\log p_\theta(x)$ for a fixed $\theta$
			 involves computing $A(\theta)$.
			 The gradient $\nabla A(\theta) = \E_\theta[\phi(x)]$ involves a similar computation.
		!comment Asymptotic analysis of pseudolikelihood
			Not all is lost, however, since maximum likelihood is not the only way to construct a loss function that converges to the correct parameters.
			 The key idea is to maximize the sum of a set of easily computable conditional likelihoods rather than a single intractable joint likelihood.
			 The \word{maximum pseudolikelihood} estimator corresponds to empirical risk minimization with the following loss function:
			 \eqnl{pseudolikelihood}{\boxed{\ell(x, \theta) = -\sum_{j=1}^d \log p_\theta(x_j \mid x_{-j}),}}
			 where $x_{-j} = (x_1, \dots, x_{j-1}, x_{j+1}, \dots, x_d)$ is all variables but $x_j$.
			 Intuitively, instead of trying to set $\theta$ to explain all of the data $x$ at once,
			 we set $\theta$ to explain each variable $x_j$ given all the other variables,
			 which is an easier problem.
			To make this rigorous, we need to check two things:
				Computational efficiency: does the pseudolikelihood estimator actually easy to compute?
				Statistical consistency: does the pseudolikelihood estimator return the right answer in the limit of infinite data?
			Computational efficiency:
			 We can check that conditional probability takes time at most linear in the number of edges $|E|$.
			 First, define the conditional log-partition function:
			 \eqn{A_j(\theta; x_{-j}) = \log \sum_{x_j \in \{-1,+1\}} \exp(\theta \cdot \phi(x)).}
			 This is analogous to the log-partion function $A(\theta)$; in fact, the summand is the same;
			 the difference is that the $x_{-j}$ part is an argument into the function
			 and $x_j$ is summed over.
			 Also, note that $A_j(\theta; x_{-j})$ is efficient to compute for a fixed $x_{-j}$.
			 Now, we have:
			 \eqn{
			 \log p_\theta(x_j \mid x_{-j})
			 &= \log p_\theta(x) - \log p_\theta(x_{-j}) \\
			 &= [\theta \cdot \phi(x) - A(\theta)] - [A_j(\theta; x_{-j}) - A(\theta)] \\
			 &= \theta \cdot \phi(x) - A_j(\theta; x_{-j}),}
			 which is easy to compute.
			Statistical consistency (and efficiency):
				Note that the loss function is convex in $\theta$,
				 so we just need to check that the expected risk $L(\theta) = \E_{x \sim p_{\theta^*}}[\ell(x, \theta)]$
				 is strictly convex around the true parameters $\theta^*$.
				First, we will check that the gradient is zero at $\theta^*$.
				 Expanding the definition \refeqn{pseudolikelihood} and moving the expectation and gradient operators in, we have:
				 \eqn{\nabla L(\theta)
				 &= \sum_{j=1}^d \E_{p_{\theta^*}(x)}[-\nabla \log p_\theta(x_j \mid x_{-j})] \\
				 &= \sum_{j=1}^d \E_{p_{\theta^*}(x)}[\E_{p_{\theta}(x_j' \mid x_{-j}')}[\phi(x')] - \phi(x)] \\
				 &= \sum_{j=1}^d \E_{p_{\theta^*}(x_{-j}') p_{\theta}(x_j' \mid x_{-j}')}[\phi(x')] - \E_{p_{\theta^*(x)}}[\phi(x)].
				 }
				 If $\theta = \theta^*$, then
				 $p_{\theta^*}(x_{-j}') p_{\theta}(x_j' \mid x_{-j}') = p_{\theta^*}(x)$,
				 so the above equations are equal to the zero vector.
				Second, we compute the asymptotic variance, which determines the statistical efficiency.
				 First, the obligatory derivatives, for reference:
				 \eqn{
				 \nabla \ell(x, \theta) &= \sum_{j=1}^d \p{\E_{p_\theta(x_j \mid x_{-j})}[\phi(x)] - \phi(x)}, \\
				 \nabla^2 \ell(x, \theta) &= \sum_{j=1}^d \text{Cov}_{p_\theta(x_j \mid x_{-j})}[\phi(x)].
				 }
				 Taking expectations of the Hessian, we have:
				 \eqn{\nabla^2 L(\theta) &= \sum_{j=1}^d \underbrace{\E[\text{Cov}_{p_\theta(x_j \mid x_{-j})}[\phi(x)]]}_{S_j}.}
				 Recall from \refeqn{parameterError2} that the asymptotic variance is
				 \eqn{\nabla^2 L(\theta^*)^{-1} \text{Cov}_{\theta^*}(\phi(x)) \nabla^2 L(\theta^*)^{-1},}
				 which is finite when $\nabla^2 L(\theta^*) \succ 0$.
				 Note that for a fixed $j$, $S_j$ will not be positive definite,
				 since when we condition on $x_{-j}$, there is zero variation in features that don't involve $x_j$,
				 and thus we can't learn about them.  This is the price we pay for using pseudolikelihood.
				 However, the requirement is that when we look over all $j$, $\sum_{j=1}^d S_j$ is positive definite,
				 which generally holds.
		Method of moments
			We have introduced $\hat\theta$ as a maximum likelihood estimator.
			 But we can also interpret it as a \word{method of moments} estimator.
			 If we rewrite the optimality conditions for $\hat\theta$ using the fact that $\nabla A(\theta) = \E_\theta[\phi(x)]$,
			 we see that
			 \eqn{\E_{\hat\theta}[\phi(x)] = \hat\mu.}
			 In other words, the empirical moments $\hat\mu$ are identical to those defined by the model $p_{\hat\theta}$.
			The method of moments principle is:
			 Choose model parameters to make the moments under the model match moments of the data.
			The maximum likelihood principle is:
			 Choose model parameters to maximize the likelihood of the entire data.
			The method of moments dates back to Karl Pearson from 1894,
			 which predates the full development of maximum likelihood by Fisher in the 1920s.
			 Since then, maximum likelihood has been the dominant paradigm for parameter estimation,
			 mainly due to its statistical efficiency and naturalness.
			 There is more ad-hocness in the method of moments,
			 but it is exactly this ad-hocness that allows us to get computationally efficient algorithms
			 at the price of reduced statistical efficiency,
			 as we'll see later for latent-variable models.
	Maximum entropy principle \currlecture \label{sec:maximumEntropy}
		Let us scrap exponential families for now
		 and embrace the method of moments perspective for a moment.
		 Can we develop an estimator from first principles?
		Setup
			We are given $n$ data points $x^{(1)}, \dots, x^{(n)}$.
			We have a feature function $\phi : \sX \to \R^d$.
			Define the \word{empirical moments}
			 to be a vector aggregating the feature function over all the $n$ examples:
			 \eqn{\hat\mu \eqdef \inv{n} \sum_{i=1}^n \phi(x^{(i)}).}
		Define $\sQ$ to be the set of distributions with these empirical moments:
		 \eqn{\boxed{\sQ \eqdef \{ q \in \Delta_{|\sX|} : \E_q[\phi(x)] = \hat\mu \},}}
		 where $\E_q[\phi(x)] \eqdef \sum_{x \in \sX} q(x) \phi(x)$.
		Since $|\sX|$ can be extremely large ($2^m$ if $\sX = \{-1, +1\}^m$),
		 there are many possible distributions $q$ but only $d$ constraints,
		 so $\sQ$ contains tons of candidates, and the problem is clearly underconstrained.
		 How do we break ties?
		 Edwin Jaynes introduced the principle of maximum entropy which provides one answer:
		\definitionHeading{maxent}{maximum entropy principle (Jaynes, 1957)}
			Choose the distribution with the empirical moments that has the highest entropy:
			 \eqnl{maxent}{\boxed{\hat q \eqdef \arg\max_{q \in \sQ} H(q),}}
			 where $H(q) \eqdef \E_q[-\log q(x)]$ is the entropy of the distribution $q$.
			 In other words: respect the data (by choosing $q \in \sQ$) but don't commit to more than that
			 (by maximizing the entropy).
		We've now seen two approaches to estimate a distribution $\hat p$ given $n$ data points:
			Maximum likelihood in exponential families
			Maximum entropy subject to moment constraints
		 Which one should we choose?  It turns out that we don't need to because they are equivalent,
		 as the following theorem shows:
		\theoremHeading{maxentDuality}{\word{maximum entropy duality}}
			Assume $\sQ$ is non-empty.
			Then maximum entropy subject to moment constraints is equivalent to maximum likelihood in exponential families:
			 \eqnl{maxentDuality}{\boxed{\arg\max_{q \in \sQ} H(q) = \arg\max_{p \in \sP} \sum_{i=1}^n \log p(x^{(i)}).}}
		Proof of \refthm{maxentDuality}:
			This result is a straightforward application of Langrangian duality.
			 \eqn{\max_{q \in \sQ} H(q)
			 &= \max_{q \in \Delta_{|\sX|}} \min_{\theta \in \R^d} H(q) - \theta \cdot (\hat\mu - \E_q[\phi(x)]).}
			 Since $\sQ$ is non-empty (Slater's condition), we can switch the $\min$ and $\max$.
			 Expanding:
			 \eqn{\min_{\theta \in \R^d} \max_{q \in \Delta_{|\sX|}} -\sum_{x \in \sX} q(x) \log q(x) - \theta \cdot \p{\hat\mu - \sum_{x \in \sX} q(x) \phi(x)}.}
			 Next, differentiate with respect to $q$ and set it to some constant $c$ (because of the sum-to-one constraint):
			 for each $x \in \sX$,
			 \eqn{c = -(1 + \log q(x)) + \theta \cdot \phi(x).}
			 Solving for $q$ (which depends on $\theta$, so we write $q_\theta$),
			 we see that $q_\theta \in \sP$ is a member of the exponential family:
			 \eqn{q_\theta(x) \propto \exp(\theta \cdot \phi(x)).}
			 Plugging this choice of $q_\theta$ into the original problem, we get:
			 \eqn{\min_{\theta \in \R^d} -(\theta \cdot \E_\theta[\phi(x)] - A(\theta)) - \theta \cdot (\hat\mu - \E_\theta[\phi(x)]),}
			 which is equivalent to the maximum likelihood objective:
			 \eqn{\max_{\theta \in \R^d} \pc{\theta \cdot \hat\mu - A(\theta)}.}
			 This completes the proof.
			 One sanity check we can optionally perform is to differentiate the maximum likelihood objective.
			 Using the fact that $\nabla A(\theta) = \E_\theta[\phi(x)]$,
			 we see that the moment constraints indeed hold at the optimal $\theta$:
			 \eqn{\hat\mu - \E_\theta[\phi(x)] = 0,}
			 so that the solution is also in $\sQ$.
		Information geometry (digression)
			We can understand understand maximum entropy duality more fully by looking at it
			 from an information geometry point of view,
			 which studies the geometry of probability distributions.
			Recall the definition of the KL divergence between two distributions $q$ and $p$:
			 \eqn{\KL{q}{p} \eqdef \E_q[\log q(x) - \log p(x)].}
			 Although KL is not a distance metric (it's not even symmetric),
			 we can still talk about the notion of projections with respect to KL.
			Remark: if $\KL{q}{p}$ is finite, then $p(x) = 0$ implies $q(x) = 0$
			 ($p$ must place positive probability mass in at least as many places as $q$ does).
			Just for intuition: First, observe that $\sQ$, the set of distributions consistent with a set of moments
			 is closed under convex combinations of the distributions:
			 \eqn{q_1, q_2 \in \sQ \quad\Rightarrow\quad \alpha q_1 + (1-\alpha) q_2 \in \sQ \quad\text{for all $\alpha \in [0,1]$}}
			 Second, observe that $\sP$, the set of distributions in the exponential family is closed under convex combinations of the parameters:
			 \eqn{p_{\theta_1}, p_{\theta_2} \in \sP \quad\Rightarrow\quad p_{\alpha \theta_1 + (1-\alpha) \theta_2} \in \sP \quad\text{for all $\alpha \in [0,1]$}}
			 So both $\sP$ and $\sQ$ are in some sense convex when viewed appropriately.
			We can think of a ``projection'' of an arbitrary distribution onto either $\sP$ or $\sQ$ based on the KL divergence:
				\word{M-projection} (moment projection) of some $q$ onto $\sP$: $\arg\min_{p \in \sP} \KL{q}{p}$
					Example: maximum likelihood in an exponential family $\sP$ is an M-projection of the empirical distribution $\inv{n} \sum_{i=1}^n \delta_{x^{(i)}}$
					 onto $\sP$.
				\word{I-projection} (information projection) of some $p$ onto $\sQ$: $\arg\min_{q \in \sQ} \KL{q}{p}$
					Example: maximum entropy with respect to $\sQ$ satisfying empirical moments is an I-projection of the uniform distribution onto $\sQ$.
			\theoremHeading{pythagoreanExpFamily}{Pythagorean equality for exponential families}
				Let $\hat p$ be the solution to \refeqn{maxentDuality} (maximum entropy / maximum likelihood solution).
				Then a Pythagorean identity holds:
				 \eqn{\boxed{\KL{q}{p} = \KL{q}{\hat p} + \KL{\hat p}{p} \quad \text{for all $q \in \sQ, p \in \sP$}.}}
				 \Fig{figures.slides/pythagorean}{0.4}{pythagorean}{The Pythagorean identity holds between $\sQ$
				 (distributions satisfying moment constraints) and $\sP$ (distributions in the corresponding exponential family).}
			 This theorem says that some of the intuitions of Euclidean geometry carry over to information geometry
			 when restricted to certain nice families of distributions.
			 See \reffig{pythagorean} for a visualization.
	!verbatim \lecture{3}
	Method of moments for latent-variable models \currlecture \label{sec:mixtureModels}
		Motivation
			For exponential families,
			 we saw that maximum likelihood (choosing $\hat\theta$ to maximize the likelihood of the data)
			 and method of moments (choosing $\hat\theta$ to match the moments of the data)
			 resulted in the same estimator.
			 Furthermore, this estimator can be solved efficiently by convex optimization.
			 Case closed.
			In this section, we consider parameter estimation in \word{latent-variable models},
			 which include examples such as
			 Gaussian mixture models (GMMs),
			 Hidden Markov Models (HMMs),
			 Latent Dirichlet Allocation (LDA),
			 etc.
			 These models are useful in practice because our observed data is often incomplete.
			 For example, in a mixture model,
			 we assume that each data point belongs to a latent cluster (mixture component).
			While latent-variable models are powerful,
			 parameter estimation in latent-variable models turns out rather tricky.
			 Intuitively, it is because we need to both estimate the parameters and infer the latent variables.
			 We can adapt maximum likelihood to the latent-variable setting
			 by maximizing the marginal likelihood (integrating out the latent variable).
			 However, this is in general a \word{non-convex} optimization problem.
			In practice, people often use Expectation Maximization (EM) to (approximately) optimize these objective functions,
			 but EM is only guaranteed to converge to a local optimum,
			 which can be arbitrarily far away from the global optimum.
			 The same problem afflicts other optimization methods such as gradient descent.
			 To get around this problem, one typically optimizes from different random initializations,
			 but this is a heuristic without any theoretical guarantees.
			In this section, we will explore an alternative technique for parameter estimation
			 based on \word{method of moments}.
			 We will see that breaking free of the likelihood allows us to get
			 an efficient algorithm with strong formal guarantees.
		Setup
			We will develop a method of moments estimator for the following mixture model:
			\exampleHeading{bagOfWords}{\word{Naive Bayes mixture model}}
				Let $k$ be the number of possible document clusters (e.g., sports, politics, etc.).
				Let $b$ be the number of possible words in the vocabulary (e.g., ``game'', ``election'').
				let $L$ be the length of a document.
				Model parameters $\theta = (\pi, B)$:
					$\pi \in \Delta_k$: prior distribution over clusters.
					$B = (\beta_1, \dots, \beta_k) \in (\Delta_b)^k$:
					 for each cluster $h$,
					 $\beta_h \in \Delta_b$ is a distribution over words for cluster $h$.
					 These are the columns of $B$, which is a $b \times k$ matrix.
				 Let $\Theta$ denote the set of all valid parameters.
				The probability model $p_\theta(h, x)$ is defined as follows:
					Sample the cluster: $h \sim \text{Multinomial}(\pi)$
					Sample the words in the document independently:
					 $x = (x_1, \dots, x_L) \mid h \sim \text{Multinomial}(\beta_h)$
			The parameter estimation problem is as follows:
			 Given $n$ documents $x^{(1)}, \dots, x^{(n)}$ drawn i.i.d.~from $p_{\theta^*}$,
			 return an estimate $\hat\theta = (\hat \pi, \hat B)$ of $\theta^* = (\pi^*, B^*)$.
			One minor comment is that we can only hope to recover $B^*$ up to permutation of its columns,
			 since the number of the clusters is arbitrary and any permutation yields the same
			 distribution over the observed variables.
			Maximum (marginal) likelihood is the standard approach to parameter estimation.
			 For completeness, we will review the procedure,
			 although we will not need it in the sequel, so this part can be skipped.
			 The maximum likelihood estimator is:
			 \eqn{\hat\theta = \arg\min_{\theta \in \Theta} \sum_{i=1}^n -\log \sum_{h=1}^k p_\theta(h, x^{(i)}).}
			 A popular optimization algorithm is to use the EM algorithm,
			 which can be shown to be either a bound optimization algorithm
			 (which repeatedly constructs a lower bound and optimizes)
			 or a coordinate-wise ascent on a related objective:
				E-step: for each example $i$, compute the posterior
				 \eqn{q_i(h) = p_\theta(h^{(i)} = h \mid x^{(i)}).}
				M-step: optimize the expected log-likelihood:
				 \eqn{\max_{\theta} \sum_{i=1}^n \sum_{h=1}^k q_i(h) \log p_\theta(h, x^{(i)}).}
			 The EM algorithm is widely used and can get excellent empirical results,
			 although there are no theoretical guarantees in general that EM will converge to a global optimum,
			 and in practice, it can get stuck in bad local optima.
		Method of moments
			Step 1: define a \word{moment mapping} $M$ relating the model parameters $\theta$
			 to moments $m$ of the data distribution specified by those parameters.
			Step 2: \word{plug in} the empirical moments $\hat m$ and invert the mapping
			 to get parameter estimates $\hat\theta$.
			 \Fig{figures.slides/momentMappingDiagram}{0.4}{momentMappingDiagram}{
			 Schema for method of moments: We define a moment mapping $M$ from parameters to moments.
			 We estimate the moments (always easy) and invert the mapping to recover parameter estimates (sometimes easy).}
		Moment mapping
			Let $\phi(x) \in \R^p$ be an observation function which only depends on the observed variables $x$.
				Example: $\phi(x) = (x, x^2)$.
			Define the \word{moment mapping}
			 \eqn{\boxed{M(\theta) \eqdef \E_{x \sim p_\theta}[\phi(x)],}}
			 which maps each parameter vector $\theta \in \R^d$ to the expected value of $\phi(x)$
			 with respect to $p_\theta(x)$.
			 This mapping is the key that links moments (which are simple functions of the observed data)
			 with the parameters (which are quantities that we want to estimate).
			Example (Gaussian distribution):
				Suppose our model is a univariate Gaussian with parameters $\theta = (\mu, \sigma^2)$.
				Then for $M$ defined above, the moment equations are as follows:
				 \eqn{M((\mu, \sigma^2)) = \E_{x \sim \sN(\mu, \sigma^2)}[(x, x^2)] = (\mu, \sigma^2 + \mu^2).}
			Let's see how moment mappings are useful.
			 Suppose that someone told us some moments $m^*$ (where $m^* = M(\theta^*)$).
			 Then assuming $M$ were invertible, we could solve for $\theta^* = M^{-1}(m^*)$.
			 Existence of the inverse is known as identifiability:
			 we say that the parameters of a model family $\Theta$ are \word{identifiable}
			 from the moments given by observation function $\phi$ if $M^{-1}$ exists.
			 Note that for mixture models, strict identifiability never holds,
			 because we can always permute the $k$ cluster labels.
			 So we say that a mixture model is identifiable if $|M^{-1}(m)| = k!$ for all $m \in M(\Theta)$.
			Example (Gaussian distribution):
			 We can recover the parameters $\theta^* = (\mu^*, \sigma^{2*})$ from $m^* = (m_1^*, m_2^*)$ as follows:
				$\mu^* = m_1^*$
				$\sigma^{2*} = m_2^* - m_1^{2*}$
			 Thus, the parameters are identifiable given the first two moments.
		Plug-in
			In practice, of course, we don't have access to the true moments $m^*$.
			 However, the key behind the method of moments is that we can estimate it extremely easily
			 using a sample average over the data points.
			 These are the \word{empirical moments}:
			 \eqn{\boxed{\hat m \eqdef \inv{n} \sum_{i=1}^n \phi(x^{(i)}).}}
			Given these empirical moments, we can simply \word{plug in} $\hat m$ for $m^*$
			 to yield the method of moments estimator:
			 \eqn{\boxed{\hat\theta \eqdef M^{-1}(\hat m).}}
		Asymptotic analysis
			How well does this procedure work?
			 It is relatively easy to study how fast $\hat m$ converges to $m^*$, at least in an asymptotic sense.
			 We can show that (i) $\hat m$ is close to $m^*$, and then use that tho show that (ii) $\hat\theta$ is close to $\theta^*$.
			 The analysis reflects the conceptual simplicity of the method of moments.
			First, since $\hat m$ is just an average of i.i.d.~variables, we can apply the
			 central limit theorem:
			 \eqn{\sqrt{n} (\hat m - m^*) \cvd \sN(0, \cov_{x \sim p^*}[\phi(x)]).}
			Second, assuming that $M^{-1}$ is continuous around $m^*$,
			 we can use the delta method to argue that $\hat\theta$ converges $\theta^*$:
			 \eqnl{momAsympVariance}{\boxed{\sqrt{n} (\hat\theta - \theta^*) \cvd \sN(0, \blue{\nabla M^{-1}(m^*)} \cov_{x \sim p^*}[\phi(x)] \blue{\nabla M^{-1}(m^*)^\top}),}}
			 where $\nabla M^{-1}(m^*) \in \R^{d \times p}$ is the Jacobian matrix for the inverse moment mapping $M^{-1}$.
			 Therefore, the parameter error depends on how sensitive $M^{-1}$ is around $m^*$.
			 It is also useful to note that $\nabla M^{-1}(m^*) = \nabla M(\theta^*)^{\dagger}$ (where $\dagger$ denotes pseudoinverse).
			These asymptotics are rough calculations that shed some light onto
			 when we would expect the method of moments to work well:
			 $\nabla M(\theta) \in \R^{p \times d}$ must have full column rank
			 and moreover, the first $d$ singular values should be far away from zero.
			 Intuitively, if we perturb $\theta$ by a little bit, we want $m$ to move a lot,
			 which coincides with our intuitions about the asymptotic error of parameter estimation.
		In general, computing the inverse moment mapping $M^{-1}$ is not easier than the maximum likelihood estimate.
		 Method of moments is only useful if we can find an appropriate observation function $\phi$ such that:
			The moment mapping $M$ is invertible,
			 and hopefully has singular values that are bounded below
			 ($M$ provides enough information about the parameters $\theta$).
			The inverse moment mapping $M^{-1}$ is computationally tractable.
		Computing $\hat\theta$ for \refex{bagOfWords}
			Now that we have established the principles behind method of moments,
			 let us tackle the Naive Bayes clustering model (\refex{bagOfWords}).
			 The strategy is to just start writing some moments and see what kind of equations we get
			 (they turn out to be product of matrices) \citep{anandkumar12moments}.
			Preliminaries
				Assume each document has $L \ge 3$ words.
				Assume $b \ge k$ (at least as many words as clusters).
				We represent each word $x_j \in \R^b$ as an indicator vector
				 which is equal to one in the entry of that word and zero elsewhere.
			Let's start with the first-order moments:
			 \eqn{M_1 \eqdef \E[x_1] = \sum_{h = 1}^k \pi_h \beta_h = B \pi.}
				Note that the moments require marginalizing out the latent variables,
				 and marginalization corresponds to matrix products.
				Interpretation: $M_1 \in \R^b$ is a vector of marginal word probabilities.
				Clearly this is not enough information to identify the parameters.
			We can write the second-order moments:
			 \eqn{M_2 \eqdef \E[x_1 x_2^\top] = \sum_{h = 1}^k \pi_h \beta_h \beta_h^\top = B \blue{\diag(\pi)} B^\top.}
				Interpretation: $M_2 \in \R^{d \times d}$ is a matrix of co-occurrence word probabilities.
				 Specifically, $M_2(u,v)$ is the probability of seeing word $u$ with word $v$,
				 again marginalizing out the latent variables.
				Are the parameters identifiable given $M_1$ and $M_2$?
				 It might be tempting to conclude yes, since
				 there are $O(kb)$ parameters and $M_2$ already specifies $O(b^2)$ equations.
				 But it turns out that the parameters are still non-identifiable from second-order moments
				 (so one has to be careful with parameter counting!)
				 The intuition is that we are trying to decompose $M_2$ into $A A^\top$,
				 where $A = B \diag(\pi)^{1/2}$.
				 However, $M_2$ also equals $(A Q) (A Q)^\top$ for any orthogonal matrix $Q \in \R^{k \times k}$,
				 so we can only identify $A$ up to rotation.
				 In our setting, $A$ has non-negativity constraints,
				 but there's only $O(k)$ of them as opposed to the $O(k^2)$ degrees of freedom in $Q$.
				 So we cannot identify the parameters $\theta$ from just $M_1$ and $M_2$.
			Let us proceed to third-order moments to get more information.
			 \eqn{M_3(\eta) \eqdef \E[x_1 x_2^\top (x_3^\top \eta)] = \sum_{h = 1}^k \pi_h \beta_h \beta_h^\top (\beta_h^\top\eta) = B \blue{\diag(\pi) \diag(B^\top \eta)} B^\top.}
			 Here, $M_3(\eta)$ is a projection of a rank-3 tensor onto $\eta \in \R^p$.
			 Think of $M_3(\eta)$ as $M_2$ (they have the same row and column space),
			 but someone has come in and tweaked the diagonal entries.
			 It turns out that this is enough to pin down the parameters.
			 But first, a useful lemma which captures the core computation: 
			Lemma
				Suppose we observe matrices $X = B D B^\top$ and $Y = B E B^\top$ where
					$D,E$ are diagonal matrices such that the ratios $\{ D_{ii}/E_{ii} \}_{i=1}^k$ are all non-zero and distinct.
					$B \in \R^{b \times k}$ has full column rank
					 (this is a reasonable condition which intuitively says that all the clusters are different
					 in a linear algebriac sense).
					 Note this automatically implies $b \ge k$.
				Then we can recover $B$ (up to permutation/scaling of its columns).
			Proof:
				Simple case
					Assume $B$ is invertible ($b = k$).
					Then $X$ and $Y$ are also invertible.
					Compute
					 \eqn{Y X^{-1} = B E B^\top B^{-\top} D^{-1} B^{-1} = B \underbrace{\blue{E D^{-1}}}_\text{diagonal} B^{-1}.}
					The RHS has the form of an eigendecomposition, so the eigenvectors of $Y X^{-1}$ are be exactly the columns of $B$ up to permutation
					 and scaling.  We actually know the scaling since the columns of $B$ are probability distributions and must sum to $1$.
					Since the diagonal elements of $E D^{-1}$ are distinct and non-zero
					 by assumption, the eigenvectors are distinct.
				Now suppose $X$ and $Y$ are not invertible.
				 Note that $X$ and $Y$ have the same column space,
				 so we can basically project them down into that column space where they will be invertible.
				Let $U \in \R^{b \times k}$ be any orthonormal basis of the column space of $B$
				 (taking the SVD of $M_2$ suffices: $M_2 = U S U^\top$).
				 Note that $U$ has full column rank by assumption.
				Important: although $B \in \R^{b \times k}$ is not invertible, $\tilde B \eqdef U^\top B \in \R^{k \times k}$ is invertible.
				So let us project $X$ and $Y$ onto $U$ by both left multiplying by $U^\top$ and right multiplying by $U$.
				Then we have the following decomposition:
					$U^\top X U = \tilde B D \tilde B^\top$
					$U^\top Y U = \tilde B E \tilde B^\top$
				Now we are back in the simple case,
				 which allows us to recover $\tilde B$.
				 We can obtain $B = U \tilde B$.
			We apply the lemma with $X = M_2$ and $Y = M_3(\eta)$.
			 Since $\eta$ is chosen at random, $D = \diag(\pi)$ and $E = \diag(\pi) \diag(B^\top\eta)$
			 will have distinct ratios with probability $1$.
			Once we have recovered $B$, then we can recover $\pi$ easily by setting $\pi = B^\dagger M_1$.
			We have therefore shown that with infinite data,
			 the method of moments approach recovers the true parameters.
			 For finite data, we have to use a matrix perturbation argument, detailed in \citet{anandkumar12moments}.
		We have so far that we can solve the moment equations for the Naive Bayes clustering model
		 and recover the parameters given the moments.
		 These techniques can be further extended to obtain consistent parameter estimates
		 for spherical Gaussian mixture models \citep{hsu13spherical},
		 hidden Markov models \citep{anandkumar12moments},
		 Latent Dirichlet allocation \citep{anandkumar12lda},
		 stochastic block models for community detection \citep{anandkumar2013community},
		 noisy-or Bayesian networks \citep{halpern2013unsupervised},
		 mixture of linear regressions \citep{chaganty2014graphical},
		 general graphical models \citep{chaganty2014graphical},
		 neural networks \citep{janzamin2015beating},
		 and others.
		 An active area of research is to extend these techniques to yet other model families.
	Fixed design linear regression \currlecture \label{sec:fixedDesignLinearRegression}
		So far, we have considered parameter estimation of $\theta^*$
		 given data $x^{(1)}, \dots, x^{(n)}$ drawn i.i.d.~from $p_{\theta^*}$.
		 We will now move towards the \word{prediction} setting,
		 where we are trying to learn a function from an input $x$ to an output $y$.
		 Of course, we could still study parameter estimation in this context,
		 but we will use the opportunity to segue into prediction.
		Setup
			Our goal is to predict a real-valued output (response) $y \in \R$
			 given an input (covariates/features) $x \in \R^d$.
			The \word{fixed design} setting means that
			 we have a fixed set of $n$ inputs $x_1, \dots, x_n$,
			 which are treated as constants.
			 As an example, imagine that $x_1, \dots, x_n$ as the 50 states in America,
			 and $y_1, \dots, y_n$ represent their demographics, weather, etc.
			 Fixed design makes sense here because the states isn't random or growing--they're just fixed.
			 In general, one can think of the fixed design setting as performing signal reconstruction,
			 where the $x_i$'s correspond to fixed locations, and $y_i$ is a noisy sensor reading.
			We assume that there is a true underlying parameter vector $\theta^* \in \R^d$,
			 which governs the outputs.  For each $i = 1, \dots, n$:
			 \eqnl{regressionModel}{y_i = x_i \cdot \theta^* + \epsilon_i,}
			 where we assume that the noise terms $\epsilon_i$ are i.i.d.~with mean $\E[\epsilon_i] = 0$
			 and variance $\var[\epsilon_i] = \sigma^2$.
			 Note that $\epsilon_1, \dots, \epsilon_n$ are the only source of randomness in the problem.
			FIGURE: [linear regression line over $x_1, \dots, x_n$]
			At training time, we observe one realization of $y_1, \dots, y_n$.
			 For convenience, let's put the data into matrices:
				$X = [x_1, \dots, x_n]^\top \in \R^{n \times d}$
				$\epsilon = [\epsilon_1, \dots, \epsilon_n]^\top \in \R^d$
				$Y = [y_1, \dots, y_n]^\top \in \R^d$
				$\Sigma = \inv{n} X^\top X \in \R^{d \times d}$ (second moment matrix)
			FIGURE: [matrices $X$, $\theta$, $Y$]
			Our goal is to minimize the \word{expected risk}\footnote{
			 We are using expected risk in the machine learning sense,
			 which is a quality metric on parameters, not expected risk in statistics, which is
			 a quality metric on decision procedures.}
			 as defined by the squared loss:
			 \eqnl{regressionRisk}{L(\theta) \eqdef \inv{n} \sum_{i=1}^n \E[(x_i \cdot \theta - y_i)^2]
			 = \inv{n} \E[\|X \theta - Y\|_2^2].}
			 Note that the expectation is over the randomness in $Y$ (recall that $X$ is fixed).
			The least squares estimator is as follows:
			 \eqn{\hat\theta \eqdef \arg\min_{\theta \in \R^d} \inv{n} \|X\theta - Y\|_2^2.}
			 By taking derivatives and setting the result to zero,
			 we obtained the following closed form solution:
			 \eqn{\hat\theta = X^\top X^{-1} X^\top Y = \inv{n} \Sigma^{-1} X^\top Y,}
			 where we assume that the covariance matrix $\Sigma \eqdef \inv{n} X^\top X \in \R^{d \times d}$ is invertible.
			Our goal is to study the expected risk of the least squares estimator:
			 \eqn{L(\hat\theta).}
			 For simplicity, we will study the expectation $\E[L(\hat\theta)]$,
			 where the expectation is taken over the training data.
		Let us first understand the expected risk $L(\theta)$ of some fixed $\theta$.
			The goal will be to understand this quantity geometrically.
			 The basic idea is to expand $Y = X \theta^* + \epsilon$;
			 the rest is just algebra.
			 We have:
			 \eqn{L(\theta)
			 &= \inv{n} \E[\|X \theta - Y\|_2^2] \\
			 &= \inv{n} \E[\|X \theta - X \theta^* + \epsilon\|_2^2] \aside{by definition of $Y$ \refeqn{regressionModel}} \\
			 &= \inv{n} \E[\|X \theta - X \theta^*\|_2^2 + \|\epsilon\|_2^2] \aside{cross terms vanish} \\
			 &= \inv{n} (\theta - \theta^*)^\top (X^\top X) (\theta - \theta^*) + \sigma^2 \aside{algebra, definition of $\epsilon$} \\
			 &= \|\theta - \theta^*\|_\Sigma^2 + \sigma^2.
			 }
			Intuitively, the first term of the 
			 expected risk is the squared distance between the estimate $\theta$ and
			 the true parameters $\theta^*$ as measured by the shape of the data.
			 If the data does not vary much in one direction,
			 then the discrepancy between $\theta$ and $\theta^*$ will
			 be downweighted in that direction,
			 because that direction doesn't matter for prediction,
			 which depends on $x \cdot \theta$.
			The second term is the unavoidable variance term coming from the noise,
			 which is present even with the optimal parameters $\theta^*$.
			 Note that $L(\theta^*) = \sigma^2$.
			In conclusion, the \word{excess risk}---how far we are from optimal---is:
			 \eqnl{excessRiskRregression}{\boxed{L(\theta) - L(\theta^*) = \|\theta - \theta^*\|_\Sigma^2.}}
		Let us now analyze the excess risk $L(\hat\theta) - L(\theta^*)$
		 of the least squares estimate $\hat\theta$.
			Assume that $X^\top X \succ 0$
			 (which means necessarily that $n \ge d$).
			 The key is to expand $\hat\theta$ and $Y$ based on their definitions
			 and perform algebra.
			 The expectation is over the test data now.
			 Rewriting the excess risk:
			 \eqn{
			 L(\hat\theta) - L(\theta^*)
			 &=\|\hat\theta - \theta^*\|_\Sigma^2 \aside{by \refeqn{excessRiskRregression}} \\
			 &= \inv{n} \|X \hat\theta - X \theta^*\|_2^2 \\
			 &= \inv{n} \|X (X^\top X)^{-1} X^\top (X \theta^* + \epsilon) - X \theta^*\|_2^2 \\
			 &= \inv{n} \|\Pi X \theta^* + \Pi \epsilon - X \theta^*\|_2^2 \aside{projection $\Pi \eqdef X(X^\top X)^{-1} X^\top$} \\
			 &= \inv{n} \|\Pi \epsilon\|_2^2 \aside{projection doesn't change $X \theta^*$, cancel} \\
			 &= \inv{n} \tr(\Pi \epsilon \epsilon^\top) \aside{projection is idempotent and symmetric}.
			 }
			Taking expectations (over the training data), and using the fact that $\E[\epsilon \epsilon^\top] = \sigma^2 I$,
			 we get:
			 \eqnl{fixedDesignUnreg}{\boxed{\E[L(\hat\theta) - L(\theta^*)] = \frac{d \sigma^2}{n}.}}
			 Note that the complexity of the problem is completely determined by the dimensionality $d$
			 and the variance of the noise $\sigma^2$.
			Intuitively, the noise $\epsilon$ is an $n$-dimensional vector gets projected onto $d$ dimensions
			 by virtue of having to fit the data using a linear function with $d$ degrees of freedom.
	!verbatim \lecture{4}
	General loss functions and random design \currlecture \label{sec:fullAsymptotics}
		Motivation
			In the previous section, the simplicity of the least squares problem
			 in the fixed design setting allowed us to compute everything exactly.
			 What happens if we are in the random design setting or if we wanted to handle
			 other loss functions (e.g., logistic loss)?
			 We can't hope to compute the excess risk $L(\hat\theta) - L(\theta^*)$ exactly.
			In this unit, we will return to using asymptotics to get a handle on this quantity.
			 In particular, we will show that the excess risk approaches a simple
			 quantity as the number of data points $n$ goes to infinity
			 by performing \word{Taylor expansions} around $\theta^*$.
			 In brief, we will see that $\hat\theta - \theta^*$ is approximately Gaussian with some variance
			 that is $O\p{\frac{1}{n}}$,
			 and assuming that $L$ is continuous, we can convert this result into one about the expected risk.
				FIGURE: [$\hat\theta$ in a ellipsoid Gaussian ball around $\theta^*$]
			We will carry out the asymptotic analysis in a fairly general setting:
				We assume a general (twice-differentiable) loss function.
				We do not make any assumptions about the data generating distribution.
		Setup
			$z = (x,y)$ is an example
			$\ell(z, \theta)$ is a loss function on example $z$ with parameters $\theta \in \R^d$
				Example: $\ell((x,y), \theta) = \half (\theta \cdot x - y)^2$
			Let $p^*$ be the true probability distribution over examples $z \in \sZ$,
			 not assumed to be related to our loss function in any way.
			Let $\theta^* \in \R^d$ be the minimizer of the expected risk:
			 \eqn{\theta^* \eqdef \arg\min_{\theta \in \R^d} L(\theta), \quad L(\theta) \eqdef \E_{z \sim p^*}[\ell(z, \theta)]}
			Let $\hat\theta \in \R^d$ be the minimizer of the empirical risk:
			 \eqn{\hat\theta \eqdef \arg\min_{\theta \in \R^d} \hat L(\theta), \quad \hat L(\theta) \eqdef \inv{n} \sum_{i=1}^n \ell(z^{(i)}, \theta),}
			 where $z^{(1)}, \dots, z^{(n)}$ are drawn i.i.d.~from $p^*$.
			Recall that we are interested in studying the excess risk $L(\hat\theta) - L(\theta^*)$.
		Assumptions on the loss
			Loss function $\ell(z,\theta)$ is twice differentiable in $\theta$ (works for squared and logistic loss, but not hinge)
			Let $\nabla \ell(z,\theta) \in \R^d$ be the gradient of the loss at $\theta$.
				Example: $\nabla \ell(z, \theta) = (\theta \cdot x - y) x$
			Let $\nabla^2 \ell(z,\theta) \in \R^{d \times d}$ be the Hessian of the loss at $\theta$.
				Example: $\nabla^2 \ell(z, \theta) = x x^\top$
			Assume that the expected loss Hessian $\E_{z \sim p^*}[\nabla^2 \ell(z, \theta)] \succ 0$ is positive definite
			 for all $\theta \in \R^d$.
			 This assumption is actually not needed, but it will make the math simpler.\footnote{
			 In fact, if the expected loss is rank deficient, things are actually even better,
			 since the complexity will depend on the rank of the Hessian rather than the dimensionality.}
		Outline of analysis
			Step 0 (\word{consistency}): show that $\hat\theta \cvP \theta^*$.
			 This is obtained by a uniform convergence argument to show that $\hat L$ approximates $L$ well.
			 Then, since the Hessian $\E[\nabla^2 \ell(z, \theta)]$ is positive definite,
			 minimizing $\hat L$ will eventually minimize $L$.
			 Uniform convergence will be discussed later,
			 so we will not dwell on this point.
			Step 1: obtain an asymptotic expression for the \word{parameter error} by Taylor expanding the gradient of the empirical risk.
			 \eqn{\boxed{\hat\theta - \theta^*.}}
			Step 2: obtain an asymptotic expression for the \word{excess risk} by Taylor expanding the expected risk.
			 \eqn{\boxed{L(\hat\theta) - L(\theta^*).}}
		 While we haven't made any assumptions about the relationship between $p^*$ and $\ell$,
		 results become a lot nicer if there is.  This relationship is captured by the following definition:
		\definitionHeading{wellSpecified}{well-specified model}
			Assume that the loss function corresponds to the log-likelihood under a probabilistic model $p_\theta$:
			 \eqn{\ell((x,y); \theta) = -\log p_\theta(y \mid x),}
			 so that $\hat\theta$ is the (conditional) maximum likelihood estimate under this model.
			We say that this model family $\{p_\theta\}_{\theta \in \R^d}$ is
			 \word{conditionally well-specified} if $p^*(x, y) = p^*(x) p_{\theta^*}(y \mid x)$ for some parameter $\theta^* \in \R^d$.
			Suppose each model $\theta$ actually specifies a joint distribution over both $x$ and $y$:
			 $p_\theta(x, y)$.  We say that this model family $\{p_\theta\}_{\theta \in \R^d}$ is
			 \word{jointly well-specified} if $p^*(x, y) = p_{\theta^*}(x, y)$ for some parameter $\theta^* \in \R^d$.
			 This places a much stronger assumption on the data generating distribution.
			 If $x$ is an image and $y$ is a single binary label, this is much harder to satisfy.
			Of course, jointly well-specified implies conditionally well-specified.
		In the conditionally well-specified case,
		 the Bartlett identity allows us to equate the variance of the risk gradient with the risk Hessian.
		 This quantity is the \word{Fisher information} matrix (or rather, a generalization of it).
		\theoremHeading{bartlett}{Bartlett identity}
			In the well-specified case (conditionally, and thus also jointly),
			 the following identity holds:
			 \eqnl{bartlett}{\boxed{\nabla^2 L(\theta^*) = \cov[\nabla \ell(z, \theta^*)].}}
		Proof of \refthm{bartlett}:
			Recall that $z = (x,y)$.
			Using the fact that probability densities integrate to one:
			 \eqn{\int p^*(x) \underbrace{e^{-\ell(z, \theta^*)}}_{p_{\theta^*}(y \mid x)} dz = 1.}
			Assuming regularity conditions, differentiate with respect to $\theta^*$:
			 \eqn{\int p^*(x) e^{-\ell(z, \theta^*)} (-\nabla \ell(z, \theta^*)) dz = 0.}
			 Note that this implies $\E[\nabla \ell(z, \theta^*)] = 0$,
			 which shouldn't be surprising since $\theta^*$ minimizes $L(\theta) = \E[\ell(z, \theta^*)]$.
			Differentiating again, using the product rule:
			 \eqn{\int p^*(x) [-e^{-\ell(z, \theta^*)} \nabla^2 \ell(z, \theta^*) + e^{-\ell(z, \theta^*)} \nabla \ell(z, \theta^*) \nabla \ell(z, \theta^*)^\top] dz = 0.}
			Re-arranging:
			 \eqn{\E[\nabla^2 \ell(z, \theta^*)] = \E[\nabla \ell(z, \theta^*) \nabla \ell(z, \theta^*)^\top].}
			Using the fact that $\E[\nabla \ell(z, \theta^*)] = 0$ and the definition of $L(\theta)$ yields the result.
		Remark: our general analysis does not assume the model is well-specified.
		 We will only make this assumption for examples to get simple expressions for intuition.
		\exampleHeading{regressionAsymptotics}{well-specified random design linear regression}
			Assume that the conditional model is as follows:
				$x \sim p^*(x)$ for some arbitrary $p^*(x)$
				$y = \theta^* \cdot x + \epsilon$, where $\epsilon \sim \sN(0, 1)$
			Loss function: $\ell((x,y), \theta) = \half(\theta \cdot x - y)^2$ (the log-likelihood up to constants)
			Check that the following two are equal:
				Hessian: $\nabla^2 L(\theta) = \E[x x^\top]$ (covariance matrix of data)
				Variance of gradient: $\cov[\nabla\ell(z, \theta)] = \E[\epsilon x x^\top \epsilon] - \E[\epsilon x] \E[\epsilon x]^\top = \E[x x^\top]$,
				 where the last equality follows from independence of $x$ and $\epsilon$, and the fact that $\E[\epsilon x] = 0$.
		 Now let us analyze the parameter error (step 1) and expected risk (step 2) in the general (not necessarily well-specified) case.
		 In the following, pay attention how fast each of the terms is going to zero.
		Step 1: analysis of \textbf{parameter error}
			Since $\ell$ is twice differentiable, we can perform a Taylor expansion of the gradient of the empirical risk ($\nabla \hat L$) around $\theta^*$:
			 \eqn{\nabla \hat L(\hat\theta) = \nabla \hat L(\theta^*) + \nabla^2 \hat L(\theta^*) (\hat\theta - \theta^*) + O_p(\|\hat\theta - \theta^*\|_2^2).}
			Using the fact that the LHS $\nabla \hat L(\hat\theta) = 0$ (by optimality conditions of the empirical risk minimizer) and rearranging:
			 \eqnl{parameterError}{\hat\theta - \theta^* = -\nabla^2 \hat L(\theta^*)^{-1} \p{\nabla \hat L(\theta^*) + O_p(\|\hat\theta - \theta^*\|_2^2)}.}
			As $n \to \infty$:
				By the weak law of large numbers,
				 we have 
				 \eqn{\nabla^2 \hat L(\theta^*) \cvP \nabla^2 L(\theta^*).}
				 Since the inverse is a smooth function around $\theta^*$ (we assumed $\nabla^2 L(\theta^*) \succ 0$),
				 we can apply the continuous mapping theorem:
				 \eqn{\nabla^2 \hat L(\theta^*)^{-1} \cvP \nabla^2 L(\theta^*)^{-1}.}
				$\hat L(\theta^*)$ is a sum of mean zero i.i.d. variables, so by the central limit theorem,
				 $\sqrt{n} \cdot \nabla \hat L(\theta^*)$ converges in distribution:
				 \eqn{\sqrt{n} \cdot \nabla \hat L(\theta^*) \cvd \sN(0, \cov[\nabla \ell(z, \theta^*)]).}
				 An intuitive implication of this result is that $\nabla \hat L(\theta^*) = O_p\p{\frac{1}{\sqrt{n}}}$.
				Suppose $\hat\theta - \theta^* = O_p(f(n))$.
				 By \refeqn{parameterError}, $f(n)$ goes to zero at a rate which is the maximum of $\frac{1}{\sqrt{n}}$ and $f(n)^2$.
				 This implies that $f(n) = \frac{1}{\sqrt{n}}$, so we have:
				 \eqn{\sqrt{n} \cdot O_p(\|\hat\theta - \theta^*\|_2^2) \cvP 0.}
			By Slutsky's theorem (see Appendix),
			 we can substitute the limits into \refeqn{parameterError} to obtain:
			 \eqnl{parameterError2}{\boxed{\sqrt{n} \cdot \underbrace{(\hat\theta - \theta^*)}_\text{parameter error} \cvd \sN\p{0, \blue{\nabla^2 L(\theta^*)^{-1}} \green{\cov[\nabla \ell(z, \theta^*)]} \blue{\nabla^2 L(\theta^*)^{-1}}},}}
			 where we used the fact that if $x_n \cvd \sN(0, \Sigma)$, then $A x_n \cvd \sN(0, A \Sigma A^\top)$.
			 This also establishes that the parameter error behaves $\hat\theta - \theta^* = O_p\p{\frac{1}{\sqrt{n}}}$ as expected.
				$\blue{\nabla^2 L(\theta^*)}$: measures the amount of \word{curvature} in the loss function at $\theta^*$.
				 The more there is, the more stable the parameter estimates are.
				 This quantity is analogous to the Jacobian of the moment mapping for method of moments.
				$\green{\cov[\nabla \ell(z, \theta^*)]}$: measures the \word{variance} in the loss gradient.
				 The less there is, the better.
				 This quantity is analogous to variance of the observation function in the method of moments.
				When $\sqrt{n} (\hat\theta - \theta^*) \cvd \sN(0, \Sigma)$, $\Sigma$ is known as the \word{asymptotic variance} of the estimator $\hat\theta$.
			\exampleHeading{regressionAsymptotics2}{well-specified linear regression}
				In this case, due to \refeqn{bartlett},
				 we have $\cov[\nabla \ell(z, \theta^*)] = \E[\nabla^2 \ell(z, \theta^*)] = \E[xx^\top]$,
				 so the variance factor is canceled out by one of the curvature factors.
			 \eqn{\boxed{\sqrt{n} \cdot (\hat\theta - \theta^*) \cvd \sN\p{0, \blue{\E[x x^\top]^{-1}}}.}}
			 Intuition: the larger $x$ is, the more stable the parameter estimates;
			 think about wiggling a pencil by either holding it with two hands out at the ends (large $x$) or near the center (small $x$).
		Step 2: analysis of excess risk
			Perform a Taylor expansion of the expected risk around $\theta^*$:
			 \eqn{L(\hat\theta) = L(\theta^*) + \nabla L(\theta^*)^\top (\hat\theta - \theta^*) + \half (\hat\theta - \theta^*)^\top \nabla^2 L(\theta^*) (\hat\theta - \theta^*) + O_p(\|\hat\theta - \theta^*\|_2^3).}
			By optimality conditions of the expected risk minimizer $\theta^*$, we have $\nabla L(\theta^*) = 0$.
			 This the key to getting $O\p{\frac{1}{n}}$ rates of convergence.
			Multiplying by $n$ and rearranging:
			 \eqn{n(L(\hat\theta) - L(\theta^*)) = \half \sqrt{n} (\hat\theta - \theta^*)^\top \nabla^2 L(\theta^*) \sqrt{n} (\hat\theta - \theta^*) + O_p(n \|\hat\theta - \theta^*\|_2^3).}
			Substituting in the parameter error:
				Facts
					If $x_n \cvd \sN(0, \Sigma)$,
					 then $x_n x_n^\top \cvd \sW(\Sigma, 1)$,
					 where $\sW(\Sigma, 1)$ is a Wishart distribution with mean $\Sigma$ and 1 degree of freedom.
					Taking the trace of both sides, we have that
					 $x_n^\top x_n = \tr(x_n x_n^\top) \cvd \tr(\sW(\Sigma, 1))$.\footnote{We are abusing notation slightly by writing the trace of a distribution $D$ to mean the distribution of the trace of $x \sim D$.}
					The distribution on the RHS is a weighted sum of $d$ chi-squared distributed variables,
					 whose distribution is the same as $\sum_{j=1}^d \Sigma_{jj} v_j^2$,
					 where $v_j \sim \sN(0, 1)$ is a standard Gaussian and $v_j^2 \sim \chi^2_1$ is a chi-squared.
				In our context, let us define
				 \eqn{x_n = \sqrt{n} (\nabla^2 L(\theta^*))^{\frac12} (\hat\theta - \theta^*).}
				 Then
				 \eqn{\Sigma
				 &= \nabla^2 L(\theta^*)^{-\half} \cov[\nabla \ell(z,\theta^*)] \nabla^2 L(\theta^*)^{-\half}.}
				 Therefore:
				 \eqn{\boxed{n(L(\hat\theta) - L(\theta^*)) \cvd \half \tr \sW\p{\blue{\nabla^2 L(\theta^*)^{-\half} \cov[\nabla \ell(z,\theta^*)] \nabla^2 L(\theta^*)^{-\half}}, 1},}}
				 where $\sW(V, n)$ is the Wishart distribution with scale matrix $V$ and $n$ degrees of freedom.
			\exampleHeading{wellSpecifiedRisk}{well-specified models}
				Since the model is well-specified, everything cancels nicely, resulting in:
				 \eqnl{dnAsymptoticsLinear}{\boxed{n(L(\hat\theta) - L(\theta^*)) \cvd \half \tr \sW(I_{d \times d}, 1).}}
				The limiting distribution is half times a $\chi^2_d$ distributed random variable,
				 which has mean $\frac{d}{2}$ and variance $d$.
				To get an idea of their behavior, we can compute the mean and variance:
					Mean: $\E[n(L(\hat\theta) - L(\theta^*))] \cv \frac{d}{2}$.
					Variance: $\var[n(L(\hat\theta) - L(\theta^*))] \cv d$.
				 In short, 
				 \eqn{\boxed{L(\hat\theta) - L(\theta^*) \sim \blue{\frac{d}{2n}}.}}
				 Note that this recovers the result from fixed-design linear regression \refeqn{fixedDesignUnreg}
				 (the factor of $\frac1{2\sigma^2}$ is due to the different definition of the loss function).
				Interestingly, in the well-specified case,
				 the expected risk does not depend asymptotically on any properties of $x$.
					For parameter estimation, the more $x$ varies, the more accurate the parameter estimates.
					For prediction, the more $x$ varies, the harder the prediction problem.
					The two forces cancel each other out exactly (asymptotically).
		Remarks
			For this brief section, suppose the model is well-specified.
			We have shown that excess risk $L(\hat \theta) - L(\theta^*)$ is exactly $\frac{d}{2n}$ asymptotically;
			 we emphasize that there are no hidden constants and this is equality, not just a bound.
			Lower-order terms
				Of course there could be more error lurking in the lower order ($\inv{n^2}$) terms.
				For linear regression, the low-order terms $O_p(\|\hat\theta-\theta^*\|_2^2)$ in the Taylor expansion
				 are actually zero, and the only approximation comes from estimating the second moment matrix $\E[x x^\top]$.
				For the fixed design linear regression setting, $\nabla^2 \hat L(\theta^*) = \nabla^2 L(\theta^*)$,
				 so all lower-order terms are identically zero,
				 and so our asymptotic expressions are exact.
			Norm/regularization
				We only obtained results in the unregularized case.
				 Asymptotically as $n \to \infty$ and the dimension $d$ is held constant,
				 the optimal thing to do (up to first order) is not use regularization.
				So these results are only meaningful when $n$ is large compared to the complexity (dimensionality) of the problem.
				 This is consistent with the fact that the type of analyses we do are fundamental local around the optimum.
			!comment Question: can we improve on the empirical risk minimizer (maximum likelihood estimator)?
				The basic answer is no, in that it achieves the lowest asymptotic variance
				 of asymptotically unbiased estimators.
				The Cramer-Rao lower bound states that the variance of an unbiased estimator
				 is lower bounded by the inverse of the Fisher information matrix.
		!comment Let us compare the asymptotic expression \refeqn{dnAsymptoticsLinear} with results that we've derived previously in this class.
			Using uniform convergence, we were only able to get a $\inv{\sqrt{n}}$ convergence rate for the unrealizable setting.
			 This was unavoidable using our techniques since we relied on concentration of empirical risk to expected risk,
			 which even for a single hypothesis is already $\frac{1}{\sqrt{n}}$.
			How did we get a faster rate?
			 While asymptotics gives us the correct rate (as long as $n \to \infty$ while all other quantities such as dimensionality remain constant),
			 there are non-asymptotic effects hidden away in the $O_p\p{n^{-\frac32}}$ terms.
			!comment The picture is more complicated: there can be both $\frac{1}{\sqrt{n}}$ and $\frac{1}{n}$ terms with different constants
			 capturing different aspects regarding the complexity of the learning problem.
			 For instance, $\frac{1}{\sqrt{n}}$ is the rate associated with the norm,
			 while $\frac{1}{n}$ is associated with the dimensionality.
			In online learning, we were able to get a $\frac{\log n}{n}$ bound for strongly convex loss functions.
			 However, strong convexity is too much to ask for, since any linear model will not satisfy this.
			 In the asymptotic setting, we only need strong convexity in expectation
			 (averaged over data points) at $\theta^*$ (remember, the entire analysis operates locally around $\theta^*$).
			 It is also possible to analyze online learning under statistical assumptions and
			 obtain bounds that depend on strong convexity in expectation.
		!comment Comparison of generative versus discriminative models (skipped in lecture)
			Recall one of the main motivations of asymptotic analysis was that we could compare different estimators.
			In this section, let's assume that the model is \emph{jointly} well-specified and is an exponential family
			 (includes logistic regression, conditional random fields, MRFs, etc):
			 \eqn{p_\theta(x,y) = \exp\p{ \phi(x,y)^\top\theta - A(\theta)},}
			 where
				$\phi(x,y) \in \R^d$ is the feature vector,
				$\theta \in \R^d$ are the parameters, and
				$A(\theta) = \log \int_{\sX \times \sY} \exp(\phi(x,y)^\top\theta) dx dy$ is the joint log-partition function.
				$A(\theta; x) = \log \int_{\sY} \exp(\phi(x,y)^\top\theta) dy$ is the conditional log-partition function (useful later).
			We consider two estimators which are used to train:
				Generative: $\lgen((x,y), \theta) = -\log p_\theta(x,y)$ defines estimator $\hat\theta_\text{gen}$
				Discriminative: $\ldis((x,y), \theta) = -\log p_\theta(y \mid x)$ defines estimator $\hat\theta_\text{dis}$
			 Here, we are being careful to define the estimators with respect to the same model, but only changing the estimator
			 as to pin down the underlying essence between generative and discriminative estimation.
			Important: note that we are using different loss functions at training time, although at test time, we still evaluate using the discriminative loss $\ldis$.
			Recall that the asymptotic variance of the estimators $\hat\theta - \theta^*$ are functions of
			 $\nabla^2 \ell(z; \theta)^{-1}$.
			For exponential families, the derivatives are simply the moments of the distributions:
				$L_\text{gen}(\theta^*) = \E[\nabla^2 \lgen((x,y), \theta^*)] = \nabla^2 A(\theta^*) = \cov[\phi(x,y)]$.
				$L_\text{dis}(\theta^*) = \E[\nabla^2 \ldis((x,y), \theta^*)] = \E[\nabla^2 A(\theta; x)] = \E[\cov[\phi(x,y) \mid x)]]$.
			Key variance decomposition identity:
			 \eqn{\var[\phi(x,y)] = \E[\var[\phi(x,y) \mid x]] + \var[\E[\phi(x,y) \mid x]].}
			 Since variance matrices are PSD, we have that
			 \eqn{\var[\phi(x,y)] \succeq \E[\var[\phi(x,y) \mid x]].}
			 Inverting:
			 \eqn{\underbrace{\var[\phi(x,y)]}_\text{asymptotic variance of $\hat\theta_\text{gen}-\theta^*$} \preceq \underbrace{\E[\var[\phi(x,y) \mid x]]}_\text{asymptotic variance of $\hat\theta_\text{dis}-\theta^*$}.}
			 This says that the asymptotic variance of generative estimator ($\hat\theta_\text{gen}$) to be at most
			 the asymptotic variance of the discriminative estimator ($\hat\theta_\text{dis}$).
			To get the expected risk from the parameter error,
			 we simply left and right multiply by the \emph{same} matrix $\nabla^2 L_\text{dis}(\theta^*)^{-\half}$.
			 Therefore, the expected risk of the generative estimator is at most the generaliztaion of the discriminative estimator.
			However, if the model is not jointly well-specified, then the two estimators will not even converge to the same $\theta^*$ in general,
			 and the discriminative estimator will clearly be better since it converges to the expected risk minimizer.
	Regularized fixed design linear regression \currlecture \label{sec:regularizedLinearRegression}
		So far, we have considered asymptotic analyses of estimators,
		 which holds when the number of examples $n \to \infty$ holding the dimension $d$ of the problem fixed.
		 In this regime, maximum likelihood estimators are the best option (if we can compute them efficiently).
		 However, when $d$ is comparable to $n$,
		 it turns out that \word{regularization} can improve the accuracy of our estimates.
		James-Stein estimator (digression)
			Before diving in to linear regression, let us visit the simpler problem of Gaussian mean estimation,
			 which we started with:
			 given $x^{(1)}, \dots, x^{(n)}$ drawn i.i.d.~from $\sN(\theta^*, \sigma^2 I)$,
			 what estimator $\hat\theta$ should we use?
			We defined the sample mean estimator $\hat\theta = \inv{n} \sum_{i=1}^n x^{(i)}$ \refeqn{gaussianSampleMean}
			 and computed its expected MSE to be $\frac{d\sigma^2}{n}$.
			 Can we do better?
			The answer is yes, surprisingly, as demonstrated by Charles Stein in 1955 in what is famously
			 known as Stein's paradox.
			 The James-Stein estimator (1961) is one popular estimator that improves over the sample mean estimator:
			 \eqnl{jamesStein}{\hat\theta_\text{JS} \eqdef \p{1 - \frac{(d - 2) \sigma^2}{n \|\hat\theta\|_2^2}} \hat\theta.}
			 In words, this estimator shrinks the sample mean $\hat\theta$ by some data-dependent factor towards $0$
			 ($0$ is not special---any fixed point would work).
			 The amount of shrinkage is governed by the size of the initial estimate $\|\hat\theta\|_2^2$;
			 the smaller it is, the more we shrink.
			For intuition, suppose $\theta^* = 0$.
			 Then $\|\hat\theta\|_2^2$ is approximately $\frac{d \sigma^2}{n}$, which provides a massive shrinkage
			 factor of $\frac{2}{d}$.
			 When $\theta^* \neq 0$, then the shrinkage factor goes to $0$ as we get more data,
			 which is desired.
			There is much more to say about the Stein's paradox,
			 but the point of this digression is to show that the standard estimators
			 are often not optimal even when the intuitively feel like they should be.
		Recall the fixed design linear regression setup:
			We have $n$ examples, inputs are $d$-dimensional.
			Design matrix: $X \in \R^{n \times d}$
			Responses: $Y \in \R$
			Noise: $\epsilon \in \R$, where components are independent and
			 $\E[\epsilon_i] = 0$ and $\var[\epsilon_i] = \sigma^2$
			Assume data satisfies:
			 \eqn{Y = X \theta^* + \epsilon.}
		Now consider the \word{regularized least squares} estimator (ridge regression):
		 \eqn{\hat\theta
		 &= \arg\min_{\theta \in \R^d} \inv{n} \|X \theta - Y\|_2^2 + \blue{\lambda \|\theta\|_2^2} \\
		 &= \inv{n} \Sigma_\lambda^{-1} X^\top Y.}
		 where
		 \eqn{\Sigma_\lambda \eqdef \inv{n} X^\top X + \lambda I.}
		 Here, $\lambda \ge 0$ is the regularization strength that
		 controls how much we want to shrink the estimate towards $0$ to guard against overfitting.
		 Intuitively, the more data points we have (larger $n$),
		 the less we should regularize (smaller $\lambda$),
		 and vice-versa.
		 But what precisely should $\lambda$ be and what is the resulting expected risk?
		 The subsequent analysis will provide answers to these questions.
		The first main insight is the \word{bias-variance tradeoff},
		 whose balance is determined by $\lambda$.
		 Let us decompose the excess risk:
		 \eqn{\E[\|\hat\theta - \theta^*\|_\Sigma^2]
		 &= \E[\|\hat\theta - \E[\hat\theta] + \E[\hat\theta] - \theta^*\|_\Sigma^2] \\
		 &= \underbrace{\E[\|\hat\theta - \E[\hat\theta]\|_\Sigma^2]}_{\eqdef \text{Var}} + \underbrace{\|\E[\hat\theta] - \theta^*\|_\Sigma^2}_{\eqdef \text{Bias}^2},
		 }
		 where the cross terms are designed to cancel out.
		 Note that in the unregularized case ($\lambda = 0$),
		 the bias is zero (provided $\Sigma \succ 0$) since
		 $\E[\hat\theta] = (X^\top X)^{-1} X^\top (X \theta^* + \E[\epsilon]) = \theta^*$,
		 but when $\lambda > 0$, the bias will be non-zero.
		The second main insight is that the risk of the regularized least squares estimate on the original
		 problem is the same as the risk of an equivalent problem which has been \word{rotated} into a basis
		 that is easier to analyze.
			Suppose we rotate each data point $x_i$ by an orthogonal matrix $R \in \R^{d \times d}$ ($x_i \mapsto R^\top x_i$),
			 so that $X \mapsto X R$ for some orthogonal matrix $R$.
			 Correspondingly, we must rotate the parameters $\theta^* \mapsto R^\top \theta^*$.
			 Then the claim is that the excess risk does not change.
			The excess risk of the modified problem is:
			 \eqn{\E[\|X R(R^\top X^\top X R + n \lambda I)^{-1} R^\top X^\top (X R R^\top \theta^* + \epsilon) - X R R^\top \theta^*\|_2^2].}
			 Simplification reveals that we get back exactly the original excess risk:
			 \eqn{\E[\|X (X^\top X + n \lambda I)^{-1} X^\top (X \theta^* + \epsilon) - X\theta^*\|_2^2].}
			 This equivalence holds for any orthogonal matrix $R$.
			If we take the SVD $X = U S V^\top$ and rotate by $R = V$,
			 then we can see that $X^\top X \mapsto (V^\top V S U^\top) (U S V^\top V) = S^2$,
			 which is diagonal.
			 Therefore, for the purposes of analysis, we can assume that $\Sigma$ is diagonal without loss of generality:
			 \eqn{\Sigma = \diag(\tau_1, \dots, \tau_d).}
			 Diagonal covariances have the advantage that we can analyze each component separately,
			 turning matrix computations into independent scalar computations.
		Let us compute the mean of the estimator:
		 \eqn{\bar\theta_j
		 & \eqdef \E[\hat\theta_j] \\
		 &= \E[\Sigma_\lambda^{-1} n^{-1} X^\top (X \theta^* + \epsilon)]_j \aside{expand $Y$} \\
		 &= \E[\Sigma_\lambda^{-1} \Sigma \theta^* + \Sigma_\lambda^{-1} n^{-1} X^\top\epsilon]_j \aside{algebra} \\
		 &= \frac{\tau_j}{\tau_j + \lambda} \theta^*_j \aside{since $\E[\epsilon] = 0$}.
		 }
		 Thus, the expected value of the estimator is the true parameter value $\theta^*$
		 shrunk towards zero by a strength that depends on $\lambda$.
		Compute the squared bias term of the expected risk:
		 \eqn{
		 \text{Bias}^2
		 &= \|\bar\theta - \theta^*\|_\Sigma^2 \\
		 &= \sum_{j=1}^d \tau_j \p{\frac{\tau_j}{\tau_j + \lambda} \theta^*_j - \theta_j^*}^2 \\
		 &= \sum_{j=1}^d \frac{\tau_j \lambda^2 (\theta_j^*)^2}{(\tau_j + \lambda)^2}.
		 }
		 If $\lambda = 0$, then the squared bias is zero as expected.
		 As $\lambda \to \infty$, the squared bias tends to $\|\theta^*\|_2^2$,
		 reflecting the fact that the estimator $\hat\theta \to 0$.
		Compute the variance term of the expected risk:
		 \eqn{
		 \text{Var}
		 &= \E[\|\hat\theta - \bar\theta\|_\Sigma^2] \\
		 &= \E[\|\Sigma_\lambda^{-1} n^{-1} X^\top \epsilon\|_\Sigma^2] \aside{definition of $\hat\theta$} \\
		 &= \inv{n^2} \E[\epsilon^\top X \Sigma_\lambda^{-1} \Sigma \Sigma_\lambda^{-1} X^\top \epsilon] \aside{expand} \\
		 &= \inv{n^2} \tr(\Sigma_\lambda^{-1} \Sigma \Sigma_\lambda^{-1} X^\top \E[\epsilon \epsilon^\top] X) \aside{cyclic trace} \\
		 &= \frac{\sigma^2}{n} \tr(\Sigma_\lambda^{-1} \Sigma \Sigma_\lambda^{-1} \Sigma) \aside{since $\E[\epsilon \epsilon^\top] = \sigma^2 I$} \\
		 &= \frac{\sigma^2}{n} \sum_{j=1}^d \frac{\tau_j^2}{(\tau_j + \lambda)^2} \aside{matrices are diagonal}.
		 }
		 If we didn't regularize, the variance would be $\frac{d \sigma^2}{n}$.
		 Regularization reduces the variance since $\frac{\tau_j^2}{(\tau_j + \lambda)^2} \le 1$.
		Balancing squared bias and variance
			Having computed the squared bias and variance terms,
			 we can now try to choose $\lambda$ to minimize their sum, the expected risk.
			Rather than minimize their sum as a function as $\lambda$, which would be annoying,
			 let us upper bound both terms to get simpler expressions.
			Fact: $(a + b)^2 \ge 2ab$.
			Upper bound the squared bias and variance by replacing the denominators with a smaller quantity:
			 \eqn{
			 \text{Bias}^2 & \le \sum_{j=1}^d \frac{\lambda (\theta_j^*)^2}{2} = \frac{\blue{\lambda} \|\theta^*\|_2^2}{2} \\
			 \text{Var} & \le \sum_{j=1}^d \frac{\tau_j \frac{\sigma^2}{n}}{2 \lambda} = \frac{\tr(\Sigma) \sigma^2}{2 n \blue{\lambda}}.}
			Now we can minimize the sum over the upper bounds with respect to $\lambda$.
			This pattern shows up commonly:
				Suppose we want to minimize $a \lambda + \frac{b}{\lambda}$.
				Then the optimal $\lambda = \sqrt{\frac{b}{a}}$ yields $2\sqrt{ab}$.
			Optimizing yields an bound on the excess risk:
			 \eqnl{ridgeOptRisk}{\boxed{\E[L(\hat\theta) - L(\theta^*)] \le \sqrt{\frac{\|\theta^*\|^2_2 \tr(\Sigma) \sigma^2}{n}},}}
			 with the regularization strength set to:
			 \eqnl{riskOptLambda}{\lambda = \sqrt{\frac{\tr(\Sigma) \sigma^2}{\|\theta^*\|_2^2 n}}.}
		Remarks
			FIGURE: [spectra]
			Let us compare the risk bound for regularized least squares \refeqn{riskOptLambda}
			 with that of ordinary least squares \refeqn{fixedDesignUnreg}, which is $\frac{d\sigma^2}{n}$.
			The main observation is that the bound no longer depends on the dimensionality $d$.
			 Instead, we have a more nuanced dependency in terms of $\tr(\Sigma)$,
			 which can be thought of as the ``true dimensionality'' of the problem.
			 Note that $\tr(\Sigma)$ is the sum of the eigenvalues.
			 If the data can be described (say, via PCA) by a few dimensions,
			 then the excess risk is governed by this effective dimension.
			 The upshot is that we can be in very high dimensions,
			 but so long as we regularize properly, then our risk will be well-behaved.
			On the other hand, what we pay is that the excess risk is now $O(\sqrt{\inv{n}})$,
			 which is slower than the previous rate of $O(\inv{n})$.
			In the random design where $X$ is a random variable,
			 things are more complicated.
			 In that setting, $X^\top X$ would be random and different
			 from the expected covariance $\E[X^\top X]$.
			 We would need to argue that the two converge.
			 See the Hsu/Kakade/Zhang paper for more details.
	Summary \currlecture
		This concludes our unit on asymptotic analysis.
		 The central problem is this: given data, $x^{(1)}, \dots, x^{(n)}$,
		 we get an estimator $\hat\theta$.  How well does this estimator do,
		 either in terms of parameter error for estimation ($\|\hat\theta - \theta^*\|_2^2$)
		 or expected risk for prediction ($L(\theta)$)?
		 We were able to get quantities that depend on the key quantities of interest:
		 number of examples $n$, dimensionality $d$, noise level $\sigma^2$.
		At a high-level, what makes everything work is the central limit theorem.
			This is most obvious for estimating the means of Gaussians and multinomial distributions.
			For exponential families, where the canonical parameters are a non-linear function of the moments,
			 we can apply the delta method to get asymptotic normality of the canonical parameters.
			For general loss functions, we need to apply Slutsky's theorem because the non-linear mapping
			 (governed by $\nabla^2 L(\theta^*)$) is also being estimated.
		We considered estimators based on the method of moments as an alternative to maximum likelihood.
		 For exponential families, these two estimators are the same,
		 but for more complex models such as mixture models, it is possible to get method of moments
		 estimators that are computationally efficient to compute,
		 whereas the maximum likelihood estimator would involve solving a non-convex optimization problem.
		 The high-level point is that the design space of estimators is quite large,
		 and for many problem settings, it's an open question as to what the right estimator to use is.
		 We can evaluate these estimators based on computational efficiency,
		 and asymptotics provides a way to evaluate them based on statistical efficiency.
		Finally, one weakness of the classical asymptotics that we've studied here is that
		 it assumes that the number of examples $n$ is substantially larger than
		 the dimensionality $d$ of the problem.
		 If this is not true, then we need to employ regularization to guard against overfitting.
		 We gave one simple example (fixed design linear regression) where we could obtain a meaningful
		 expected risk even when $d \gg n$, as long as norms were bounded.
		 In the next unit, we will develop the theory of uniform convergence,
		 which will allow us to analyzed regularized estimators much more generally.
	References
		Sham Kakade's statistical learning theory course
			http://stat.wharton.upenn.edu/~skakade/courses/stat928/
		Hsu/Kakade/Zhang, 2014: Random design analysis of ridge regression
			http://arxiv.org/pdf/1106.2363.pdf
		van der Vaart, 2000: Asymptotic Statistics
			http://books.google.ch/books/about/Asymptotic_Statistics.html?id=UEuQEM5RjWgC
		Csiszár/Shields, 2004: Information Theory and Statistics: A Tutorial
			http://www.renyi.hu/~csiszar/Publications/Information_Theory_and_Statistics:_A_Tutorial.pdf
		Anandkumar/Hsu/Kakade, 2012: A Method of Moments for Mixture Models and Hidden Markov Models
			http://arxiv.org/pdf/1203.0683v3.pdf
		Anandkumar/Ge/Hsu/Kakade/Telgarsky, 2012: Tensor Decompositions for Learning Latent Variable Models
			http://arxiv.org/pdf/1210.7559v2.pdf

!verbatim \newpage
Uniform convergence
	!verbatim \lecture{5}
	Overview \currlecture
		The central question is:
		 \begin{quote}
		 \emph{Why does minimizing training error reduce test error?}
		 \end{quote}
		 The answer is not obvious for the training error and test error are
		 two separate quantities which can in general be arbitrarily far apart.
		 This deep question is at the core of statistical learning theory,
		 and answering it reveals what it means to learn.
		In the previous unit on asymptotics,
		 we analyzed the performance of learning algorithms (estimators)
		 in the regime where
		 the number of parameters $d$ is fixed and
		 the number of examples $n$ grows.
		 There are two deficiencies of this analysis:
			It doesn't tell you how large $n$ has to be before the ``asymptotics kick in.''
			 This is problematic in the high-dimensional settings of modern statistics and machine learning,
			 where $d$ is actually fairly large compared to $n$.
			It applies only to unregularized estimators operating on smooth loss functions.
			 However, we want to analyze things like the zero-one loss for classification.
			 We also want to consider different forms of regularization like $L_1$ regularization
			 for performing feature selection.
		 We did analyze regularized least squares at the end of last unit,
		 but that analysis was very specific to linear regression.
		 How can we generalize to other problems and estimators?
		In this unit, we develop a new suite of tools to answer this question.
		 We will shift our focus from asymptotic variance of an estimator
		 to thinking about estimators that choose a predictor from a \word{hypothesis class}.
		 We then study how the \word{empirical risk} (a.k.a. training error, which our estimator is based on)
		 relates to the \word{expected risk} (a.k.a. test error, which we're evaluated on)
		 on this hypothesis class using \word{uniform convergence}.
		 In the process, we will develop some fairly general machinery from probability theory;
		 these tools are more broadly applicable outside machine learning.
		 These techniques are mathematically quite involved,
		 so make sure you have a good understanding of probability!
	Formal setup \currlecture
		In this section, we formalize the (batch) supervised learning setting.
		 Much of what we will do also works for unsupervised learning,
		 but we will describe it in the context of supervised learning to provide intuition.
		Consider the problem of predicting an output $y \in \sY$ given an input $x \in \sX$.
		 Example: $\sX = \R^d$, $\sY = \{ -1, +1 \}$.
		Let $\sH$ be a set of \word{hypotheses}.
		 Usually, each $h \in \sH$ maps $\sX$ to $\sY$.
		 Example: $\sH = \{ x \mapsto \sign(w \cdot x) : w \in \R^d \}$ is all thresholded linear functions.
		Let $\ell : (\sX \times \sY) \times \sH \to \R$ be a \word{loss function}.
		 Example: $\ell((x,y), h) = \1[y \neq h(x)]$ is the zero-one loss.
		Let $p^*$ denote the true underlying data-generating distribution over input-output pairs $\sX \times \sY$.
		\definitionHeading{generalizationError}{expected risk}
			Let $L(h)$ be the \word{expected risk} (test error) of a hypothesis $h \in \sH$,
			 which is the loss that $h$ incurs on a new test example $(x,y)$ in expectation:
			 \eqn{\boxed{L(h) \eqdef \E_{(x,y) \sim p^*}[\ell((x,y), h)].}}
			 Getting low expected risk is in some sense the definition of successful learning.
			Define an \word{expected risk minimizer} $h^*$ to be any hypothesis that minimizes the expected risk:
			 \eqn{h^* \in \arg\min_{h \in \sH} L(h).}
			 This is the thing that we can only aspire to.
			 $L(h^*)$ is the lowest possible expected risk
			 (which might be large if the learning problem is noisy
			 or your hypothesis class is too small).
		To do learning,
		 we are given $n$ \word{training examples}, which are a set of input-output pairs:
		 \eqn{(x^{(1)}, y^{(1)}), \dots, (x^{(n)}, y^{(n)}),}
		 where each $(x^{(i)}, y^{(i)})$ is drawn \textbf{i.i.d.}~from $p^*$.
			Note: the training and test distributions are the same.
			 While this assumption often doesn't hold exactly in practice,
			 the training and test distributions morally have to be related somehow,
			 or else there's no hope that what we learned from training
			 would be useful at test time.\footnote{
			 If the two distributions are different but still related somehow,
			 not all hope is lost; dealing with this discrepancy is called domain adaptation.}
			Note: the independence assumption, which also doesn't hold exactly in practice,
			 ensures that more training data gives us more information
			 (or else we would get the same training example over and over again, which would be useless).\footnote{
			 Pure i.i.d. is not necessary, but certainly some amount of independence is necessary.}
		\definitionHeading{empiricalRisk}{empirical risk}
			Let $\hat L(h)$ be the \word{empirical risk} (training error) of a hypothesis $h \in \sH$
			 as the average loss over the training examples:
			 \eqn{\boxed{\hat L(h) \eqdef \inv{n} \sum_{i=1}^n \ell((x^{(i)}, y^{(i)}), h).}}
			 Note that for a fixed $h$, $\hat L(h)$ is just an empirical average
			 with mean $L(h)$.  This is key.
			Define an \word{empirical risk minimizer} (ERM) be any hypothesis that minimizes the empirical risk:
			 \eqn{\hat h \in \arg\min_{h \in \sH} \hat L(h).}
		Let us pause a moment to remember what are the random variables and what the independence assumptions are:
		 $\hat h$ is a random variable that depends the training examples (in a rather complicated way),
		 but $h^*$ is non-random.
		 The empirical risk $\hat L(h)$ is a random variable for each $h$,
		 but the expected risk $L(h)$ is non-random.
		Recall that we are interested in the expected risk of the ERM:
		 \eqn{L(\hat h).}
		 We will not study this quantity directly, but rather look at its difference with a baseline.
		 There are two questions we can ask:
			How does the expected and empirical risks compare for the ERM?
			 \eqn{\underbrace{L(\hat h)}_\text{expected risk of ERM} - \underbrace{\hat L(\hat h)}_\text{empirical risk of ERM}.}
			How well is ERM doing with respect to the best in the hypothesis class?
			 \eqn{\underbrace{L(\hat h)}_\text{expected risk of ERM} - \underbrace{L(h^*)}_\text{lowest expected risk}.}
			 This is know as as the \word{excess risk}.
		How do we analyze the excess risk?
		 The excess risk is a random quantity that depends on the training examples (through $\hat h$).
		 It is possible that the excess risk is high even for large $n$
		 (for instance, if we just happened to see the same example over and over again).
		 So we can't deterministically bound the excess risk.
		 Note that in asymptotics, we dealt with this via the central limit theorem,
		 but now $n$ is an arbitrarily small finite number.
		Fortunately, we can show that bad outcomes are not too likely.
		 We will prove bounds of the following flavor: With probability at least $1-\delta$,
		 the excess risk is upper bounded by some $\epsilon$ ($L(\hat h) - L(h^*) \le \epsilon$),
		 where $\epsilon$ is generally a function that depends on $\delta$ (and other aspects of the learning problem).
		 More formally, the types of statements we'd like to show can be written compactly as:
		 \eqn{\boxed{\BP[L(\hat h) - L(h^*) > \epsilon] \le \delta.}}
			FIGURE: [distribution over $L(\hat h)$, $\delta$ is area of tail, $\epsilon$ is difference on x-axis from $L(h^*)$]
		 Note that the randomness in the probability is over draws of the
		 $n$ training examples:
		 $(x^{(1)}, y^{(1)}), \dots, (x^{(n)}, y^{(n)}) \sim p^*$.
		It is important to note that there are two sources of randomness at play here:
			Expected risk (upper bounded by $\epsilon$) is defined with respect to randomness over the test example.
			Confidence (lower bounded by $1-\delta$) is defined with respect to randomness over the training examples.
		Probably Approximately Correct (PAC) framework [Leslie Valiant, 1984]
			A learning algorithm $\sA$ PAC learns $\sH$ if for any distribution $p^*$ over $\sX \times \sY$, $\epsilon > 0$, $\delta > 0$,
			 $\sA$ (which takes as input $n$ training examples along with $\epsilon$ and $\delta$),
			 returns $\hat h \in \sH$ such that with probability at least $1-\delta$,
			 $L(\hat h) - L(h^*) \le \epsilon$,
			 and $\sA$ runs in $\text{poly}(n, \text{size}(x), 1/\epsilon, 1/\delta)$ time.
			Remark: time complexity upper bounds sample complexity,
			 because you have to go through all the data points.
			 In this class, we will not focus so much on the computational aspect in our presentation,
			 but just work with the ERM,
			 and assume that it can be optimized efficiently
			 (even though that's not true for general non-convex losses).
			The ideas in PAC learning actually predate Valiant and were studied in the field of empirical process theory,
			 but Valiant and others focused more on more combinatorial problems with computation in mind.
	Realizable finite hypothesis classes \currlecture
		So far, we have presented a framework that is very general,
		 and in fact, so general, that we can't say anything.
		 So we need to make some assumptions.
		 Ideally, the assumptions would be both realistic (not too strong)
		 but still allow us to prove something interesting (not too weak).
		In this section, we will start with an easy case,
		 where we have a finite number of hypotheses,
		 at least one of which has zero expected risk.
		 These assumptions are as formalized as follows:
		\assumptionHeading{finiteHypothesis}{\word{finite hypothesis space}}
			Assume $\sH$ is finite.
		\assumptionHeading{realizable}{\word{realizable}}
			Assume there exists a hypothesis $h^* \in \sH$ that obtains zero expected risk, that is:
		 	 \eqn{L(h^*) = \E_{(x,y) \sim p^*}[\ell((x,y), h^*)] = 0.}
		\theoremHeading{realizableFinite}{realizable finite hypothesis class}
			Let $\sH$ be a hypothesis class, where each hypothesis $h \in \sH$ maps some $\sX$ to $\sY$.
			Let $\ell$ be the zero-one loss: $\ell((x,y), h) = \1[y \neq h(x)]$.
			Let $p^*$ be any distribution over $\sX \times \sY$.
			Assume Assumptions~\ref{ass:finiteHypothesis} and \ref{ass:realizable} hold.
			Let $\hat h$ be the empirical risk minimizer.
			Then the following two equivalent statements hold
			 (each has a different interpretation depending on what we're interested in):
				Interpretation 1: what is the error after training on $n$ examples (\textbf{expected risk})?
				 Answer: with probability at least $1-\delta$,
				 \eqnl{finiteFast}{\boxed{L(\hat h) \le \frac{\log |\sH| + \log (1/\delta)}{n}.}}
				 Usually, think of $\log (1/\delta)$ as a constant (e.g., $\delta = 0.01$, then $\log (1/\delta) \approxeq 4.6$),
				 so the
				 \eqn{\underbrace{L(\hat h)}_{\text{expected risk}} = O\left(\frac{\overbrace{\log |\sH|}^\text{complexity}}{\underbrace{n}_\text{number of training examples}}\right)}
				Interpretation 2: how many examples $n$ (\word{sample complexity})
				 do I need to obtain expected risk at most $\epsilon$
				 with confidence at least $1-\delta$?
				 Answer: With probability at least $1-\delta$:
				 \eqn{\boxed{n \ge \frac{\log |\sH| + \log (1/\delta)}{\epsilon} \quad\Rightarrow\quad L(\hat h) \le \epsilon.}}
		Remarks
			Statisticians generally talk about error,
			 and computer scienists like to talk about sample complexity.
			 But they're just two sides of the same coin.
			In this case, the excess risk behaves as $O(1/n)$,
			 which is known as a ``fast'' rate.
			 This is because we've assumed realizability:
			 as soon as $h$ makes even a single mistake on a training example, we can throw it away.
			The excess risk only grows logarithmically with $|\sH|$, so we can use pretty big hypothesis classes.
			Note that our result is independent of $p^*(x,y)$.
			 This is known as a \word{distribution-free} result, which is great,
			 because typically we don't know what $p^*$ is.
		!comment \exampleHeading{conjunctions}{learning conjunctions}
			Let $\sX = \{0,1\}^d$ (we have $d$ boolean features)
			Let $\sY = \{0,1\}$ (doing binary classification)
			$\sH = \{ f_J : J \subseteq \{1,\dots,d\} \}$ is the set of conjunctive formulas,
			 where $f_J(x) = \prod_{j \in J} x_j$ is a conjunction over the features in $J$.
			Here, $|\sH| = 2^d$, but $\log |\sH| = O(d)$,
			 so we can learn as long as the number of examples is at least the number of features.
		Proof of \refthm{realizableFinite}
			FIGURE: [a row of hypotheses, sorted by increasing $L(h)$]
			We'd like to upper bound the probability of the bad event that $L(\hat h) > \epsilon$.
			Let $B \subseteq \sH$ be the set of bad hypotheses $h$: $B = \{ h \in \sH : L(h) > \epsilon \}$.
			 We can rewrite our goal as upper bounding the probability of selecting a bad hypothesis:
			 \eqn{\BP[L(\hat h) > \epsilon] = \BP[\hat h \in B].}
			Recall that the empirical risk of the ERM is always zero ($\hat L(\hat h) = 0$) because
			 at least $\hat L(h^*) = L(h^*) = 0$.
			 So if we selected a bad hypothesis ($\hat h \in B$),
			 then some bad hypothesis must have zero empirical risk:
			 \eqn{\BP[\hat h \in B] \le \BP[\exists h \in B : \hat L(h) = 0].}
			We now get to the heart of the argument, which consists of two steps.
			Step 1: bound $\BP[\hat L(h) = 0]$ for a \textbf{fixed} $h \in B$.
				On each example, hypothesis $h$ does not err with probability $1 - L(h)$.
				Since the training examples are i.i.d. and the fact that
				 $L(h) > \epsilon$ for $h \in B$:
				 \eqn{\BP[\hat L(h) = 0] = (1 - L(h))^n \le (1 - \epsilon)^n \le e^{-\epsilon n},}
				 where the last step follows since $1-a \le e^{-a}$.
				Remark: this probability \textbf{decreases exponentially} with $n$, which is important.
			Step 2: show that step 1 holds simultaneously for all $h \in B$:
				We apply the \word{union bound} to bound the probability of the event for any $h \in B$:
				 \eqn{\BP{\pb{\exists h \in B : \hat L(h) = 0}} \le \sum_{h \in B} \BP[\hat L(h) = 0].}
				The rest is straightforward:
				 \eqn{\sum_{h \in B} \BP[\hat L(h) = 0]
				 &\le |B| e^{-\epsilon n} \\
				 &\le |\sH| e^{-\epsilon n} \\
				 &\eqdef \delta.}
			Taking logs of the last equality and rearranging:
			 \eqn{\epsilon = \frac{\log |\sH| + \log (1/\delta)}{n}.}
			 The theorem follows by substuting this expression for $\delta$.
	Generalization bounds via uniform convergence \currlecture
		The proof of \refthm{realizableFinite} is elementary but illustrates an important pattern
		 that will recur again in more complex scenarios.
		 At a high level, we are interested in expected risk $L$,
		 but only have access to empirical risk $\hat L$ to choose the ERM $\hat h$.
		 In the proof, we saw two steps:
			Step 1 (convergence): For a \textbf{fixed} $h$, show that $\hat L(h)$ is close to $L(h)$ with high probability.
			 In the above proof, this meant showing that if $L(h) > \epsilon$, then $\hat L(h) = 0$ is unlikely.
			Step 2 (uniform convergence): Show that the above holds simultaneously for \textbf{all} hypotheses $h \in \sH$.
			 In the above proof, this meant using a union bound. 
		 The difference between convergence and uniform convergence is absolutely crucial.
		 It is important to note that $\hat h$ is a random variable that depends on the training examples,
		 so $\hat L(\hat h)$ is not just a sum of i.i.d. variables, so step 1 does not apply directly.
		\refthm{realizableFinite} also made two restrictive assumptions.
			First, there exists a perfect hypothesis (realizability).
			 What happens when the problem is not realizable (all hypotheses make some error)?
			 To answer this, we consider the general problem of convergence of random variables using \word{concentration inequalities}.
			Second, the hypothesis class is finite.
			 What happens when the number of hypotheses is infinite?
			 We can't just apply a union bound any more.
			 To answer this, we need to have more suitable ways of measuring the ``size'' of a set other than cardinality.
			 This leads to \word{Rademacher complexity}, \word{VC dimension}, and \word{covering numbers}
			 as ways for measuring the ``size'' of an infinite set from the point of view of the loss function.
		Breaking free of these restrictive assumptions,
		 we will show how bounding expected risk
		 can be reduced to one of uniform convergence.
		 Recall that our goal is to bound the excess risk,
		 the amount by which ERM's expected risk exceeds the lowest possible expected risk:
		 \eqn{\BP[L(\hat h) - L(h^*) \ge \epsilon] \le \delta.}
		 Note that the difference between $\ge$ and $>$ isn't important here,
		 and it will be more convenient to use $\ge$.
		 \Fig{figures.slides/uniformConvergence}{0.5}{uniformConvergence}{Uniform convergence.}
		Let us first relate expected risk to empirical risk, since that's what the ERM is defined in terms of:
		 \eqn{L(\hat h) - L(h^*) = \underbrace{[L(\blue{\hat h}) - \hat L(\blue{\hat h})]}_\text{concentration} + \underbrace{[\hat L(\hat h) - \hat L(h^*)]}_{\le 0} + \underbrace{[\hat L(h^*) - L(h^*)]}_\text{concentration}.}
		The second term is non-positive by definition of the empirical risk minimizer.
		The third term involves a comparison of $\hat L(h^*)$ and $L(h^*)$.
		 If we expand things, we realize that this is just a question of the difference between an average of $n$ i.i.d.~random variables and its mean:
		 \eqn{\hat L(h^*) = \inv{n} \sum_{i=1}^n \ell((x^{(i)}, y^{(i)}), h^*), \quad L(h^*) = \E_{(x,y) \sim p^*}[\ell((x,y), h^*)].}
		 We'll see how concentration inequalities can be used to control this difference.
		What about the first term?
		 The same reasoning doesn't apply because $\hat h$ depends on the training examples,
		 and so $\hat L(\hat h)$ is not a sum of i.i.d.~random variables!
		 We'll need something something more sophisticated: uniform convergence.
		 Suppose we could ensure that $\hat L(h)$ and $L(h)$ were close (say within $\frac{\epsilon}{2}$) \emph{for all} $h \in \sH$.
		 Then, we could be ensure that $\hat L(\hat h)$ and $L(\hat h)$ were within $\frac{\epsilon}{2}$,
		 as well as $\hat L(h^*)$ and $L(h^*)$.
		The contrapositive can be written formally as:
		 \eqnl{generalizationUniform}{\boxed{\BP[L(\hat h) - L(h^*) \ge \epsilon] \le \BP\left[\blue{\sup_{h \in \sH} |L(h) - \hat L(h)|} \ge \frac{\epsilon}{2} \right].}}
		 On the LHS is a statement about excess risk, and on the RHS is a statement about uniform convergence.
		 The RHS is the probability of the event that the largest difference between the empirical and expected risk is at least $\frac{\epsilon}{2}$,
		 or equivalently, the event that this difference exceeds $\frac{\epsilon}{2}$ for at least one $h \in \sH$.
		Note: the classic example of uniform convergence is the Glivenko-Cantelli theorem
		 (also called the uniform law of large numbers),
		 for estimating distribution functions.
		 Given $x_1, \dots, x_n$ drawn i.i.d.~from some distribution with cumulative distribution function (CDF) $F(x)$,
		 we can form the empirical CDF $F_n(x) = \inv{n} \sum_{i=1}^n \1[x \le x_i]$.
		 One can ask for the convergence of the CDF function in the uniform norm:
		 \eqnl{ulln}{\|F_n - F\|_\infty \eqdef \sup_{x \in \R} |F_n(x) - F(x)| \cvP 0.}
		 What we will be studying is a generalization of \refeqn{ulln},
		 where we have arbitrary hypotheses $h \in \sH$ rather than $x \in \R$.
		Note: if we look at the difference between $\hat L$ and $L$, we can construct
		 something called an \word{empirical process}:
		 \eqn{\{ G_n(h) \}_{h \in \sH}, \quad G_n \eqdef \sqrt{n}(\hat L(h) - L(h)),}
		 which is a stochastic process (collection of random variables indexed by $h \in \sH$).
		 Empirical process theory focuses on studying empirical processes.
		 We know that for a given $h \in \sH$, 
		 $G_n(h)$ converges to a normal distribution by the central limit theorem.
		 We can think about $G_n$ converging to a Gaussian process $G$
		 with covariance function
		 \eqn{\cov[G(h), G(h')] = \cov[\ell(z, h), \ell(z, h')].}
		 This stochastic process viewpoint allows one to talk not just about the supremum $\sup_{h \in \sH} G_n(h)$,
		 but also get distributional results,
		 which is useful for computing confidence intervals.
		 This is outside the scope of the class;
		 for additional information, Pollard has an excellent book on this.
	Concentration inequalities \currlecture
		Concentration inequalities are a very powerful set of techniques from probability theory
		 that shows that an appropriate combination of independent random variables will \emph{concentrate}
		 around its expectation.
		 From the point of view of learning theory,
		 the random variables of interest are the losses of hypotheses on training examples.
		Mean estimation
			Let $X_1, \dots, X_n$ be i.i.d.~real-valued random variables with mean $\mu \eqdef \E[X_1]$
			Define the empirical mean as follows:
			 \eqn{\hat\mu_n \eqdef \inv{n} \sum_{i=1}^n X_i}
			Question: how does $\hat\mu_n$ relate to $\mu$?
			Examples
				$X_i$ is the height of the $i$-th person sampled from some population.
				$X_i$ is the loss of a \emph{fixed} hypothesis $h \in \sH$ on the $i$-th example.
		FIGURE: [$\mu$ with distribution over $\hat\mu_n$]
		Types of statements
			\word{Consistency}: by the law of large numbers, \eqn{\hat\mu_n - \mu \cvP 0,}
			 where $\cvP$ denotes convergence in probability.\footnote{Convergence in probability:
			 For each $\epsilon > 0$,
			 $\BP[|\hat\mu_n - \mu| \ge \epsilon] \cv 0$ as $n \to \infty$.}
			 Consistency assures us that as we get more data ($n \to \infty$),
			 we will approach the correct answer, but it doesn't tell us how quickly.
			\word{Asymptotic normality}: Letting $\var[X_1] = \sigma^2$, by the central limit theorem,
			 \eqn{\sqrt{n} (\hat\mu_n - \mu) \cvd \sN(0, \sigma^2),}
			 where $\cvd$ denotes convergence in distribution.\footnote{Convergence in distribution:
			 For each $t$, $\BP[\frac{\sqrt{n} (\hat\mu_n - \mu)}{\sigma} \le t] \to \Phi(t)$ as $n \to \infty$,
			 where $\Phi$ is the cumulative distribution of the standard Gaussian distribution.}
			 Asymptotic normality says that if $n$ is large enough, then $\hat\mu_n - \mu$ behaves as $\frac{\sigma}{\sqrt{n}})$,
			 where the variance is decreasing at a rate of $1/n$.
			 But this result is only asymptotic, it doesn't tell us anything precise for a particular value of $n$ (say, $n = 10$).
			\word{Tail bounds}: Ideally, we want a statement of the following form:
			 \eqn{\BP[|\hat\mu_n - \mu| \ge \epsilon] \le \SomeFunction(n, \epsilon) = \delta.}
			 Based on the Gaussian approximation, we expect that the bounding function on the RHS would decay double exponentially in $\epsilon$
			 and exponentially in $n$.
			 We shall see shortly that this intuition is indeed true.
			 In the context of learning theory,
			 $\epsilon$ would be the bound on the difference between empirical and expected risks,
			 and $1-\delta$ would be the confidence.
			Note: of course, as we sweep $\epsilon$ from $0$ to $\infty$ and look at how much probability
			 mass is past $\epsilon$, we get a complete picture of the full distribution.
			 However, typically tail bounds are simple upper bounds which are often loose
			 and only become more reliable for small $\epsilon$.
		Our starting point is Markov's inequality,
		 a very simple tool that allows us to control the deviation of a non-negative random variable from its mean
		 using the expectation of that random variable.
		 In short, it turns expectations (which are easier to work with) into tail probabilities (what we want).
		\theoremHeading{markov}{\word{Markov's inequality}}
			Let $Z \ge 0$ be a random variable.
			Then
			 \eqn{\BP[Z \ge t] \le \frac{\E[Z]}{t}.}
		Proof:
			Since $Z$ is non-negative, we have $t \1[Z \ge t] \le Z$.
			Taking expectations on both sides and rearranging completes the proof.
		Remarks
			We can apply $Z = (X - \mu)^2$ (second moment) and $t = \epsilon^2$ to obtain \word{Chebyshev's inequality}:
			 \eqn{\BP[|X - \mu| \ge \epsilon] \le \frac{\var[X]}{\epsilon^2}.}
			 Applying the inequality to the average over i.i.d. variables ($\hat\mu_n = \inv{n} \sum_{i=1}^n X_i$),
			 then $\var[\hat\mu_n] = \frac{\var[X_1]}{n}$.
			 This is a very weak result, because the tail probability is decaying only at at polynomial rate ($1/n$).
			To get stronger bounds, we need to apply Markov's inequality on higher order moments.
			 In particular, we will look at all moments by considering $Z = e^{t X}$,
			 where $t$ is a free parameter we will use later to optimize the bound.
			 ``All the moments'' is captured by the moment generating function:
		\definitionHeading{mgf}{\word{moment generating function}}
			For a random variable $X$, the moment generating function (MGF) of $X$ is:
			 \eqn{\boxed{M_X(t) \eqdef \E[e^{t X}].}}
		One useful way to think about the MGF is in terms of its Taylor expansion:
		 \eqnl{taylorMGF}{M_X(t) = 1 + t \, \E[X] + \frac{t^2}{2} \E[X^2] + \frac{t^3}{6} \E[X^3] + \cdots.}
		The moment generating function receives its name because the $k$-th derivative
		 evaluated at $t=0$ yield the $k$-th moment (assuming we can swap integration and differentiation):
		 \eqn{\frac{d^k M_X(t)}{dt^k}\rvert_{t=0} = \E[X^k].}
		One important property is that the MGF of a sum of independent random variables is simply the product of the MGFs.
		 If $X_1$ and $X_2$ are independent random variables, then we have:
		 \eqn{M_{X_1+X_2}(t) = M_{X_1}(t) M_{X_2}(t).}
		 The distribution over $X_1 + X_2$ can be computed using a convolution,
		 which is typically cumbersome,
		 but MGFs (like Fourier transforms) turn convolutions into products.
		Applying Markov's inequality to $Z = e^{t X}$,
		 we get that
		 \eqnl{mgfBound}{\BP[X \ge \epsilon] \le \frac{M_X(t)}{e^{t \epsilon}} \text{ for all $t > 0$}.}
		 We can apply this to the case of sample means ($X = \hat\mu_n$)
		 by computing $\BP[\hat\mu_n \ge \epsilon] = \BP[X_1 + \cdots + X_n \ge n \epsilon]$.
		 We get that all $t > 0$:
		 \eqnl{mgfBoundn}{\BP[\hat\mu_n \ge \epsilon] \le \p{\frac{M_{X_1}(t)}{e^{t \epsilon}}}^n.}
			Provided that $\frac{M_{X_1}(t)}{e^{t \epsilon}} < 1$ for some $t$,
			 getting $n$ independent samples means that our tail probability will decrease exponentially.
			 This is key.
			Why should we expect this to happen?
			 Assume If $\E[X_1] = 0$.
			 The dominant term in the Taylor expansion of $M_{X_1}(t)$, the numerator of \refeqn{mgfBoundn}, is $1 + O(t^2)$.
			 The dominant term in the Taylor expansion of $e^{t\epsilon}$, the denominator of \refeqn{mgfBoundn}, is $1 + O(t)$.
			 We can always choose $t$ such that the ratio is strictly less than 1 since $t^2 \ll t$ for small enough $t$.
			Note that $M_{X_1}(t)$ could be infinite for some $t$, in which case the bounds are vacuous.
			 In this class, we will work with distributions $X$ such that $M_{X_1}(t) < \infty$ for all $t > 0$.
		Now let's actually compute the MGF of some probability distributions of interest.
		 The Gaussian distribution is a natural place to start, as we will see.
		\exampleHeading{mgfGaussian}{MGF of Gaussian variables}
			Let $X \sim \sN(0, \sigma^2)$.
			Then $\boxed{M_X(t) = e^{\sigma^2 t^2/2}}$.
			Derivation (by completing the square):
			 \eqn{M_X(t) = \E[e^{tX}]
			 &= \int (2\pi\sigma^2)^{-\half} \exp\p{\frac{x^2 - 2 \sigma^2 t x}{-2\sigma^2}} dx \\
			 &= \int (2\pi\sigma^2)^{-\half} \exp\p{\frac{(x - \sigma^2 t)^2 - \sigma^4 t^2}{-2\sigma^2}} dx \\
			 &= \exp\p{\frac{\sigma^2 t^2}{2}}.}
		\lemmaHeading{tailGaussian}{Tail bound for Gaussian variables}
			Having control on the Gaussian MGF, we can now derive a tail bound by plugging the form of the MGF into \refeqn{mgfBound}.
			 This yields:
			 \eqn{\BP[X \ge \epsilon] \le \inf_{t} \exp\p{\frac{\sigma^2 t^2}{2} - t \epsilon}.}
			 The infimum on the RHS is attained by setting $t = \epsilon/\sigma^2$, yielding:
			 \eqnl{tailGaussian}{\boxed{\BP[X \ge \epsilon] \le \exp\p{\frac{-\epsilon^2}{2\sigma^2}}.}}
		What about non-Gaussian variables?  Note that the bounds would still hold if we replaced $M_X(t)$ with an upper bound.
		 This motivates the following definition:
		\definitionHeading{subgaussian}{sub-Gaussian}
			A mean-zero random variable $X$ is \word{sub-Gaussian} with parameter $\sigma^2$ if its moment generating function is bounded as follows:
			 \eqn{\boxed{M_X(t) \,\blue{\le}\, \exp\p{\frac{\sigma^2 t^2}{2}}.}}
			It follows immediately by analogy with \refeqn{tailGaussian} that:
			 \eqnl{tailSubGaussian}{\boxed{\BP[X \ge \epsilon] \le \exp\p{\frac{-\epsilon^2}{2\sigma^2}}.}}
		Examples
			Gaussian random variables: If $X \sim \sN(0, \sigma^2)$, then $X$ is sub-Gaussian with parameter $\sigma^2$ (trivial).
			 Note that the sub-Gaussian parameter and the variance coincide in this case, but this is not true in general.
			Bounded random variables (\word{Hoeffding's lemma}): \label{lem:hoeffding}
			 If $a \le X \le b$ with probability $1$ and $\E[X] = 0$, then $X$ is sub-Gaussian with parameter $(b-a)^2/4$.
				Intuition (not a proof):
				 in the Gaussian case, the sub-Gaussian parameter was the variance.
				 Suppose $a = -b$.  Then the variance of $X$ is $b^2 = (b-a)^2/4$,
				 the sub-Gaussian parameter given by the lemma.
				 In general, the variance is at most the sub-Gaussian parameter.
			Non-examples: exponential and Gamma variables,
			 since these distributions have tails which are too fat,
			 decaying exponentially rather than double exponentially.\footnote{These variables
			 have moment generating functions which are defined not for all $t$ but only small $t$.
			 These are known as sub-exponential variables.
			 This is consistent with the central limit theorem, which states
			 that eventually (large enough $n$, small enough $t$) things behave Gaussian.
			 }
		Properties
			Sum: If $X_1$ and $X_2$ are independent sub-Gaussian variables with parameters $\sigma^2_1$ and $\sigma^2_2$, respectively, then
			 $X_1 + X_2$ is sub-Gaussian with parameter $\sigma^2_1 + \sigma^2_2$.
			Multiplication by a constant: if $X$ is sub-Gaussian with parameter $\sigma^2$,
			 then for any $c > 0$, $c X$ is sub-Gaussian with parameter $c^2 \sigma^2$.
			Not surprisingly, these properties coincide with those of Gaussians.
			Note that the product of two sub-Gaussian variables is in general not sub-Gaussian.
		Given the machinery thus far,
		 we can easily obtain the following classic tail bound for bounded random variables:
		\theoremHeading{hoeffding}{\word{Hoeffding's inequality}}
			Let $X_1, \dots, X_n$ be independent random variables.
			Assume each $X_i$ is bounded: $a_i \le X_i \le b_i$.
			Let $\hat\mu_n = \frac{1}{n} \sum_{i=1}^n X_i$ be the sample mean.
			Then
			 \eqn{\BP[\hat\mu_n \ge \E[\hat\mu_n] + \epsilon] \le \exp \left( \frac{-2 n^2 \epsilon^2}{\sum_{i=1}^n (b_i - a_i)^2} \right).}
			Special case ($a_i = -B, b_i = +B$):
			 \eqn{\BP[\hat\mu_n \ge \E[\hat\mu_n] + \epsilon] \le \exp \left( \frac{-n \epsilon^2}{2 B^2} \right).}
		Proof of \refthm{hoeffding}:
			Using compositional properties of sub-Gaussian,
			 $\hat\mu_n - \E[\hat\mu_n]$ is sub-Gaussian with parameter $\inv{\blue{ n^2}} \sum_{i=1}^n \frac{(b_i - a_i)^2}{4}$.
			Apply \refeqn{tailSubGaussian}.
		Summary so far
			We've shown that Gaussian and bounded random variables are sub-Gaussian and enjoy sharp tail bounds.
			Furthermore, if we have an average of $n$ independent sub-Gaussian variables,
			 then the bound simply gets powered up by $n$.
	!verbatim \lecture{6}
	Finite hypothesis classes \currlecture
		Having a good understanding of basic concentration inequalities,
		 let's put them to use by analyzing the excess risk of the ERM
		 for a finite hypothesis class, where now we do not assume realizability;
		 that is, it is possible that every hypothesis $h \in \sH$ has non-zero loss.
		 The analysis will proceed via the two-step concentration + union bound format.
		\theoremHeading{finiteHypothesis}{finite hypothesis class}
			Let $\sH$ be a hypothesis class, where each hypothesis $h \in \sH$ maps $\sX$ to $\sY$.
			Let $\ell$ be the zero-one loss: $\ell((x,y), h) = \1[y \neq h(x)]$.
			Assume $\sH$ is finite (Assumption~\ref{ass:finiteHypothesis} holds).
			Let $\hat h$ be the empirical risk minimizer.
			Then with probability at least $1-\delta$, the excess risk is bounded as follows:
			 \eqn{L(\hat h) - L(h^*) \le \sqrt{\frac{2(\log |\sH| + \log (2/\delta))}{n}} = O\p{\sqrt{\frac{\log |\sH|}{n}}}.}
		Proof of \refthm{finiteHypothesis}:
			Recall by \refeqn{generalizationUniform},
			 the excess risk $L(\hat h) - L(h^*)$
			 is upper bounded using uniform convergence
			 where we control $\sup_{h \in \sH} |L(h) - \hat L(h)|$.
			We adopt the same high-level strategy as for the realizable case:
				Show convergence of a fixed hypothesis $h \in \sH$ (using Hoeffding's inequality),
				Show uniform convergence using a union bound.
			Step 1 (convergence)
				For a fixed $h \in \sH$, note that $\hat L(h)$ is an empirical average over $n$ i.i.d.~loss terms (bounded in $[0,1]$)
				 with expectation $L(h)$.
				 Therefore, by Hoeffding's inequality,
				 \eqn{\BP[\hat L(h) - L(h) \ge \epsilon] \le \exp\p{-2 n \epsilon^2}.}
				To bound the absolute value, we apply the bound again on the negative loss and combine using the union bound:
				 \eqn{\BP[|\hat L(h) - L(h)| \ge \epsilon] \le 2 \exp\p{-2 n \epsilon^2}.}
			Step 2 (uniform convergence)
				Since $\sH$ is finite, we can apply the union bound over $|\sH|$ hypotheses, obtaining:
				 \eqn{\BP\left[\sup_{h \in \sH} |\hat L(h) - L(h)| \ge \frac{\epsilon}{2}\right] \le \blue{|\sH|} \cdot 2 \exp\p{-2 n \p{\frac{\epsilon}{2}}^2} \eqdef \delta.}
				 Note that we substituted $\frac{\epsilon}{2}$ for $\epsilon$,
				 which is needed by the generalization bound \refeqn{generalizationUniform}.
				Rearranging completes the proof.
		Comparison with the realizable case \refeqn{finiteFast}:
			We still have logarthmic dependence on $|\sH|$ and $1/\delta$.
			The main difference is that the realizable bound had a $\inv{n}$ rate,
			 whereas when there is noise, we now get a $\inv{\sqrt{n}}$ rate,
			 which is due to the use of Hoeffding's inequality.
			 This gap is real and is due to the fact that learning is much easier / faster
			 when there is no noise.
			Note that when we performed asymptotic analysis,
			 we also got a $\inv{n}$ rate and did not assume realizability.
			 However, there were two differences there:
			 we assumed the loss was twice differentiable (in contrast to the zero-one loss here),
			 and we were analyzing the risk near $\theta^*$
			 as opposed to globally across $\sH$.
	Concentration inequalities (continued) \currlecture
		Now that we have relaxed the realizability assumption,
		 let us now try to relax the finiteness one too.
		 To do this, we will develop the theory of Rademacher complexity,
		 but first, we will take a detour to McDiarmid's inequality,
		 which will help us convert the tail bound (RHS of \refeqn{generalizationUniform})
		 into an expectation.
		McDiarmid's inequality is a generalization of Hoeffding's inequality,
		 where we want to bound not the average of random variables $X_1, \dots, X_n$,
		 but any function on $X_1, \dots, X_n$ satisfying an appropriate bounded differences condition.
		\theoremHeading{mcdiarmid}{\word{McDiarmid's inequality} (bounded differences inequality)}
			Let $f$ be a function satisfying the following bounded differences condition:
			 \eqn{|f(x_1, \dots, \blue{x_i}, \dots, x_n) - f(x_1, \dots, \blue{x_i'}, \dots, x_n)| \le c_i}
			 for all $i = 1, \dots, n$ and all $x_1, \dots, x_n, x_i'$.
			 Intuition: modifying one coordinate doesn't change the function value too much.
			Let $X_1, \dots, X_n$ be independent random variables.
			Then
			 \eqn{\BP[f(X_1, \dots, X_n) - \E[f(X_1, \dots, X_n)] \ge \epsilon] \le \exp\p{\frac{-2 \epsilon^2}{\sum_{i=1}^n c_i^2}}.}
		Remarks
			McDiarmid's inequality generalizes Hoeffding's inequality as follows.
			 Define the function $f(x_1, \dots, x_n) = \frac1n \sum_{i=1}^n x_i$, where each $x_i$ satisfies $a_i \le x_i \le b_i$.
			 Then $f$ satisfies the bounded differences condition with $c_i = \inv{n} (b_i - a_i)$.
			 Substituting this value of $c_i$ recovers Hoeffding's inequality exactly.
			This result is quite powerful, as it holds for any independent random variables,
			 and $f$ could be quite complex (e.g., fitting a neural network).
			 As long as the function isn't too sensitive
			 to perturbations in one of its arguments, we get good concentration.
			The proof is not difficult, but it requires introducing martingales,
			 which generalize partial sums of independent random variables.
		\definitionHeading{martingale}{martingale}
			A sequence of random variables $Z_0, Z_1, \dots, Z_n$ is a \word{martingale sequence}
			 with respect to another sequence of random variables $X_1, \dots, X_n$
			 iff $Z_i$ is a function of $X_{1:i}$, $\E[|Z_i|] < \infty$, and
			 \eqnl{martingale}{\E[Z_i \mid X_{1:i-1}] = Z_{i-1}.}
			Define $D_i \eqdef Z_i - Z_{i-1}$.
			 We call $D_{1:n}$ a \word{martingale difference sequence} with respect to $X_{1:n}$.
			 Another way to write \refeqn{martingale} is
			 \eqnl{martingale2}{\E[D_i \mid X_{1:i-1}] = 0.}
		Examples
			Random walk: $Z_0 = 0$, $Z_i = Z_{i-1} + 1$ with probability $\frac12$ and $Z_i = Z_{i-1} - 1$ with probability $\frac12$.
			 This is just a sum of i.i.d. random variables.
			Random walk with absorbing state:
			 same as above but $Z_i = Z_{i-1}$ if $Z_{i-1} = 42$.
		Intuitions
			Given $X_{1:i-1}$ (the past),
			 the expected value of $Z_i$ is the same as $Z_{i-1}$ (i.e., can't predict the future).
			Think of $D_{1:n}$ as a generalization of an i.i.d.~sequence.
		Recall that we showed that sums of independent sub-Gaussian variables are sub-Gaussian.
		 The exact same result holds for martingales, as shown in the following lemma:
		\lemmaHeading{subGaussianMartingale}{\word{sub-Gaussian martingales}}
			Let $Z_0, Z_1, \dots, Z_n$ be a martingale with respect to $X_1, \dots, X_n$.
			Suppose that each difference $D_i = Z_i - Z_{i-1}$ is
			 \emph{conditionally} sub-Gaussian with parameter $\sigma_i^2$, that is:
			 \eqn{\E[e^{t D_i} \mid X_{1:i-1}] \le \exp(\sigma_i^2 t^2/2).}
			Then $Z_n - Z_0 = \sum_{i=1}^n D_i$ is sub-Gaussian with parameter
			 $\sigma^2 \eqdef \sum_{i=1}^n \sigma_i^2$.
		Proof of \reflem{subGaussianMartingale}
			This proof is straightforward by induction on $n$:
			 \eqn{
			 \E[\exp(t(Z_n - Z_0))]
			 & = \E[\exp(t D_n) \exp(t (Z_{n-1} - Z_0))] \\
			 & = \E[\E[\exp(t D_n) \exp(t (Z_{n-1} - Z_0)) \mid X_{1:n-1}]] \\
			 & \le \exp(\sigma_n^2 t^2/2) \E[\exp(t (Z_{n-1} - Z_0))] \\
			 & \le \exp\p{\sum_{i=1}^n \sigma_i^2 t^2/2}.}
			The key is that conditioned on $X_{1:i-1}$, $D_i$ is just a sub-Gaussian variable
			 and $Z_{i-1} - Z_0$ is just a constant.
			 The last inequality follows by repeating the argument recursively.
		Proof of \refthm{mcdiarmid} (McDiarmid's inequality) 
			We construct a particular type of martingle called \word{Doob martingale}:
			 \eqn{Z_i = \E[f(X_1, \dots, X_n) \mid X_{1:i}].}
			 Note the extremes:
			 $Z_0 = \E[f(X_1, \dots, X_n)]$ and $Z_n = f(X_1, \dots, X_n)$.
			 We can think of this martingale as exposing more information about $f(X_1, \dots, X_n)$ over time.
			 Using this notation, we are interested in bounding $\BP[Z_n - Z_0 \ge \epsilon]$.
			To show that we have a sub-Gaussian martingale, let's study $D_i = Z_i - Z_{i-1}$:
			 Both $Z_i$ and $Z_{i-1}$ condition on $X_{1:i-1}$ and take expectations over $X_{i+1:n}$.
			 The only difference is in the treatment of $X_i$:
			 \eqn{
			 Z_{i-1} &= \E[f(\overbrace{X_1, \dots, X_{i-1}}^\text{condition}, \overbrace{\red{X_i}, X_{i+1}, \dots, X_n}^\text{marginalize}) \mid X_{1:i-1}] \\
			 Z_i &= \E[f(\underbrace{X_1, \dots, X_{i-1}, \red{X_i}}_\text{condition}, \underbrace{X_{i+1}, \dots, X_n}_\text{marginalize}) \mid X_{1:i}]
			 }
			 Therefore we would expect that $D_i$ is contained in some interval of length $c_i$.
			 Of course, we need to handle the marginalization over $X_{i+1:n}$ properly.
			To make this precise, define the lower and upper bounds which measure how large $Z_i$ could get:
			 \eqn{
			 L_i &= \inf_x \E[f(X_{1:n}) \mid X_{1:i-1}, X_i = x] - \E[f(X_{1:n}) \mid X_{1:i-1}], \\
			 U_i &= \sup_x \E[f(X_{1:n}) \mid X_{1:i-1}, X_i = x] - \E[f(X_{1:n}) \mid X_{1:i-1}].
			 }
			 Note that $L_i \le D_i \le U_i$.
			Let $x_L$ and $x_U$ correspond to the $x$'s achieving $L_i$ and $U_i$, respectively.
			 By the bounded differences assumption,
			 \eqn{f(X_{1:i-1}, x_U, X_{i+1}) - f(X_{1:i-1}, x_L, X_{i+1}) \le c_i.}
			Now here's the key step:
			 By independence of the $X_i$'s, the distribution over $X_{i+1:n}$ is the same
			 no matter on whether we condition on $X_i = x_L$ or $X_i = x_U$.
			 Therefore, we can take an expectation over $X_{i+1:n}$ to get
			 that
			 \eqn{U_i - L_i &= \E[f(X_{1:i-1}, x_U, X_{i+1}) \mid X_{1:i-1}, X_i = x_U] \\ &\quad - \E[f(X_{1:i-1}, x_L, X_{i+1}) \mid X_{1:i-1}, X_i = x_L] \le c_i.}
			 This means that $D_i \in [L_i, U_i]$ is sub-Gaussian with parameter $c_i^2/4$ conditioned on $X_{1:i-1}$.
			Applying \reflem{subGaussianMartingale}, we have
			 that $Z_n - Z_0$ is sub-Gaussian with parameter $\sum_{i=1}^n c_i^2/4$.
			 Finally, apply the sub-Gaussian tail inequality \refeqn{tailSubGaussian}.
	Rademacher complexity \currlecture
		So far, we've relied on concentration inequalities (Hoeffding's inequality) along with the union bound
		 to derive generalization bounds via uniform convergence.
		 However, we can't directly apply the union bound to infinite hypothesis classes (set of all linear classifiers).
		 We need a more sophisticated way to measure the complexity of a hypothesis class.
		 In this section, we will develop such a framework known as Rademacher complexity,
		 which has many nice properties and connects to other measures of complexity such as VC dimension and covering numbers.
		We will arrive at Rademacher complexity by deriving generalization bounds.
		 Specifically, we carry out the following steps:
			Step 1: Define a random variable $G_n$ (maximum difference between expected and empirical risk).
			Step 2: Show that it concentrates to $\E[G_n]$ using McDiarmid's inequality.
			Step 3: Use a technique called \word{symmetrization} to bound the expectation using
			 a quantity known as the Rademacher complexity.
		Step 1 (setup)
			Consider the largest difference between the expected and the empirical risk over all possible hypotheses:
			 \eqn{\boxed{G_n \eqdef \sup_{h \in \sH} L(h) - \hat L(h).}}
			 Here, $G_n$ is a random variable that depends on the data points $Z_1, \dots, Z_n$.
			 An upper bound on $G_n$ would ensure that if you observed empirical risk $\hat L(\hat h)$,
			 the expected risk $L(\hat h)$ will not be much higher.
			In order to bound the excess risk \refeqn{generalizationUniform},
			 we actually need to also bound $G_n'$ defined analogously for
			 the negative loss $\ell'(z,h) = -\ell(z,h)$, so that
			 \eqn{\BP[L(\hat h) - L(h^*) \ge \epsilon]
			 & \le \BP\pb{\sup_{h \in \sH} |L(h) - \hat L(h)| \ge \frac{\epsilon}{2}} \\
			 & \le \BP\pb{G_n \ge \frac{\epsilon}{2}} + \BP\pb{G_n' \ge \frac{\epsilon}{2}}.}
			 Usually, the tail bound for $G_n$ is the same as for $G_n'$, as we'll see later.
		Step 2 (concentration): convert tail bound into an expectation
			Let $g$ be the deterministic function such that $G_n = g(Z_1, \dots, Z_n)$.
			Then $g$ satisfies the following bounded differences condition:
			 \eqn{|g(Z_1, \dots, \blue{Z_i}, \dots, Z_n) - g(Z_1, \dots, \blue{Z_i'}, \dots, Z_n)| \le \frac{1}{n}.}
			Proof:
				Recall $\hat L(h) = \inv{n} \sum_{i=1}^n \ell(Z_i, h)$.
				We have:
				 \eqn{\Big|\underbrace{\sup_{h \in \sH} [L(h) - \hat L(h)]}_{g(Z_1, \dots, Z_i, \dots, Z_n)} - \underbrace{\sup_{h \in \sH} [L(h) - \hat L(h) + \blue{\frac{1}{n}(\ell(Z_i, h) - \ell(Z_i', h))}]}_{g(Z_1, \dots, Z_i', \dots, Z_n)}\Big| \le \frac{1}{n}.}
				For each $h \in \sH$,
				 the difference between the $G_n$ and the perturbed $G_n$ is at most $\frac{1}{n}$ since the loss is bounded: $\ell(z, h) \in [0,1]$.
				 Taking the supremum (which is a contraction) cannot increase the difference.
			Now we can apply McDiarmid's inequality (\refthm{mcdiarmid}) to get that:
			 \eqnl{Gnbound}{\boxed{\BP\pb{G_n \ge \E[G_n] + \epsilon} \le \exp(-2n\epsilon^2).}}
			Remark: Note that $g$ is quite a non-trivial function (involving a sup over a huge hypothesis class),
			 but because it satisfies the bounded differences condition,
			 we can simply apply McDiarmid's inequality.
		Step 3 (symmetrization)
			Now we need to bound $\E[G_n]$.
			 This quantity is quite difficult since it depends on $L(h)$, an expectation
			 over the unknown distribution $p^*$.
			 The goal of symmetrization is to remove this strong dependence on $p^*$ and
			 replace it with a quantity that \emph{only depends on $p^*$ through data points $Z_1, \dots, Z_n$}.
			The key idea of symmetrization is to introduce a ghost dataset $Z_1', \dots, Z_n'$, drawn i.i.d.~from $p^*$.
			 Let $\hat L'(h) = \inv{n} \sum_{i=1}^n \ell(Z_i', h)$ be the empirical risk with respect to this ghost dataset.
			Rewriting $L(h)$ in terms of the ghost dataset:
			 \eqn{\E[G_n] = \E[\sup_{h \in \sH} \E[\hat L'(h)] - \hat L(h)].}
			Bring $\hat L(h)$ into the inner expectation by conditioning on the original dataset $Z_{1:n}$ (the two datasets are independent):
			 \eqn{\E[G_n] = \E[\sup_{h \in \sH} \E[\hat L'(h) - \hat L(h) \mid Z_{1:n}]].}
			Pushing the $\sup$ inside the expectation can only increase:
			 \eqn{\E[G_n] \le \E[\E[\sup_{h \in \sH} \hat L'(h) - \hat L(h) \mid Z_{1:n}]].}
			Apply law of iterated conditional expectation:
			 \eqn{\E[G_n] \le \E[\sup_{h \in \sH} \hat L'(h) - \hat L(h)].}
			Expanding, we see that we have successfully gotten an expression that depends on $p^*$ through $Z_{1:n}$,
			 but also the ghost dataset $Z_{1:n}'$.
			 \eqn{\E[G_n] \le \E\pb{\sup_{h \in \sH} \inv{n} \sum_{i=1}^n [\ell(Z_i', h) - \ell(Z_i, h)]}.}
			Let's try to remove the dependence on the ghost dataset $Z_{1:n}'$ now.
			 To that end, introduce i.i.d.~\word{Rademacher variables} $\sigma_1, \dots, \sigma_n$ independent of $Z_{1:n},Z_{1:n}'$,
			 where $\sigma_i$ is uniform over $\{-1,+1\}$.
			 Since $\ell(Z_i',h) - \ell(Z_i, h)$ is symmetric around $0$, multiplying by $\sigma_i$ doesn't
			 change its distribution.
			 Now expanding the definition of the empirical risk:
			 \eqn{\E[G_n] \le \E\pb{\sup_{h \in \sH} \inv{n} \sum_{i=1}^n \sigma_i [\ell(Z_i', h) - \ell(Z_i, h)]}.}
			 In fact, this holds for every $\sigma_{1:n}$ and doesn't depend on its distribution.
			Pushing the $\sup$ inside $\sup_h [a_h - b_h] \le \sup_h [a_h] + \sup_h [-b_h]$:
			 \eqn{\E[G_n] \le \E\pb{\sup_{h \in \sH} \inv{n} \sum_{i=1}^n \sigma_i \ell(Z_i', h) + \sup_{h \in \sH} \inv{n} \sum_{i=1}^n (-\sigma_i) \ell(Z_i, h)]}.}
			By linearity of expectation,
			 the fact that $Z_i'$ has the same distribution as $Z_i$,
			 and the fact that $\sigma_i$ and $-\sigma_i$ have the same distribution, we get:
			 \eqnl{GnRn}{\boxed{\E[G_n] \le 2 \E\pb{\sup_{h \in \sH} \inv{n} \sum_{i=1}^n \sigma_i \ell(Z_i, h)}}.}
			 The RHS motivates the following general definition of the Rademacher complexity:
		\definitionHeading{rademacher}{Rademacher complexity}
			Let $\sF$ be a class of real-valued functions $f : \sZ \to \R$.
			 Example: $\sZ = \sX \times \sY$ consists of input-output pairs.
			Define the \word{Rademacher complexity} (or Rademacher average) of $\sF$ to be
			 \eqnl{rademacherComplexity}{\boxed{R_n(\sF) \eqdef \E\pb{\sup_{f \in \sF} \inv{n} \sum_{i=1}^n \sigma_i f(Z_i)},}}
			 where
				$Z_1, \dots, Z_n$ are drawn i.i.d.~from $p^*$ (data points); and
				$\sigma_1, \dots, \sigma_n$ are drawn i.i.d.~from the uniform distribution over $\{-1,+1\}$ (Rademacher variables).
			Interpretation of Rademacher complexity:
				Consider a binary classification problem
				 with inputs $Z_1, \dots, Z_n$ and \emph{random labels} $\sigma_1, \dots, \sigma_n$.
				 Clearly, this is a meaningless learning problem.
				The Rademacher complexity captures (in expectation) how well the best function from the function class $\sF$
				 can align with these random labels.
				 In other words, how well $\sF$ can fit noise?
				 A large $\sF$ will be able to fit noise better and thus have a larger Rademacher complexity.
				As we'll see later, we'd like $R_n(\sF)$ to go to zero as $n$ increases.
			Define the \word{empirical Rademacher complexity} of $\sF$ to be:
			 \eqn{\boxed{\hat R_n(\sF) \eqdef \E\pb{\sup_{f \in \sF} \inv{n} \sum_{i=1}^n \sigma_i f(Z_i) \mid Z_{1:n}},}}
			 which is a random variable depending on the data.
			 Note that $R_n(\sF) = \E[\hat R_n(\sF)]$, where the expectation is taken over the $n$ training examples.
		\theoremHeading{rademacher}{generalization bound based on Rademacher complexity}
			Define $\sA = \{ z \mapsto \ell(z, h) : h \in \sH \}$ to be the \word{loss class},
			 the composition of the loss function with each of the hypotheses.
			 With probability at least $1-\delta$,
			 \eqnl{rademacherBound}{\boxed{L(\hat h) - L(h^*) \le 4 \blue{R_n(\sA)} + \sqrt{\frac{2 \log(2/\delta)}{n}}.}}
		Proof of \refthm{rademacher}:
			Note that $\E[G_n] \le 2 R_n(\sA)$ by definition of Rademacher complexity.
			Since negation doesn't change the Rademacher complexity,
			 $R_n(\sA) = R_n(-\sA)$, so $\E[G_n'] \le 2 R_n(-\sA)$.
			Let's apply the tail bound \refeqn{Gnbound} on both $G_n$ and $G_n'$:
			 \eqn{
			 \BP\pb{G_n \ge \frac{\epsilon}{2}}
			 & \le \exp\p{-2n\p{\frac{\epsilon}{2} - \E[G_n]}^2} \\
			 & \le \exp\p{-2n\p{\frac{\epsilon}{2} - 2 R_n(\sA)}^2} \aside{for $\epsilon \ge 4 R_n(\sA)$} \\
			 & \eqdef \frac{\delta}{2}.}
			 We have an analogous inequality for $G_n'$:
			 \eqn{\BP\pb{G_n' \ge \frac{\epsilon}{2}} \le \frac{\delta}{2}.}
			 Adding them and solving for $\epsilon$ yields the result.
			Remark: We see that the essence of generalization is captured in the Rademacher
			 complexity of the loss class $R_n(\sA)$.
		Summary
			We have reduced the problem of bounding $G_n$,
			 which involved a complex comparison between $L(h)$ and $\hat L(h)$,
			 to something that only depends on samples.  In fact, the empirical Rademacher complexity
			 can even be computed from data.
			 We'll see that in the analysis to follow,
			 we will often condition on $Z_{1:n}$; the points take a backseat as
			 it is the randomness of $\sigma_i$ and the supremum over $\sF$
			 that really determine the Rademacher complexity.
			We will study the Rademacher complexity $R_n(\sF)$ of various function classes $\sF$.
			 First, let us discuss some basic compositional properties that Rademacher complexity enjoys,
			 mostly due to linearity of expectation:
		\textbf{Basic properties of Rademacher complexity}
			Boundedness
				$R_n(\sF) \le \max_{f \in \sF} \max_z f(z)$.
				This is a trivial bound.
				 It's not very impressive to show that the Rademacher complexity is a constant;
				 rather, we'd ideally like it to go to zero as $n \to \infty$.
			Singleton
				$R_n(\{ f \}) = 0$
				Proof: $\sigma_i$ has zero mean and is independent of everything else, so $\E[\sigma_i f(Z_i)] = 0$
			Monotonicity
				$R_n(\sF_1) \le R_n(\sF_2)$ if $\sF_1 \subseteq \sF_2$
				Proof: $\sup_{f \in \sF_2}$ ranges over at least as many functions as $\sup_{f \in \sF_1}$, which makes it at least as large.
			Linear combination
				$R_n(\sF_1 + \sF_2) = R_n(\sF_1) + R_n(\sF_2)$ for $\sF_1 + \sF_2 = \{ f_1 + f_2 : f_1 \in \sF_1, f_2 \in \sF_2 \}$
				Proof: linearity of expectation
			Scaling
				$R_n(c \sF) = |c| R_n(\sF)$
				Proof: for $c > 0$, this is trivial; if $c < 0$,
				 then we can absorbe the negation into $\sigma_i$'s, which are symmetric around zero.
			Lipschitz composition (a generalization of scaling):
				$R_n(\phi \circ \sF) \le c_\phi R_n(\sF)$, where $\phi \circ \sF = \{ z \mapsto \phi(f(z)) : f \in \sF \}$
				 and $c_\phi$ is the Lipschitz constant of $\phi$:
				 $|\phi(z) - \phi(z')| \le c_\phi \|z - z'\|_2$.
				Proof: see Corollary 3.17 of Ledoux and Talagrand 1991
				If $\phi$ is differentiable, then $c_\phi$ is just a bound on the $L_2$ norm of the gradient.
				This property is useful because, as we will see later,
				 we can analyze the complexity of our hypothesis class (e.g., linear functions),
				 and compose with $\phi$ to get the loss class (e.g., 0-1 loss of linear functions).
			Convex hull
				$R_n(\text{convex-hull}(\sF)) = R_n(\sF)$ for finite $\sF$
				Proof: supremum over the convex hull is attained at one of its vertices
				This property is useful because if we want to compute the Rademacher of a polytope (an infinite set),
				 it suffices to compute the Rademacher complexity of its vertices (a finite set).
				 We will use this property when we look at $L_1$ regularization.
	!verbatim \lecture{7}
	Finite hypothesis classes \currlecture
		So far, we've set up Rademacher complexity as a means of bounding the complexity of
		 a function class (specifically, the loss class associated with a hypothesis class).
		 Now, let's instantiate Rademacher complexity for the case where
		 the function class is finite.
		 This may initially seem like overkill (since we already analyzed the finite hypothesis case),
		 but doing this analysis in the Rademacher complexity is a good sanity check,
		 and it will reveal an unexpected but important aspect of finiteness,
		 which we will exploit when we talk about shattering coefficients in the next section.
		As a motivating example of a finite hypothesis class, consider the set of functions that take
		 conjunctions (products) of $k$ of $d$ boolean features:
		 \eqn{\sF = \pc{ z \mapsto \prod_{j \in J} z_j : J \subseteq \{ 1, \dots, d \}, |J| = s }.}
		 Question: what should the Rademacher complexity of $\sF$ be?
		 Well, $|\sF| = \binom{d}{s}$, and the complexity should depend on $\log |\sF| = O(s \log d)$,
		 and also go down as $1/\sqrt{n}$.
		\lemmaHeading{massart}{\word{Massart's finite lemma}}
			Throughout this lemma, condition on $Z_1, \dots, Z_n$ (treat them as constants).
			Let $\sF$ be a finite set of functions.
			Let $M^2$ be a bound on the second moment:
			 \eqn{\sup_{f \in \sF} \inv{n} \sum_{i=1}^n f(Z_i)^2 \le M^2.}
			Then the empirical Rademacher complexity is upper bounded:
			 \eqn{\boxed{\hat R_n(\sF) \le \sqrt{\frac{2 M^2 \log |\sF|}{n}}.}}
		Proof of \reflem{massart}:
			The key idea of the proof is to leverage the moment generating function of an average of i.i.d.~variables.
			Let $W_f = \inv{n} \sum_{i=1}^n \sigma_i f(Z_i)$.
			We are interested in bounding $\hat R_n(\sF) = \E[\sup_{f \in \sF} W_f \mid Z_{1:n}]$.
			Exponentiate and use convexity of $x \mapsto \exp(t x)$ for $t \ge 0$ to push the $\exp$ inside the expectation,
			 which turns it into the moment generating function of $\sup_{f \in \sF} W_f$:
			 \eqn{\exp(t \E[\sup_{f \in \sF} W_f \mid Z_{1:n}]) \le \E[\exp(t \sup_{f \in \sF} W_f) \mid Z_{1:n}].}
			By monotonicity, we can pull the $\sup$ out of the exponent:
			 \eqn{= \E[\sup_{f \in \sF} \exp(t W_f) \mid Z_{1:n}].}
			The $\sup$ over non-negative terms can be upper bounded by the sum:
			 \eqn{\le \sum_{f \in \sF} \E[\exp(t W_f) \mid Z_{1:n}].}
			Now we have to bound $\E[\exp(t W_f)]$, the moment generating function of a single $W_f$.
				By Hoeffding's lemma (after \reflem{hoeffding}), $\sigma_i$ is a bounded random variable with sub-Gaussian parameter $2^2/4 = 1$.
				By scaling and independence of $\sigma_i$, we get that $W_f$ is sub-Gaussian with parameter $\inv{n^2} \sum_{i=1}^n f(Z_i)^2 \le \frac{M^2}{n}$
				 (remember that we're conditioning on $Z_{1:n}$, which are just treated like constants).
				By the definition of sub-Gaussian, we have
				 \eqn{\E[exp(t W_f)] \le \exp\p{\frac{t^2 M^2}{2n}}.}
				 The important thing is that $W_f$ is an average of i.i.d.~variables,
				 so its moment generating function decays exponentially fast.
			Plugging this in to the overall bound and the rest is algebra:
			 \eqn{\exp(t \hat R_n(\sF)) \le \sum_{f \in \sF} \E[\exp(t W_f) \mid Z_{1:n}] \le |\sF| \exp\p{\frac{t^2 M^2}{2 n}}.}
			Taking logs and dividing by $t$:
			 \eqn{\hat R_n(\sF) \le \frac{\log |\sF|}{t} + \frac{t M^2}{2 n}.}
			Optimizing over $t$ yields the result as desired (by just taking the twice the geometric average of $\log |\sF|$ and $M^2/(2n)$):
			 \eqn{\hat R_n(\sF) \le \sqrt{\frac{2 M^2 \log |\sF|}{n}}.}
		We can immediately apply Massart's finite lemma to any finite loss class $\sA$.
		 For example, for the zero-one loss,
		 we have each $f(Z_i) = \ell(Z_i, h) \in [0, 1]$ (for $h \in \sH$ corresponding to $f \in \sF$) so we can take $M = 1$.
		 Combined with \refeqn{rademacherBound}, this gives us
		 %\eqn{\sup_{h \in \sH} L(h) - \hat L(h) \le \sqrt{\frac{8 \log |\sH|}{n}} + \sqrt{\frac{\log(1/\delta)}{2n}} = O\p{\sqrt{\frac{\log |\sH|}{n}}},}
		 \eqn{L(\hat h) - L(h^*) \le \sqrt{\frac{32 \log |\sH|}{n}} + \sqrt{\frac{2 \log(2/\delta)}{n}} = O\p{\sqrt{\frac{\log |\sH|}{n}}},}
		 which gives the same result as \refthm{finiteHypothesis} (just with worse constants).
	Shattering coefficient \currlecture
		What happens if we want to bound a class of half-spaces passing through the origin:
		 \eqn{\sF = \{ z \mapsto \1[w \cdot z \ge 0] : w \in \R^d \}.}
		 What should the complexity of $\sF$ be?
		 Since $\sF$ is infinite, we can't just apply Massart's finite lemma directly.
		 Here's some really sloppy intuition though:
		 consider a 32-bit precision representation of the weight vector $w$;
		 then there are $(2^{32})^d$ possible representations, so we might expect the Rademacher complexity to depend on $d$.
		Before we give up on Massart's finite lemma, let's take a closer look.
		 The lemma places a bound on the empirical Rademacher complexity $\hat R_n$,
		 which only depends on the $n$ data points.
		 If we had an infinite function class $\sF$
		 that acts the same on the $n$ points
		 as a finite function class $\sF'$ (which is allowed to depend on $Z_{1:n}$, which are as constants),
		 then the two function classes have the same empirical Rademacher complexity.
		For example, 
		 consider two points in 2D: $Z_1 = [3, 0], Z_2 = [-3, 0]$.
		 Then
		 \eqn{\sF' = \{ z \mapsto \1[z \ge 0], z \mapsto \1[-z \ge 0] \}}
		 has the same behaviors as $\sF$, namely $[1, 0]$ and $[0, 1]$.
			FIGURE: [draw points with lines]
		More formally, if
		 \eqn{\{ [f(Z_1), \dots, f(Z_n)] : f \in \sF \} = \{ [f'(Z_1), \dots, f'(Z_n)] : f' \in \sF' \},}
		 then $\hat R_n(\sF) = \hat R_n(\sF')$.
		 Therefore, all that matters as far as empirical Rademacher complexity is concerned is
		 \textbf{the behavior of a function class on $n$ data points}.
		 This is the key observation!
		This observation is useful when we are trying to analyze \textbf{boolean functions},
		 those that return either $0$ or $1$ (or any function class with a small finite range).
		 An important example is the loss class $\sA = \{ z \mapsto \ell(z, h) : h \in \sH \}$
		 where $\ell$ is the zero-one loss.
		 In this case, the number of behaviors on $n$ points is certainly finite (and perhaps even small).
		The behavior of $\sF$ on $n$ data points is captured by the shattering coefficient:
		\definitionHeading{shattering}{shattering coefficient (growth function)}
			Let $\sF$ be a family of functions that map $\sZ$ to a finite set (usually $\{0,1\}$).
			The \word{shattering coefficient} of $\sF$ is the maximum number of
			 behaviors over $n$ points:
			 \eqnl{shatter}{s(\sF, n) \eqdef \max_{z_1, \dots, z_n \in \sZ} |\{ [f(z_1), \dots, f(z_n)] : f \in \sF \}|.}
		We can use Massart's finite lemma to upper bound the empirical Rademacher complexity by the shattering coefficient.
		 If $\sF$ contains boolean functions, we have the bound $M=1$, so that:
		 \eqnl{rademacherShatter}{\boxed{\hat R_n(\sF) \le \sqrt{\frac{2 \log s(\sF, n)}{n}}.}}
		 The significance is that we can apply Massart's finite lemma to
		 \textbf{infinite function classes with finite shattering coefficient}.
		 In order to have applied this bound, it is important to condition on the data
		 (note that the distribution over the data has disappeared completely!)
		 After applying Massart's finite lemma, we can take expectations over the data without changing the bound.
		 We have turned an annoying expectation over $p^*$ into a purely combinatorial notion of complexity.
		Intuition:
			For boolean functions,
			 if $s(\sF, n) = 2^n$ (we obtain all possible labelings), then we say $\sF$ \word{shatters}
			 any $n$ points $z_1, \dots, z_n$ that achieve the maximum of \refeqn{shatter}.
			To get meaningful bounds, we want $s(\sF, n)$ to grow sub-exponentially with $n$;
			 otherwise the Rademacher complexity will not go to zero,
			 and we will not obtain uniform convergence.
			 This is expected since if $\sF$ can really hit all labelings for all $n$,
			 then we would be able to fit any labeling of the data, leading to massive overfitting.
			Example: consider $n$ points in 1D.
			 The class $\sF = \{ z \mapsto \1[z \ge t] : t \in \R \}$ has shattering coefficient $s(\sF, n) = n + 1$.
		Shattering coefficient of hypotheses class and loss class
			The bounds we derive depend on the shattering coefficient $s(\sA, n)$ of the loss class $\sA$
			 (via \refeqn{rademacherShatter} and \refeqn{rademacherBound}).
			 However, it will be more intuitive to talk about the shattering coefficient hypothesis class $\sH$.
			 We now show that the two are the same for zero-one loss and binary classifiers.
			Let $\sH$ be a set of binary classifiers $h : \sX \to \{0,1\}$
			 and zero-one loss $\ell((x,y), h) = \1[y \neq h(x)]$.
			Note that the hypothesis class $\sH$ contains functions on $\sX$,
			 and the loss class $\sA$ contains functions on $\sX \times \{0,1\}$:
			 $\sA = \{ (x,y) \mapsto \1[y \neq h(x)] : h \in \sH \}$.
			The key point is that
			 \eqnl{shatterHAequal}{\boxed{s(\sH, n) = s(\sA, n).}}
			The reason is as follows:
			 for any $(x_1,y_1),\dots,(x_n,y_n)$,
			 there is a bijection between the behavior of the losses and the hypotheses:
			 \eqnl{shatteringLoss}{[\ell((x_1,y_1),h), \dots, \ell((x_n,y_n),h)] \quad \Leftrightarrow \quad [h(x_1), \dots, h(x_n)],}
			 where the translation between the two sets of vectors
			 is obtained by taking the XOR with $[y_1, \dots, y_n]$.
			 Therefore, as we range over $h \in \sH$, we obtain the same
			 number of behaviors from $\ell$ as we do from $h$.
	VC dimension \currlecture
		In the previous section, we saw that the shattering coefficient can be used to upper bound the Rademacher complexity
		 of a family of boolean functions
		 \refeqn{rademacherShatter}, which in turn can be used to upper bound the excess risk (\refthm{rademacher}).
		 Although the shattering coefficient nicely captures the behavior of an infinite $\sH$,
		 it is not necessarily the most convenient quantity to get a handle on.
		 In this section, we will use a concept called VC dimension to gain more intuition about the shattering coefficient.
		\definitionHeading{vc}{VC dimension}
			The \word{VC dimension} of a family of functions $\sH$ with boolean outputs
			 is the maximum number of points that can be shattered by $\sH$:
			 \eqn{\boxed{\VC(\sH) = \sup \{ n : s(\sH, n) = 2^n \}.}}
			Intuition: the VC dimension of $\sH$ is the maximum number of points whose labels can be memorized perfectly
			 by choosing some $h \in \sH$.
		Example (intervals)
			Let $\sH = \{ z \mapsto \1[z \in [a, b]] : a, b \in \R \}$.
			$s(\sH, 1) = 2 = 2^1$ (can shatter)
			$s(\sH, 2) = 4 = 2^2$ (can shatter)
			$s(\sH, 3) = 7 < 2^3$ (can't shatter, because can't isolate the middle point)
			$s(\sH, n) = \binom{n+1}{2} + 1$
			Therefore, $\VC(\sH) = 2$.
		Important note: to show that a class $\sH$ has VC dimension $d$, one needs to
			Derive an upper bound: show that \emph{no} $d+1$ points can be shattered.
			 This is done by showing that for \emph{any} $d+1$ points,
			 there is some labeling that cannot be achieved by $\sH$.
			Derive a lower bound: show that there exists $d$ points can be shattered.
			 This is done by showing that there \emph{exists} $d$ points such
			 that \emph{all} $2^d$ labelings can be achieved by $\sH$.
		One can verify that that the VC dimension of rectangles in two dimensions is 4.
		 Based on this, one might be tempted to conclude that the VC dimension of a hypothesis class with $d$ parameters is $d$,
		 but the following example shows that this is in general not the case (there are pathological examples).
		Example (infinite VC dimension with one parameter)
			Let $\sH = \{ x \mapsto \1[\sin(\theta x) \ge 0] : \theta \in \R \}$.
			We have that $\VC(\sH) = \infty$.
		It turns out that it's not the number of parameters that matter, but the dimension of the function space:
		\theoremHeading{finiteVC}{finite-dimensional function class}
			Let $\sF$ be a function class containing functions $f : \sX \to \R$.
			Recall that the dimension of $\sF$ is the number of elements in a basis of $\sF$.
			 For example, if $\sF = \{ x \mapsto w \cdot x : w \in \R^d \}$, then $\dim(\sF) = d$.
			Let $\sH = \{ x \mapsto \1[f(x) \ge 0] : f \in \sF \}$ (for example, $\sF$ could be linear functions, but not necessarily).
			Then we have
			 \eqnl{vc}{\boxed{\VC(\sH) \le \dim(\sF).}}
			Remark: this allows us to connect the linear algebraic properties of $\sF$ (dimension)
			 with the combinatorial properties of $\sH$ (VC dimension).
		Proof of \refthm{finiteVC}:
			Take any $n$ points $x_1, \dots, x_n$ with $n > \dim(\sF)$.
			 We will show that these $n$ points cannot be shattered.
			 The key is to find some direction that $\sF$ can't cover.
			Consider the linear map $M(f) \eqdef [f(x_1), \dots, f(x_n)] \in \R^n$ which
			 maps each function $f \in \sF$ to the function values on the $n$ points
			 (function evaluation is linear).
			The vector space $\{ M(f) : f \in \sF \} \in \R^n$ has dimension at most $\dim(\sF)$
			 (applying linear maps can't increase dimension).
			Since $n > \dim(\sF)$, there is some non-zero vector $c \in \R^n$ such that $M(f) \cdot c = 0$ for all $f \in \sF$.
			Without loss of generality, some component of $c$ is negative (otherwise, just take $-c$, which satisfies $M(f) \cdot c = 0$ too).
			So we have for all $f \in \sF$:
			 \eqnl{vcPartition}{\sum_{i : c_i \ge 0} c_i f(x_i) + \sum_{i : c_i < 0} c_i f(x_i) = 0.}
			For the purposes of contradiction, suppose $\sH$ shatters $\{x_1, \dots, x_n\}$.
				Then we could find an $h = (x \mapsto \1[f(x) \ge 0]) \in \sH$ (equivalently some $f \in \sF$) such that
					$h(x_i) = 1$ ($f(x_i) \ge 0$) whenever $c_i \ge 0$ (so the first term of \refeqn{vcPartition} is non-negative)
					$h(x_i) = 0$ ($f(x_i) < 0$) whenever $c_i < 0$ (so the second term of \refeqn{vcPartition} is strictly positive).
				The sum of the two terms couldn't equal zero, which contradicts \refeqn{vcPartition}.
			Therefore, $\sH$ can't shatter $\{x_1, \dots, x_n\}$ for any choice of $x_1, \dots, x_n$,
			 so $\VC(\sH) \le \dim(\sF)$.
		 Here is the classic application of \refthm{finiteVC}:
		\exampleHeading{halfSpacesVC}{Half-spaces passing through the origin}
			Let $\sH = \{ x \mapsto \1[w \cdot x \ge 0] : w \in \R^d \}$ be the set of half-spaces in $d$ dimensions.
			By \refthm{finiteVC}, the VC dimension of $\sH$ is at most $d$
			 ($\sH$ is backed by a linear function class of dimension $d$).
			Upper bounds suffice for obtaining generalization bounds,
			 but just for fun, let's get a lower bound on the VC dimension.
			Showing that the VC dimension of some $\sH$ is at least $d$ is easier
			 because we just have to construct some set of $d$ points that can be shattered
			 rather than showing no $d+1$ points can be shattered.
			Create $d$ points which are the basis vectors:
			 \begin{align}
			 x_1 &= [1, 0, \dots, 0, 0] \\
			 & \cdots \\
			 x_d &= [0, 0, \dots, 0, 1]
			 \end{align}
			Given any subset $I \subseteq \{1, \dots, d\}$ (which defines a labeling of $x_1, \dots, x_d$),
			 we can construct a $w$ as follows to obtain that labeling:
				Set $w_i = 1$ for $i \in I$
				Set $w_i = -1$ for $i \not\in I$
			This establishes that $\VC(\sH) \ge d$.
			Putting the upper and lower bounds, we get that $\boxed{\VC(\sH) = d}$.
		We have defined VC dimension in terms of shattering coefficient \refeqn{vc},
		 and have given some examples of VC dimension.
		 Let us now upper bound the shattering coefficient in terms of the VC dimension.
		\lemmaHeading{sauer}{\word{Sauer's lemma}}
			For a class $\sH$ be a class with VC dimension $d$.
			Then
			 \eqnl{sauer}{\boxed{s(\sH, n) \le \sum_{i=0}^d \binom{n}{i} \le \begin{cases} 2^n & \text{if $n \le d$} \\ \p{\frac{en}{d}}^d & \text{if $n > d$}. \end{cases}}}
		Intuition
			For $n \le d$, the shattering coefficient grows exponentially in $n$.
			For $n > d$, the shattering coefficient grows only polynomially in $n$.
			In some sense, $\sH$ can only represent any subset of up to $d$ of the $n$ points.
		The ramification of Sauer's lemma is that now we have a new upper bound on the Rademacher complexity
		 (and thus on uniform convergence):
		 \eqn{\hat R_n(\sA) \le \sqrt{\frac{2 \log s(\sA, n)}{n}} = \sqrt{\frac{2 \log s(\sH, n)}{n}} \le \boxed{\sqrt{\frac{2 \VC(\sH) (\log n + 1)}{n}}},}
		 where the first inequality follows from \refeqn{rademacherShatter},
		 the second equality follows from \refeqn{shatterHAequal},
		 and the final inequality follows from \refeqn{sauer} and holds for $n \ge d$.
		Proof of \reflem{sauer} (somewhat technical and specific, skipped in class)
			The idea is to take $\sH$ and transform it into $\sH'$
			 with the same shattering coefficient ($s(\sH, n) = s(\sH', n)$)
			 but where $s(\sH', n)$ will be easier to bound.
			Key: all that matters is the action of $\sH$ and $\sH'$ on a finite set of $n$ points,
			 which we will represent as a table.
			Draw a table whose columns are the $n$ points and rows are the $s(\sH, n)$ possible labelings,
			 and each entry is either a $0$ or $1$.  The question is how many rows there are.
			Here's an example table $T$:
			 \begin{tabular}{|cccc|} \hline
			 $x_1$ & $x_2$ & $x_3$ & $x_4$ \\ \hline
			 0 & 1 & 0 & 1 \\
			 0 & 0 & 0 & 1 \\
			 1 & 1 & 1 & 0 \\
			 1 & 0 & 1 & 0 \\
			 \hline
			 \end{tabular}
			We will transform the table to a canonical form as follows:
				Pick a column $j$.
				For each row $r$ with $r_j = 1$, set $r_j = 0$ if the resulting $r$ doesn't exist in the table.
				Repeat until no more changes are possible.
			Here is the resulting table $T'$ (corresponding to some $\sH'$):
			 \begin{tabular}{|cccc|} \hline
			 $x_1$ & $x_2$ & $x_3$ & $x_4$ \\ \hline
			 0 & 1 & 0 & 1 \\
			 0 & 0 & 0 & 1 \\
			 0 & 1 & 0 & 0 \\
			 0 & 0 & 0 & 0 \\
			 \hline
			 \end{tabular}
			Step 1: Note that the number of rows is still the same and all the rows are all distinct,
			 so $s(\sH, n) = s(\sH', n)$.  So we just have to compute $s(\sH', n)$, which should be easier.
			Step 2: We show that the VC dimension doesn't increase by transformation ($\VC(\sH') \le \VC(\sH)$)
				The transformations proceed one column at a time:
				 \eqn{T \rightarrow T_1 \rightarrow \cdots T_k \stackrel{\text{transform column $j$}}{\rightarrow} T_{k+1} \rightarrow \cdot \rightarrow T'.}
				Claim: After transforming any column $j$, if some subset $S \subseteq \{1, \dots, n\}$ of points
				 is shattered (all $2^{|S|}$ labelings exist on those columns) after transformation (in $T_{k+1}$),
				 then $S$ was also shattered before transformation (in $T_k$).
				Case 1: trivially true for all subsets $S$ that don't contain $j$.
				Case 2: take any subset $S$ that contains $j$.
					For any row $i$ with $1$ in column $j$, there is a row $i'$ with $0$ in column $j$ and agrees with $r$ on all columns except $j$:
					 $T_{k+1}(i,j') = T_{k+1}(i',j')$ for all $j' \in \{1, \dots, n\} \backslash \{j\}$, but $T_{k+1}(i,j) = 1$ and $T_{k+1}(i',j) = 0$.
					Note that $T_k(i,j) = 1$ (because we never turn zeros into ones).
					Note that $T_k(i',j) = 0$ because if it had been a $1$, then rows $i$ and $i'$ would have been identical,
					 and we maintain the invariant that there are no duplicate rows.
				So all $2^{|S|}$ labelings on $S$ existed before transformation in $T_k$.
			Step 3: Each row of $T'$ must contain at most $d$ ones.
				Suppose if $T'$ has a row with $k$ ones in columns $S \subseteq \{1, \dots, n \}$.
				Then for each $j \in S$, there must be another row with a labeling that assigns ones to exactly $S \subseteq \{j\}$
				 (otherwise we would have been able to transform column $j$ by changing the $1$ to a $0$).
				Reasoning recursively, all $2^k$ subsets must exist.
				Since $T'$ has VC dimension at most $d$, $k \le d$.
				Based on simple counting,
				 we find that number of rows (remember, they're all distinct!)
				 is upper bounded by $\sum_{i=0}^d \binom{n}{i}$,
				 completing the first inequality of \refeqn{sauer}.
			Finishing the second part of the inequality of \refeqn{sauer} is just algebra.
			 Observe that for $n \ge d$,
			 \eqn{
			 \sum_{i=0}^d \binom{n}{i}
			 &\le \p{\frac{n}{d}}^d \sum_{i=0}^d \binom{n}{i} \p{\frac{d}{n}}^i \\
			 &\le \p{\frac{n}{d}}^d \sum_{i=0}^n \binom{n}{i} \p{\frac{d}{n}}^i \\
			 &= \p{\frac{n}{d}}^d \p{1 + \frac{d}{n}}^n \\
			 &\le \p{\frac{n}{d}}^d e^{d}.
			 }
	Norm-constrained hypothesis classes \currlecture
		We started by establishing Rademacher complexity $R_n(\sF)$ as a measure of complexity of a function class
		 that yielded generalization bounds when applied to the loss class $\sA$.
		 Then we specialized to boolean functions and zero-one losses,
		 which led to notions of shattering coefficients and VC dimension as combinatorial means
		 of bounding the Rademacher complexity.
		However, combinatorial notions are not the most appropriate way to think about complexity.
		 For example, it is common in machine learning to have a huge number of features (large $d$)
		 in a linear classifier, but where we regularize or constrain the norm of the weights, as in an SVM:
		 \eqn{\hat w = \arg\min_{w \in \R^d : \|w\|_2 \le B_2} \inv{n} \sum_{i=1}^n \max\{0, 1 - y^{(i)} w \cdot x^{(i)}\}.}
		 In this case, $d$ is the VC dimension doesn't really capture the true complexity
		 of the hypothesis class.
		 Somehow $B_2$ should play a role, no?
		Deriving Rademacher complexity will actually be easier for the norm-constrained setting.
		 We will study the Rademacher complexity of three such examples:
			Linear functions with $L_2$ norm constraints (suitable for kernel methods)
			Linear functions with $L_1$ norm constraints (suitable for sparsity)
			!comment Neural networks (shows that we can handle non-convex functions) - skipped in class
		Rademacher complexity of linear functions with weights bounded in an $L_2$ ball
			\theoremHeading{rademacherL2}{\textbf{Rademacher complexity of $L_2$ ball}}
				Let $\sF = \{ z \mapsto w \cdot z : \|w\|_2 \le B_2 \}$ (bound on weight vectors).
				Assume $\E_{Z \sim p^*}[\|Z\|_2^2] \le C_2^2$ (bound on spread of data points).
				Then
				 \eqn{\boxed{R_n(\sF) \le \frac{B_2 C_2}{\sqrt{n}}.}}
			Proof of \refthm{rademacherL2}:
				The key idea is to exploit the linear algebraic structure of Rademacher complexity.
				Expand the definition:
				 \eqnl{wZi}{R_n(\sF) = \inv{n} \E\pb{\sup_{\|w\|_2 \le B_2} \sum_{i=1}^n \sigma_i (w \cdot Z_i)}.}
				By Cauchy-Schwartz applied to $w$ and $\sum_{i=1}^n \sigma_i Z_i$:
				 \eqn{\le \frac{B_2}{n} \E\pb{\|\sum_{i=1}^n \sigma_i Z_i\|_2}.}
				By concavity of $\sqrt{\cdot}$, we can push it outside the expectation:
				 \eqn{\le \frac{B_2}{n} \sqrt{\E\pb{\|\sum_{i=1}^n \sigma_i Z_i\|_2^2}}.}
				Distribute the sum; \textbf{expectation of cross terms is zero} by independence of $\sigma_i$
				 (this is the key point, which turns $n^2$ terms into $n$ terms):
				 \eqn{= \frac{B_2}{n} \sqrt{\E\pb{\sum_{i=1}^n \|\sigma_i Z_i\|_2^2}}.}
				We can drop $\sigma_i$ because it changes sign, not magnitude:
				 \eqn{= \frac{B_2}{n} \sqrt{\E\pb{\sum_{i=1}^n \|Z_i\|_2^2}}.}
				Use the bound on $Z_i$:
				 \eqn{\le \frac{B_2}{n} \sqrt{n C_2^2}.}
				 Simple algebra completes the proof.
		Rademacher complexity of linear functions with weights bounded in an $L_1$ ball
			Motivation
				Working with $L_2$ regularization has the advantage that we can use kernel methods,
				 and the dimensionality could be infinite as long the norm is bounded.
				In some applications, we have a finite but large set of features,
				 and we believe that there are a relatively small subset that are relevant to our task
				 (i.e., we believe in \textbf{parameter sparsity}).
				 It is common to use $L_1$ regularization, or similarily, assume that the weights satisfy $\|w\|_1 \le B_1$.
			 Let us compute the Rademacher complexity of $\sF = \{ z \mapsto w \cdot z : \|w\|_1 \le B_1 \}$.
			\theoremHeading{rademacherL1}{\textbf{Rademacher complexity of $L_1$ ball}}
				Assume that the coordinates are bounded: $\|Z_i\|_\infty \le C_\infty$ with probability $1$ for all data points $i = 1, \dots, n$.
				Then
				 \eqn{\boxed{R_n(\sF) \le \frac{B_1 C_\infty \sqrt{2 \log(2 d)}}{\sqrt{n}}.}}
			Proof of \refthm{rademacherL1}
				The key step is to realize that the $L_1$ ball ($\{ w : \|w\|_1 \le B_1 \}$) is the convex hull of the following $2d$ weight vectors:
				 \eqn{W = \cup_{j=1}^d \{ B_1 e_j, -B_1 e_j \}.}
				 Since the Rademacher complexity of a class is the same as the Rademacher complexity of its convex hull,
				 we just need to look at the finite class:
				 \eqn{R_n(\sF) = \E\pb{\sup_{w \in W} \inv{n} \sum_{i=1}^n \sigma_i (w \cdot Z_i)}.}
				Apply Hölder's inequality, we have $w \cdot Z_i \le \|w\|_1 \|Z_i\|_\infty \le B_1 C_\infty$.
				Applying Massart's finite lemma (\reflem{massart}) to the function class specified by $W$ (of size $2d$)
				 with $M^2 = B_1^2 C_\infty^2$, we get that
				 \eqn{R_n(\sF) \le \sqrt{\frac{2 B_1^2 C_\infty^2 \log (2d)}{n}}.}
			Remarks
				It is useful to recall that as $p$ increases,
					$p$-norms decrease: $\|w\|_p \ge \|w\|_q$
					Size of balls increase: $\{ w : \|w\|_p \le B \} \subseteq \{ w : \|w\|_q \le B \}$
				Note that a $L_1$ bound on the parameters is placing a much stronger constraint than the $L_2$ norm,
				 which allows us to measure the $L_\infty$ norm of the data (which is much better) rather than
				 the $L_2$ norm at the expense of a logarithmic dependence on $d$.
			Ramifications under \word{sparsity}
				FIGURE: [$w$ and $z$ as arrays]
				$L_1$ regularization is often used when we believe that most features are irrelevant;
				 formally, that the desired weight vector has $s \ll d$ non-zero entries.
				You might have seen the intuition that $L_1$ regularization has sharp corners which encourage weights to be identically zero,
				 but that doesn't directly tell us anything about generalization.
				 We would like a stronger justification.
				For convenience, assume that all entries of $w$ and $x$ have magnitude at most $1$ ($\|w\|_\infty \le 1, \|x\|_\infty \le 1$).
				It suffices to consider the hypothesis class $\|w\|_1 \le B_1 = s$.
				Then the Rademacher complexity (and thus the expected risk) is $O\p{\frac{s \sqrt{\blue{\log d}}}{\sqrt{n}}}$.
				Interpretation: essentially the number of relevant features ($s$) controls
				 the complexity, and we can have a ton of irrelevant features (an exponentially large number).
				In contrast, if we use $L_2$ regularization, we would have $B_2 = \sqrt{s}$ and $C_2 = \sqrt{d}$.
				 The Rademacher complexity of $\{ w : \|w\|_2 \le B_2 \}$ is $O\p{\frac{s\sqrt{\blue{d/s}}}{\sqrt{n}}}$.
				When $s \ll d$, then $L_1$ regularization is desirable,
				 but if $s = d$, then $L_1$ regularization is worse by a factor of $\sqrt{\log d}$. 
				Note that these are heuristic arguments since we are comparing upper bounds.
				 In this case, the heuristics are accurate, but in general, one should exercise caution.
			In general, these norm-constrained bounds either do not depend ($L_2$ constrained) or only depend weakly ($L_1$ constrained)
			 on the dimensionality $d$.
			 This supports the prevailing wisdom that it doesn't matter how many features you have (how big $d$ is).
			 As long as you regularize properly (constrain the norm of the weights),
			 then you will still have good generalization.
		From hypothesis class to loss class (for binary classification)
			Composition
				So far, we have bounded the Rademacher complexity of various hypothesis classes $\sF$.
				 We still need to turn that into a bound on the loss class $\sA$.
				 For boolean functions with finite shattering coefficients,
				 we showed that $\sF$ and $\sA$ had the same complexity \refeqn{shatteringLoss}.
				 But now $\sF$ contains real-valued functions,
				 so the previous argument for shattering coefficients doesn't hold.
				Consider a loss function $\phi$ for binary linear classification
				 that only depends on the margin $m = yx \cdot w$.
				 For example,
					Zero-one loss: $\phi(m) = \1[m \le 0]$.
					Hinge loss: $\phi(m) = \max\{ 0, 1 - m \}$.
					FIGURE: [plot these loss functions]
				 Let $w \in W$ be a set of weight vectors (for example, $\{ w : \|w\|_2 \le B_2 \}$).
				 The loss class corresponding to these weight vectors is then:
				 \eqn{\sA = \{ (x,y) \mapsto \phi(yx \cdot w) : w \in W \}.}
				Since the loss function only depends on $(x,y)$ through their product,
				 we can equivalently think about our data points as simply being $z = xy$.
				 Recall the function class that we've bounded the Rademacher complexity for:
				 \eqn{\sF = \{ z \mapsto w \cdot z : w \in W \}.}
				 So therefore, we can rewrite the loss class as a composition:
				 \eqn{\sA = \phi \circ \sF.}
				Note that we can only apply the composition rule for Rademacher complexity
				 to $\phi$ which are Lipschitz.
				 The hinge loss is $1$-Lipschitz, which means the Rademacher bounds for the function class $\sF$ directly
				 carry over to the loss class: $R_n(\sA) = R_n(\sF)$.
			The zero-one loss is not Lipschitz,
			 so we cannot cannot directly control the zero-one loss.
			 Indeed, the zero-one loss is not sensitive to the the norm of $w$,
			 so one should not expect a norm-based bound to work.
			 Working with norms allows us to go to infinite dimensions, but this is the price we pay.
			There is still a weaker statement that we can make, however.
			 Let us define a margin-sensitive version of the zero-one loss
			 which penalizes us whenever we don't predict correctly
			 by at least a margin of $\gamma$:
			 \eqn{\phi_\gamma(m) = \1[m \le \gamma]}
			 and
			 \eqn{L_\gamma(w) = \E_{z \sim p^*}[\phi_\gamma(w \cdot z)]}
			 be the associated expected risk.
			\theoremHeading{marginloss}{margin-sensitive zero-one loss for linear classifiers}
				Let $\sF$ be a set of linear functions.
				Let $L_\gamma$ be the expected risk under the margin-sensitive zero-one loss.
				Let $\hat w$ and $w^*$ be the weight vectors associated with the empirical and expected risk minimizers
				 with respect to the loss $\tilde \phi_\gamma$ (to be defined later).
				With probability at least $1-\delta$,
				 \eqn{\red{L_0}(\hat w) \le L_\gamma(w^*) + \frac{4 R_n(\sF)}{\gamma} + \sqrt{\frac{2 \log(2/\delta)}{n}}.}
				 Important: The LHS is one the usual zero-one loss.
				Remarks
					The moral of this theorem is that if it is possible to get low risk under the more stringent $L_\gamma$,
					 then we should also be able to do well on zero-one loss.
					This also in a way supports the use of SVMs, which try to maximize the margin;
					 a larger margin means we have better generalization.
					Also note that as margin $\gamma$ decreases, the bound on the RHS gets worse, blowing up as $1/\gamma$.
			Proof
				The problem is that $\phi_\gamma$ is not Lipschitz for any value of $\gamma$.
				 So the key trick is to introduce a Lipschitz version $\tilde \phi_\gamma$ that interpolates smoothly between the two:
				 \eqn{\tilde \phi_\gamma(m) = \min \pc{ 1, \max \pc{0, 1 - \frac{m}{\gamma} } }.}
					FIGURE: [plot $\tilde \phi_\gamma$]
				Then the Rademacher complexity of this intermediate loss class can be bounded:
				 \eqn{R_n(\tilde \phi_\gamma \circ \sF) = \frac{R_n(\sF)}{\gamma},}
				 because $\tilde \phi_\gamma$ is $\inv{\gamma}$-Lipschitz.
				 Applying \refthm{rademacher}, we get that:
				 \eqnl{tildegammaGen}{\tilde L_\gamma(\hat w) \le \tilde L_\gamma(w^*) + \frac{4 R_n(\sF)}{\gamma} + \sqrt{\frac{2 \log(2/\delta)}{n}}.}
				Note that $\phi_0 \le \tilde \phi_\gamma \le \phi_\gamma$,
				 so the same relation holds on the expected risks for all $w \in W$:
				 \eqn{L_0(w) \le \tilde L_\gamma(w), \quad \tilde L_\gamma(w) \le L_\gamma(w).}
				 The final result follows from applying these two inequalities to \refeqn{tildegammaGen}.
	!verbatim \lecture{8}
	Covering numbers (metric entropy) \currlecture
		Motivation
			We can measure the complexity of a finite hypothesis class simply by computing its cardinality.
			 For infinite hypothesis classes, we observed that
			 shattering coefficient was an appropriate measure since (thanks to symmetrization) all that mattered
			 was the behavior of a function class on a finite set of points.
			 However, shattering coefficient only works for functions that return a finite number of values.
			 Can we retain the combinatorial nature of shattering coefficients,
			 but allow for real-valued functions (for example, to handle regression problems)?
			The key measure we will explore in this section is covering numbers,
			 which counts the number of balls of size $\epsilon$ one needs to cover the hypothesis class.
			We can use the Massart's finite lemma to control the representatives;
			 the behavior of the other functions is controlled by virtue of being close to some representative.
			 In essence, covering numbers allows us to discretize the problem.
		 To talk about closeness of functions, we introduce the general notion of a metric space:
		\definitionHeading{metricSpace}{metric space}
			A metric space $(\sX, \rho)$ is defined over a set $\sF$ with equipped with a metric $\rho$.
			A metric $\rho : \sX \times \sX \to \R$
			 must be non-negative, symmetric, satisfy the triangle inequality,
			 and evaluate to $0$ iff its arguments are equal.
			If $\rho(f, f') = 0$ is possible for $f \neq f'$,
			 then we say $\rho$ is a pseudometric.
			 In this section, technically we will be working with the pseudometric.
		Examples of metrics $\rho$
			Euclidean distance
				For real vectors $\sX = \R^d$.
				The metric is $\rho(f, f') = \|f-f'\|_2$.
			$L_2(P_n)$
				This is the $L_2$ distance with respect to the empirical distribution over $n$ data points:
				 $P_n = \inv{n} \sum_{i=1}^n \delta_{z_i}$.
				Let $\sX$ be a family of functions mapping $\sZ$ to $\R$.
				The metric is $\rho(f, f') = \p{\inv{n} \sum_{i=1}^n (f(z_i) - f'(z_i))^2}^{\frac12}$.
				Remark: this metric can be thought of as computing an empirical standard deviation over
				 differences between $f$ and $f'$.
				Remark: since we are restricting the functions to evaluations at $n$ points,
				 we can really think of these functions as $n$-dimensional vectors,
				 with a metric which is Euclidean distance scaled down by $\sqrt{n}$.
				FIGURE: [two functions and $n$ points]
		\definitionHeading{ball}{\word{ball}}
			Let $(\sX, \rho)$ be a metric space.
			Then the ball with radius $\epsilon>0$ centered at $f \in \sX$ is defined as:
			 \eqn{B_\epsilon(f) \eqdef \{ f' \in \sX : \rho(f, f') \le \epsilon \}.}
		\definitionHeading{coveringNumber}{covering number}
			FIGURE: [covering a set with balls]
			An \word{$\epsilon$-cover} of a set $\sF \subseteq \sX$ with respect to a metric $\rho$
			 is a finite subset $C = \{f_1, \dots, f_m\} \subseteq \sX$\footnote{
			 Note: there are two variants of covers,
			 internal (where $C$ needs to be in the set $\sF$)
			 and external (where the cover $C$ need not be).
			 We will work with external covers since it gives us some flexibility,
			 and all we will care about is the cardinality of $C$.}
			 such that if we put a ball of radius $\epsilon$ at each $f_j$,
			 the resulting union is a superset of $\sF$;
			 that is $\sF \subseteq \cup_{j=1}^m B_\epsilon(f_j)$.
			Equivalently,
			 every point in $\sF$ is at most distance $\epsilon$ away (under the metric $\rho$)
			 from some point $f_j$ in the cover $C$.
			Define the $\epsilon$-\word{covering number}
			 of $\sF$ with respect to $\rho$ to be the size of the smallest cover:
			 \eqn{N(\epsilon, \sF, \rho) \eqdef \min\{ m : \exists \{f_1, \dots, f_m\} \subseteq \sX, \sF \subseteq \cup_{j=1}^m B_\epsilon(f_j) \}.}
			The \word{metric entropy} of $\sF$ is $\log N(\epsilon, \sF, \rho)$.
		As $\epsilon$ decreases, the points $f_j$ in the cover are a better approximation of the points in $B_\epsilon(f_j)$,
		 but $N(\epsilon, \sF, \rho)$ also increases.
		 What is this tradeoff?
		\exampleHeading{coveringAllFunctions}{all functions}
			FIGURE: [plot a function, discretize range, put down $z_1, \dots, z_n$]
			Let $\sF = \sX$ be all functions from $\R$ to $[0,1]$.
			Recall that under the metric $\rho = L_2(P_n)$,
			 only function evaluations on the points $z_1, \dots, z_n$ matter.
			In order to cover $\sF$, fix any $f \in \sF$.
			 Our strategy is to construct some $g \in \sF$ such that $\rho(f, g) \le \epsilon$
			 and count the number of $g$'s we obtain as we sweep $f$ across $\sF$.
			For each point $z_i$, we cover the range $[0,1]$ with the set of discrete points
			 $Y = \{ 2\epsilon, 4\epsilon, \dots, 1 \}$.
			 For any value of $f(z_i) \in [0, 1]$,
			 we can pick $g(z_i) \in Y$ so that $|f(z_i) - g(z_i)| \le \epsilon$.
			 The value of $g(z)$ for $z$ not equal to any of the $n$ points can be chosen arbitrarily.
			 Averaging over all $z_i$, we get that $\rho(f, g) \le \epsilon$.
			Furthermore, $|Y| = \frac{1}{2\epsilon}$,
			 so
			 \eqn{N(\epsilon, \sF, L_2(P_n)) \le \p{\frac{1}{2\epsilon}}^n.}
			 The metric entropy is thus $O(n \log (1/\epsilon))$, which intuitively is too large.
			 To see this, even if we ignored the discretization error $\epsilon$ for the moment,
			 we would see that the empirical Rademacher complexity of the cover,
			 by Massart's finite lemma is
			 $O(\sqrt{\frac{n \log(1/\epsilon)}{n}}) = O(1)$, which does not go to zero.
			 Clearly this class of functions is too large.
			Let's consider a smaller function class which is still infinite-dimensional and pretty rich:
		\exampleHeading{coveringIncreasing}{non-decreasing functions}
			FIGURE: [plot an increasing function, discretize range, put down $z_1, \dots, z_n$]
			Let $\sF$ be all non-decreasing functions from $\R$ to $[0, 1]$.
			Recall that $z_1, \dots, z_n$ are the $n$ fixed points.
			 WLOG, assume they are sorted in increasing order.
			Similar to \refex{coveringAllFunctions},
			 we break up the range $[0, 1]$ into $\frac{1}{\epsilon}$ levels
			 $Y = \{ \epsilon, 2\epsilon, \dots 1 \}$.
			Fix any function $f \in \sF$.
			 We will construct a piecewise constant approximation $g$ for which $\rho(f, g) \le \epsilon$.
			For each level $y \in Y$,
			 consider the points $z_i$ for which $f(z_i) \in [y-\epsilon, y]$.
			 Set $g(z_i) = y$ for these points.
			 Note that $g$ is a non-decreasing function across $z_1, \dots, z_n$.
			 By construction, $\rho(f, g) \le \epsilon$.
			Now let's count the number of possible $g$'s there are.
			 The key observation is that because each $g$ is non-decreasing
			 we can associate each level $y \in Y$ with a leftmost 
			 the leftmost point $z_i$ for which $g(z_i) = y$;
			 the choice of leftmost points ($n$ possible values) for each level ($|Y| = 1/\epsilon$ values)
			 uniquely defines $g$.
			 Therefore \eqn{N(\epsilon, \sF, L_2(P_n)) = O(n^{1/\epsilon}),}
			 and the metric entropy is $O((\log n)/\epsilon)$.
			 This is much better than that of all functions.
			 The main difference is that we're choosing a point for each level rather
			 than a level for each point.
		 Having set up covering numbers with some examples,
		 we are ready to state our main theorems.
		 Specifically, we will show that the metric entropy
		 leads to an upper bound on the Rademacher complexity.
		 This can be accomplished two ways:
			First, we will show a simple discretization argument that applies covering numbers
			 at a single resolution $\epsilon$.
			Next, we provide a more sophisticated argument called \word{chaining} that adaptively
			 chooses the resolution, yielding tighter bounds.
		\theoremHeading{discretization}{simple discretization}
			Let $\sF$ be a family of functions mapping $\sZ$ to $[-1,1]$.
			The empirical Rademacher complexity of a function class $\sF$ can be upper bounded using
			 its covering number:
			 \eqn{\hat R_n(\sF) \le \inf_{\epsilon > 0} \p{\sqrt{\frac{2 \log N(\epsilon, \sF, L_2(P_n))}{n}} + \epsilon}.}
			Remark: the RHS involves two terms:
				The first is the covering number, which increases as $\epsilon$ decreases.
				 This comes from Massart's finite lemma.
				The second is $\epsilon$ (which decreases as $\epsilon$ decreases).
				 This is the penalty we pay for having a discretization.
		Preparation
			We will also assume $Z_{1:n}$ are constant and suppress the explicit conditioning,
			 writing $\E[A]$ instead of $\E[A \mid Z_{1:n}]$.
			To simplify notation, we write $\|f\|$ for $\|f\|_{L_2(P_n)}$ and $\inner{f}{g}$ for $\inner{f}{g}_{L_2(P_n)}$.
			Overloading notation, let $\sigma : \sZ \to \{-1,+1\}$ be defined as a function
			 which when evaluated on $z_i$ returns the random sign $\sigma_i$.
			 This allows us to write the empirical Rademacher complexity succinctly as:
			 \eqn{\hat R_n(\sF) = \E\pb{\sup_{f \in \sF} \inner{\sigma}{f}}.}
			Note that $\|\sigma\| = 1$ since it's just a vector of $+1$s and $-1$s.
			In summary, think about each function $f \in \sF$ as an $n$-dimensional vector
			 \eqn{\inv{\sqrt{n}} [f(z_1), \dots, f(z_n)]}
			 and the Rademacher variables $\sigma_1, \dots, \sigma_n$ also as an $n$-dimensional vector:
			 \eqn{\inv{\sqrt{n}} [\sigma_1, \dots, \sigma_n].}
			 We then are just using the usual Euclidean distance on them.
		Proof of \refthm{discretization}:
			Fix $\epsilon > 0$ and let $C$ be an $\epsilon$-cover of $\sF$.
			The proof follows from manipulating the empirical Rademacher complexity:
			 \eqn{
			 \hat R_n(\sF)
			 &= \E\pb{\sup_{f \in \sF} \inner{\sigma}{f}} \\
			 &= \E\pb{\sup_{g \in C} \sup_{f \in \sF \cap B_\epsilon(g)} \inner{\sigma}{g} + \inner{\sigma}{f - g}} \\
			 &= \E\pb{\sup_{g \in C} \inv{n} \inner{\sigma}{g} + \epsilon} \aside{Cauchy-Schwartz} \\
			 &= \sqrt{\frac{2 \log N(\epsilon, \sF, L_2(P_n))}{n}} + \epsilon \aside{Massart's finite lemma}.
			 }
		\exampleHeading{increasingBoundDiscretization}{non-decreasing functions (with simple discretization)}
			Let $\sF$ be all non-decreasing functions from $\sZ = \R$ to $[0, 1]$.
			Plugging the covering number of $\sF$ into \refthm{discretization}:
			 \eqn{\hat R_n(\sF) \le \inf_{\epsilon > 0} \p{\sqrt{\frac{2 \cdot O(\frac{\log n}{\epsilon})}{n}} + \epsilon}.}
			 The RHS is minimized when the two terms are equal.
			 Solving for $\epsilon$ and substituting it back yields:
			 \eqn{\hat R_n(\sF) = O\p{\p{\frac{\log n}{n}}^{\frac13}}.}
			Note that this bound provides a rate which is worse than the usual $\frac{1}{\sqrt{n}}$ rates.
			 Is it because non-decreasing functions are just more complex than parametric function classes?
			 Not in this case.  We'll see shortly that this is just an artifact of the analysis being too weak,
			 because it is only able to work at one level of resolution $\epsilon$.
		Where is the looseness? 
		 The problem with simple discretization is that we are covering at resolution $\epsilon$,
		 which inevitably requires many functions ($N(\epsilon, \sF, L_2(P_n))$ of them, in fact),
		 but our application of Massart's finite lemma just assumes we have an arbitrary set of functions
		 with magitude up to $1$.  But these functions aren't arbitrary---they live in a metric space,
		 and the nearby ones are actually quite close (within $\epsilon$).
		 Rather than paying a $1$ (the maximum deviation of a function $f \in \sF$),
		 wouldn't it be nice if we could pay something like $\epsilon$
		 (the maximum difference in nearby functions in the $\epsilon$-cover)?
		 Of course, certain pairs of functions in $\sF$ are quite far.
		We will now introduce a clever technique called chaining gets at the intuition
		 by constructing covers at multiple levels of resolution,
		 and using functions in the coarser covers as waypoints.
		 The idea is that the fewer functions in the coarse cover can have larger deviation,
		 and the many functions in the finer covers has smaller deviation.
		\theoremHeading{chaining}{chaining (Dudley's theorem)}
			Let $\sF$ be a family of functions mapping $\sZ$ to $\R$.
			The empirical Rademacher complexity of a function class $\sF$ can be upper bounded using
			 an integral over its covering number:
			 \eqn{\hat R_n(\sF) \le 12 \int_{0}^\infty \sqrt{\frac{\log N(\epsilon, \sF, L_2(P_n))}{n}} d \epsilon.}
			Remark: note that compared with \refthm{discretization},
			 this bound involves an integral that sweeps across different resolutions $\epsilon$,
			 and importantly removes the additive $\epsilon$ penalty.
		Proof of \refthm{chaining}
			FIGURE: [lines corresponding to levels, where each level is discretized more finely than the previous]
			Let $\epsilon_0 = \sup_{f \in \sF} \|f\|$ be the maximum norm of a function $f \in \sF$,
			 which will serve the coarsest resolution.
			 Let $\epsilon_j = 2^{-j} \epsilon_0$ for $j = 1, \dots, m$ be successively finer resolutions.
			For each $j = 0, \dots, m$, let $C_j$ be an $\epsilon_j$-cover of $\sF$.
			Fix any $f \in \sF$.
			Let $g_j \in C_j$ be such that $\|f - g_j\| \le \epsilon_j$;
			 take $g_0 = 0$.
			 Note that $g_j$'s depend on $f$.
			Let us decompose $f$ as follows:
			 \eqn{f = f - g_m + \underbrace{g_0}_{= 0} + \sum_{j=1}^m (g_j - g_{j-1}).}
			Restating Massart's finite lemma,
			 for a class of functions $\sB$, the empirical Rademacher complexity is:
			 \eqn{\hat R_n(\sB) \le \p{\sup_{b \in \sB} \|b\|} \sqrt{\frac{2 \log |\sB|}{n}}.}
			Let us bound some norms:
				$\|f - g_m\| \le \epsilon_m$
				$\|g_j - g_{j-1}\| \le \|g_j - f\| + \|f - g_{j-1}\| \le \epsilon_j + \epsilon_{j-1} = 3 \epsilon_j$ (since $2 \epsilon_j = \epsilon_{j-1}$)
			Now compute the empirical Rademacher complexity:
			 \eqn{\hat R_n(\sF)
			 &= \E\pb{\sup_{f \in \sF} \inner{\sigma}{f}} \aside{definition} \\
			 &= \E\pb{\sup_{f \in \sF} \inner{\sigma}{f - g_m} + \sum_{j=1}^m \inner{\sigma}{g_j - g_{j-1}}} \aside{decompose $f$} \\
			 &\le \epsilon_m + \E\pb{\sup_{f \in \sF} \sum_{j=1}^m \inner{\sigma}{g_j - g_{j-1}}} \aside{Cauchy-Schwartz} \\
			 &\le \epsilon_m + \sum_{j=1}^m \E\pb{\sup_{f \in \sF} \inner{\sigma}{g_j - g_{j-1}}} \aside{push sup inside} \\
			 &\le \epsilon_m + \sum_{j=1}^m \E\pb{\sup_{g_j \in C_j, g_{j-1} \in C_{j-1}} \inner{\sigma}{g_j - g_{j-1}}} \aside{refine dependence} \\
			 &\le \epsilon_m + \sum_{j=1}^m \red{(3 \epsilon_j)} \sqrt{\frac{2 \log(\red{|C_j| |C_{j-1}|})}{n}} \aside{Massart's finite lemma} \\
			 &\le \epsilon_m + \sum_{j=1}^m (6 \epsilon_j) \sqrt{\frac{\log |C_j|}{n}} \aside{since $|C_j| \ge |C_{j-1}|$} \\
			 &= \epsilon_m + \sum_{j=1}^m 12 (\epsilon_j - \epsilon_{j+1}) \sqrt{\frac{\log |C_j|}{n}} \aside{since $\epsilon_j = 2(\epsilon_j - \epsilon_{j+1})$} \\
			 &\le 12 \int_0^\infty \sqrt{\frac{\log N(\epsilon, \sF, L_2(P_n))}{n}} d\epsilon \aside{bound sum with integral}
			 }
			 In the last step, we took $m \to \infty$, which makes the additive penalty $\epsilon_m \to 0$.
			FIGURE: [plot showing $N(\epsilon, \sF, \rho)$ as a function of $\epsilon$, with locations of $\epsilon_0, \epsilon_1, \dots$]
			Remark: The reason why chaining provides better results is the following.
			 In simple discretization, we apply Massart's finite lemma on functions whose magitude is $\sup_{f \in \sF} \|f\|$,
			 whereas in chaining, we apply Massart's finite lemma on differences between functions in $C_j$ and $C_{j-1}$
			 whose range is $3\epsilon_j$.
		\exampleHeading{increasingBoundChaining}{non-decreasing functions (with chaining)}
			Let $\sF$ be all non-decreasing functions from $\sZ$ to $[0, 1]$.
			Note that $\|f\| \le 1$ for all $f \in \sF$, so the coarsest resolution is $\epsilon_0 = 1$,
			 so we only have to integrate $\epsilon$ up to $1$, not $\infty$,
			 since $\log N(\epsilon, \sF, L_2(P_n)) = 0$ for $\epsilon \ge \epsilon_0$.
			Plugging the covering number of $\sF$ into \refthm{chaining}:
			 \eqn{\hat R_n(\sF)
			 &\le 12 \int_0^1 \p{\sqrt{\frac{O(\frac{\log n}{\epsilon})}{n}}} d\epsilon \\
			 &= O\p{\sqrt{\frac{\log n}{n}}} \int_0^1 \sqrt{\inv{\epsilon}} d\epsilon \\
			 &= O\p{\sqrt{\frac{\log n}{n}}}.
			 }
			Remark: compared with the bound using the simple discretization,
			 we get a better bound ($n^{-\frac{1}{2}}$ versus $n^{-\frac{1}{3}}$).
		Summary
			Covering numbers is a powerful technique that allows us to handle
			 rich function classes such as all non-decreasing functions.
			The essence is discretization of a function class, so that we can use Massart's finite lemma.
			Chaining allows us to leverage multiple discretization resolutions $\epsilon$ to obtain tight bounds.
			Covering numbers give you a lot of freedom for customization:
			 choice of metric and choice of discretization.
			Covering numbers can sometimes be more convenient;
			 for example, the covering number of $\sF_1 \cup \sF_2$ is at most that of $\sF_1$ plus that of $\sF_2$,
			 a property not shared by Rademacher complexity.
	!verbatim \lecture{9}
	Algorithmic stability \currlecture
		Motivation
			Let us shelve excess risk $L(\hat h) - L(h^*)$ for the moment,
			 and instead consider the gap between the expected and empirical risk.
			 It is easy to see that this quantity can be upper bounded using uniform convergence:
			 \eqnl{trainTestGap}{\BP[L(\hat h) - \hat L(\hat h) \ge \epsilon] \le \BP[\sup_{h \in \sH} L(h) - \hat L(h) \ge \epsilon].}
			 Colloquially, this answers the following question: if I get $10\%$ error on my training set,
			 what should I expect my test error to be?  (The answer is no more than $10\% + \epsilon$.)
			 Analyzing the excess risk has relied on $\hat h$ being the ERM,
			 but \refeqn{trainTestGap} actually holds for \emph{any} estimator $\hat h$.
			 It is useful to think about $\hat h$ as a function of the training examples:
			 $\hat h = A(z_1, \dots, z_n)$, where $A$ is an \emph{algorithm}.
			Uniform convergence hones in on studying the ``size'' of $\sH$
			 (using Rademacher complexity, VC dimension, etc.).
			 However, what if a learning algorithm $A$ does not ``use'' all of $\sH$?
			 For example, what if you use naive Bayes, regularization via a penalty term,
			 early stopping, or dropout training?
			 All of these could in principle can return any hypothesis from $\sH$,
			 but are somehow constrained by their objective function or algorithm,
			 so we might expect better generalization than simply looking at the complexity of $\sH$
			 in an algorithm-independent way.
			To be more concrete,
			 let us analyze $\sH = \{ x \mapsto w \cdot x : w \in \R^d \}$,
			 the class of linear functions (with no bounds on the norm).
			 Define the norm $\|h\|_\sH$ to be $\|w\|_2^2$ of the associated weight vector.
			 Define the \textbf{regularized empirical risk minimizer} as follows:
			 \eqnl{regularizedERM}{\hat h = \arg\min_{h \in \sH} \hat L(h) + \frac{\lambda}{2} \|h\|_\sH^2.}
			Of course, for a given realization of the data $z_{1:n}$,
			 the solution to \refeqn{regularizedERM} can be gotten
			 by finding the ERM on the constrained set $\{ h \in \sH : \|h\|_\sH \le B \}$ for some bound $B$.
			 But this connection is weak in that $B$ would need to depend on the data
			 to obtain an exact equivalence.
			 How can we analyze \refeqn{regularizedERM} directly?
			!comment
				K-nearest neighbors:
				 Given a dataset, the $k$-NN classifier $\hat h$
				 takes $x$ and performs a majority vote over the labels of the closest $k$ inputs.
				 More formally, let $I(x) \subseteq \{ 1, \dots, n \}$ be indices with $|I(x)| = k$ and
				 $\|x - x_i\| \le \|x - x_{i'}\|$ for all $i \in I(x)$ and $i' \not\in I(x)$.
				 Then $\hat h(x) = \arg\max_y |\{ i \in I(x) : y_i = y \}|$.
				 In this case, $\hat h$ isn't even naturally expressed as minimizing a regularized risk.
			In this section, we will introduce bounds based on a notion of \word{algorithmic stability},
			 where we view a learning algorithm as a function $A$ that take as input the data $z_{1:n}$
			 and outputs some hypothesis $\hat h$.  We will study the generalization properties
			 of $\hat h$ as a function of how stable the algorithm is.
			 Notice the focus on algorithms rather than on the hypothesis class.
		Stability will be measured with respect to the difference in behavior of an
		 algorithm on a training set $S$ and a perturbed version $S^i$:
			$S = (z_1, \dots, z_n)$: training data, drawn i.i.d. from $p^*$
			$S^i = (z_1, \dots, z_i', \dots, z_n)$: training data with an i.i.d.~copy of the $i$-th example
			$z_0$ is a new test example
		 We start with a definition of stability:
			\definitionHeading{stability}{uniform stability}
				We say that an algorithm $A : \sZ^n \to \sH$ has \word{uniform stability} $\beta$
				 (or in the context of these notes, simply $\beta$-stable) with respect to a loss function
				 $\ell$ if
				 for all training sets $S \in \sZ^n$, perturbations $S^i \in \sZ^n$, and test example $z_0 \in \sZ$,
				 \eqn{|\ell(z_0, A(S)) - \ell(z_0, A(S^i))| \le \beta.}
				Note that this is a very strong condition in that the bound must hold uniformly
				 for all $z_0, S, S^i$ and is not reliant on the probability distribution.
				 There are weaker notions detailed in Bosquet/Elisseeff (2002).
			\exampleHeading{stableMean}{stability of mean estimation}
				Assume all points $z \in \R^d$ are bounded $\|z\|_2 \le B$.
				Define the squared loss: $\ell(z,h) = \frac{1}{2} \|z - h\|_2^2$.
				Define an algorithm that computes the regularized empirical risk minimizer:
				 \eqn{A(S)
				 &\eqdef \arg\min_{h \in \R^d} \hat L(h) + \frac{\lambda}{2} \|h\|_2^2 \\
				 &= \frac1{(1+\lambda) n} \sum_{i = 1}^n z_i.}
				Then $A$ has uniform stability $\boxed{\beta = \frac{6B^2}{(1 + \lambda)n}}$.
				Intuitively, averaging in a new point should contribute $O(1/n)$ and the $1/(1+\lambda)$
				 is just due to the shrinkage towards zero induced by the regularizer.
				To derive this formally,
				 define $v_1 = A(S) - z_0$ and $v_2 = \frac{1}{(1+\lambda)n} [z_i' - z_i]$.
				 \eqn{|\ell(z_0, A(S)) - \ell(z_0, A(S^i))|
				 &\le \left|\half \|v_1\|_2^2 - \half \|v_1 + v_2\|_2^2\right| \\
				 &\le \|v_2\|_2 (\|v_1\|_2 + \half \|v_2\|_2) \\
				 &\le \frac{2B}{(1+\lambda)n} \p{2B + \frac{B}{(1 + \lambda)n}} \\
				 &\le \frac{2B}{(1+\lambda)n} \p{2B + B}.
				 }
				We can see that as we increase regularization (larger $\lambda$) or amount of data (larger $n$),
				 we have more stability (smaller $\beta$).
			\exampleHeading{stableKernel}{stability of linear predictors}
				Let $\sH = \{ x \mapsto w \cdot x : w \in \R^d \}$.
				Assume that $\|x\|_2 \le C_2$ with probability $1$
				 (according to the data-generating distribution $p^*$).
				Assume the loss $\ell$ is $1-$Lipschitz: for all $z_0 \in \sZ$ and $h,h' \in \sH$:
				 \eqn{|\ell(z_0, h) - \ell(z_0, h')| \le \|h - h'\|_\infty \eqdef \sup_{x \in \R^d} |h(x) - h'(x)|.}
				 For example, for classification ($y \in \{-1,+1\}$),
				 this holds for the hinge loss $\ell((x,y), h) = \max\{ 1 - y h(x), 0 \}$.
				Define the regularized empirical risk minimizer:
				 \eqn{A(S) \eqdef \arg\min_{h \in \sH} \hat L(h) + \frac{\lambda}{2} \|h\|_\sH^2}
				Then $A$ has uniform stability $\boxed{\beta = \frac{C_2^2}{\lambda n}}$ with respect to $\ell$.
				 Again, the stability is similar to the case of mean estimation;
				 the difference is that $\lambda = 0$ is very bad in regularized ERM while it's fine
				 for mean estimation; this is because mean estimation is naturally stabilizing,
				 whereas the ERM (with no control on the norm) is not stable.
				 See the Bousquet/Elisseeff paper on stability for the proof.
		 Now that we have computed the stability for two examples and have a better intuition for what
		 stability is measuring, let us state the main theorem, which relates stability to generalization:
			\theoremHeading{stability}{generalization under uniform stability}
				Let $A$ be an algorithm with uniform stability $\beta$.
				Assume the loss is bounded: $\sup_{z,h} |\ell(z,h)| \le M$.
				Then with probability at least $1-\delta$,
				 \eqn{\boxed{L(A(S)) \le \hat L(A(S)) + \beta + (\red{\beta n} + M) \sqrt{\frac{2 \log (1/\delta)}{n}}.}}
				Remark: Due to the presence of $\beta n$, for this bound to be not vacuous, we must have $\beta = o\p{\frac{1}{\sqrt{n}}}$.
				 Generally, we will have $\beta = O(\frac{1}{n})$,
				 which guarantees that $L(A(S)) - \hat L(A(S)) = O(\frac{1}{\sqrt{n}})$.
			Proof of \refthm{stability}:
				Our goal is to bound the difference between the expected and empirical risks:
				 \eqn{D(S) \eqdef L(A(S)) - \hat L(A(S)).}
				 Stability tells us about $\ell(z_0, A(S))$,
				 which suggests using McDiarmid's inequality.
				 That is the heart of the proof.
				Step 1: Bound the expectation of $D(S)$.
					Recall that $S = (z_1, \dots, z_n), (z_1', \dots, z_n'), z_0$ are all independent
					 and therefore exchangeable.
					 The basic trick is just to rename variables that preserve the dependency structure
					 and therefore the expectation $\E[D(S)]$,
					 and get it into a form where we can apply the definition of uniform stability:
					 \eqn{\E[D(S)]
					 & = \E\pb{\inv{n} \sum_{i=1}^n [\ell(z_0, A(S)) - \ell(z_i, A(S))]} \aside{definition} \\
					 & = \E\pb{\inv{n} \sum_{i=1}^n [\ell(z_i', A(S)) - \ell(z_i', A(S^i))]} \aside{renaming} \\
					 & \le \beta \aside{definition of uniform stability}
					 }
					 The point is that in the first term $z_0$ is not in $S$ and $z_i$ is in $S$.
					 This logic is preserved: $z_i'$ is not in $S$ and $z_i'$ is in $S^i$.
				Step 2: show that $D(S)$ satisfies the bounded differences property.
					We should expect such a property to hold given the definition of uniform stability.
					 But we are not directly applying the bounded differences to the loss $\ell(z_0, A(S))$,
					 but rather to $D(S)$.  So there is slightly more work.
					 The trick here is to break down differences using the triangle inequality
					 into a chain of comparisons, each involving a single perturbation.
					Let $\hat L^i$ denote the empirical expectation with respect to $S^i$.
					We have
					 \eqn{
					 &|D(S) - D(S^i)| \\
					 &= |L(A(S)) - \hat L(A(S)) - L(A(S^i)) + \hat L^i(A(S^i))| \\
					 &\le |L(A(S)) - L(A(S^i))| + |\hat L(A(S)) - \hat L^i(A(S^i))| \aside{triangle inequality} \\
					 %&\le \beta + |\hat L(A(S)) - \hat L(A(S^i))| + \frac{1}{n} |\ell(z_i, A(S^i)) - \ell(z_i', A(S^i))| \\
					 &\le \underbrace{|L(A(S)) - L(A(S^i))|}_{\le \beta} + \underbrace{|\hat L(A(S)) - \hat L(A(S^i))|}_{\le \beta} + \underbrace{|\hat L(A(S^i)) - \hat L^i(A(S^i))|}_{\le \frac{2M}{n}} \\
					 &\le 2\beta + \frac{2M}{n}.
					 }
					 In the first two cases, we just used the fact that $A$ is $\beta$-stable;
					 recall that $\hat L(h) = \inv{n} \sum_{i=1}^n \ell(z_i, h)$
					 and $L(h) = \E_{z_0 \sim p^*}[\ell(z_0, h)]$;
					 here it's important that $\beta$-stability holds uniformly for any first argument of $\ell$,
					 so that we can upper bound by $\beta$ regardless of the dependence structure between the two arguments.
					 In the third case, $\hat L$ and $\hat L^i$ just differ by one term
					 which can differ by at most $2M$ and is scaled by $\inv{n}$.
				Step 3: apply McDiarmid's inequality to bound $D(S)$ in high probability.
					This is a straightforward application.  We have
					 \eqn{\BP[D(S)) - \E[D(S)] \ge \epsilon]
					 &\le \exp\p{\frac{-2\epsilon^2}{n (2\beta + \frac{2M}{n})^2}} \\
					 &= \exp\p{\frac{-n \epsilon^2}{2(\beta n + M)^2}} \eqdef \delta.}
					 Rewriting the bound and using the fact that $\E[D(S)] \le \beta$,
					 we have that with probability at least $1-\delta$,
					 \eqn{D(S) \le \beta + \p{\beta n + M} \sqrt{\frac{2\log (1/\delta)}{n}}.}
			Application to linear functions
				Recall that for regularized ERM on linear functions with $1$-Lipschitz losses (like hinge loss),
				 $\beta = \frac{C_2^2}{\lambda n}$, where $\lambda$ is the regularization strength
				 and $\|x\|_2 \le C_2^2$.
				 Plugging this value of $\beta$ into \refthm{stability},
				 we get that with probability at least $1-\delta$,
				 \eqn{L(A(S)) \le \hat L(A(S)) + O\p{\frac{C_2^2 + M}{\lambda \sqrt{n}}}.}
				 Note that this bound has the right dependence on $n$,
				 but has a worse dependence on $C_2$ compared to the Rademacher bounds.
	PAC-Bayesian bounds \currlecture
		Bayesians and frequentists
			A Bayesian estimation procedure starts with a \word{prior distribution} $P(h)$ over hypotheses,
			 observe the training data $z_1, \dots, z_n$,
			 and produces a \word{posterior distribution} $Q(h) \propto P(h) \prod_{i=1}^n F(z_i \mid h)$,
			 where $F$ is the likelihood function.
			 Bayesian procedures are optimal (see the work of Edwin Jaynes)
			 assuming the prior and the likelhood functions are correct.
			 But what happens if either is wrong (which is all the time)?
			 We still want some guarantees.
			From the cold frequentist perspective,
			 this Bayesian procedure is just another estimator,
			 an algorithm that takes some data $z_1, \dots, z_n$
			 and returns a single hypothesis $\hat h$ or a posterior $Q(h)$ over hypotheses.
			 In this light, we can still ask for frequentist guarantees:
			 how does the expected and empirical risks compare when data is generated form any distribution?
			Philosophically, this hybrid view is quite satisfying:
			 Bayesian procedures are often very sensible,
			 and now we can still hold them up to the scrutiny of frequentist analysis.
		Bounds that depend on the prior
			In fact, we already have frequentist guarantees
			 that relate the expected and empirical risks for all $h \in \sH$,
			 and thus for any estimator $\hat h$.
			Recall that for finite hypothesis classes with loss bounded in $[0, 1]$,
			 using Hoeffding's inequality plus the union bound,
			 we have that with probability at least $1-\delta$,
			 \eqn{\forall h \in \sH : L(h) \le \hat L(h) + \sqrt{\frac{\red{\log |\sH|} + \log(1/\delta)}{2n}}.}
			 (This is similar to \refthm{finiteHypothesis}, but the bound is on $L - \hat L$ rather than excess risk.)
			But in this bound, each $h \in \sH$ is treated the same,
			 whereas the prior $P(h)$ explicitly says that some are more likely than others.
			 Can we \emph{incorporate the prior $P(h)$ into the analysis without assuming that the prior is ``correct''}?
			PAC-Bayesian bounds provide exactly this.
			 In this class, we will consider two types:
				Occam bound: assume a countable hypothesis class, algorithm outputs a single hypothesis
				PAC-Bayesian theorem: assume an infinite hypothesis class, algorithm outputs a posterior
			 Let us start with Occam bound, which captures the key intuitions:
		\theoremHeading{occam}{Occam bound}
			Let $\sH$ be a countable hypothesis class.
			Let the loss function be bounded: $\ell(z,h) \in [0,1]$.
			Let $P$ be any ``prior'' distribution over $\sH$.
			Then with probability at least $1-\delta$,
			 \eqn{\boxed{\forall h \in \sH : L(h) \le \hat L(h) + \sqrt{\frac{\red{\log(1/P(h))} + \log(1/\delta)}{2n}}.}}
		Let's try to understanding this bound with a few special cases.
			If the prior puts all its mass on one hypothesis $h_0$ ($P(h) = \1[h = h_0]$),
			 then the bound is just the standard Hoeffding bound you would get for a single hypothesis.
			If we have a uniform distribution over some subset of hypotheses $S \subseteq \sH$
			 ($P(h) = \1[h \in S]/|S|$),
			 then we recover the standard result for finite hypotheses
			 (similar to \refthm{finiteHypothesis}, which is for excess risk).
			This reveals how PAC-Bayes is a generalization:
			 rather than commiting to prior distributions which are uniform over some support $S \subseteq \sH$,
			 we can have prior distributions which place different probability mass on different hypotheses.
			 We can let $\sH$ be as large as we want as long as we still have
			 probabilities ($\sum_{h \in \sH} P(h) = 1$).
			 In some sense, $P(h)$ defines a fuzzy hypothesis class.
		Proof of \refthm{occam}:
			The proof is very simple.
			 The key idea is to allocate our confidence parameter $\delta$ across different hypotheses
			 proportionally based on the prior $P(h)$.
			By Hoeffding's inequality,
			 we have that for any fixed $h \in \sH$:
			 \eqn{\BP[L(h) \ge \hat L(h) + \epsilon] \le \exp(-2n\epsilon^2).}
			If we set the RHS to $\delta P(h)$, then we get that with probability at most $\delta P(h)$,
			 \eqn{L(h) \ge \hat L(h) + \sqrt{\frac{\log(1/(\delta P(h)))}{2n}}.}
			Applying the union bound across all $h \in \sH$,
			 we have that with probability at most $\delta$,
			 \eqn{\exists h \in \sH : L(h) \ge \hat L(h) + \sqrt{\frac{\log(1/(\delta P(h)))}{2n}}.}
			Take the contrapositive of this statement to get the statement in the theorem.
		The bound also suggests an algorithm:
			The ultimate goal is to finte $h$ that minimizes $L(h)$.
			 Of course, we can't evaluate $L(h)$.
			 But we do have an alleged upper bound on $L(h)$!
			The bound is interesting because the penalty term $\log (1/P(h))$ actually depends on the
			 hypothesis $h$,
			 whereas all our previous bounds depended only on the complexity of $\sH$.
			 Let's treating the RHS bound as an objective to be minimized by an algorithm.
			 Recall that this bound holds simultaneously for all $h \in \sH$, which means that it will
			 hold for the output of any algorithm $A(S)$.
			Motivated by the bound, we can define an
			 \emph{algorithm that actually uses the bound} by minimizing the RHS:
			 \eqn{A(S) \eqdef \arg\min_{h \in \sH} \hat L(h) + R(h), \quad R(h) = \sqrt{\frac{\log(1/P(h)) + \log(1/\delta)}{2n}}.}
			 From this perspective, the bound provides a regularizer $R(h)$
			 which penalizes $h$ more if it has small prior probability $P(h)$.
			 The algorithm $A(S)$ thus balances the empirical risk $\hat L(h)$ with the regularizer $R(h)$.
			As we get more data ($n \to \infty$), the regularizer also goes to zero,
			 meaning that we will trust the empirical risk more,
			 allowing us to consider more complex hypotheses in $\sH$.
			 This is a pretty nice behavior to have.
			Note this is not a Bayesian procedure.
			 The closest thing is if we let the loss function be the negative log-likelihood
			 $\ell(z, h) = -\log F(z \mid h)$,
			 then the MAP estimate would be $\arg\min_{h \in \sH} \hat L(h) + \frac{\log (1/P(h))}{n}$,
			 so the only difference is the square root.
			 A partial explanation is that in order to obtain frequentist guarantees
			 without any smoothness conditions, we need to regularize more (with the square root).
		There are two things lacking about the Occam bound:
			It only applies to countable $\sH$, which does not include the set of all weight vectors, for example.
			It only embraces half of the Bayesian story: while we have a prior $P(h)$,
			 only a single $h \in \sH$ is returned rather than a full posterior $Q(h)$.
		 The following theorem generalizes the Occam bound:
		\theoremHeading{pacBayes}{PAC-Bayesian theorem (McAllester)}
			Let the loss function be bounded: $\ell(z,h) \in [0,1]$.
			Let $P$ be any ``prior'' distribution over $\sH$.
			Let $Q_S$ be any ``posterior'' distribution over $\sH$,
			 which is a function of the training data $S$.
			Then with probability at least $1-\delta$,
			 \eqn{\boxed{\E_{h \sim Q_S}[L(h)] \le \E_{h \sim Q_S}[\hat L(h)] + \sqrt{\frac{\KL{Q_S}{P} + \log(4n/\delta)}{2n-1}}.}}
		Proof: see the McAllester paper.
		To recover the Occam bound (up to constants),
		 we simply set $Q_S$ to be a point mass at some $h$.
	Interpretation of bounds \currlecture
		Now that we've derived a whole host of generalization bounds,
		 let us take a step back and ask the question: how should we think about these bounds?
		Properties 
			One could evaluate these bounds numerically, but they will probably be too loose to use directly.
			 The primary purpose of these bounds is to instead formalize the relevant properties of a learning problem
			 and characterize their relationship to the expected risk, the quantity of interest.
			The relationships solidify intuitions about learning.  Here are some examples:
				If we have $d$ features, $n \sim d$ training examples suffices to learn.
				 If the number of features increase by $2$,
				 we need to increase $n$ by $2$ as well to maintain the same estimation error.
				We can actually have as many features $d$ as we want (even infinite),
				 so long as we regularize properly using $L_2$ regularization:
				 bounds depend on norm $B$ not dimension $d$.
				If there are many irrelevant features use $L_1$ regularization:
				 the $L_1$ ball is just much smaller than the $L_2$ ball.
				 Here, exploiting the structure of the problem leads to better bounds (and algorithms).
				If there is low noise in the problem (in the realizable setting, some predictor obtains zero expected risk),
				 then estimation error is smaller ($O(1/n)$ versus $O(1/\sqrt{n})$ convergence).
		Excess risk is only part of the story.
			It is important to note that generalization bounds focus on addressing the (excess risk
			 $L(\hat h) - L(h^*)$, not the approximation error $L(h^*)$.
			For example, if $d$ is the number of features, then $L(\hat h) - L(h^*) = O(\sqrt{\frac{d}{n}})$
			 shows that by adding more features, the estimation error will worsen.
			 However, we hope that $L(h^*)$ will improve.
			 Therefore, in practice, one can still hope to reduce $L(\hat h)$ by adding additional features.
			The technical core of this section is \textbf{concentration of measure}:
			 as you aggregate over increasing amounts of \textbf{independent} data,
			 many of the relevant quantities convergence.
			 Insight is obtained by closely observing how fast these quantities converge.
		What loss function?
			The bounds derived using finite hypothesis classes or finite VC dimension
			 operated on the zero-one loss (supposing for the moment that's our desired loss function).
			 However, the empirical risk minimizer in this case
			 is NP-hard to compute.
			However, the norm-based bounds using Rademacher complexity required a Lipschitz loss function
			 such as the hinge loss, which is a surrogate loss.
			 This gives us results on an empirical risk minimizer which we can actually evaluate
			 in practice.  One can say is that the zero-one loss is upper bounded by the hinge loss,
			 but this is relatively weak, and in particular, minimizing the hinge loss even in the limit of infinite data
			 will not give you something that minimizes the zero-one loss.
			 A special case is that for universal kernels, minimizing the hinge loss (among others)
			 does correspond to minimizing the zero-one loss in the limit of infinite data (Bartlett/Jordan/McAuliffe, 2005).
	Summary \currlecture
		The main focus of this section was to study the \textbf{excess risk} $L(\hat h) - L(h^*)$ of the \textbf{empirical risk minimizer} $\hat h$.
		 In particular, we wanted that with probability at least $1-\delta$,
		 the excess risk $L(\hat h) - L(h^*)$ is upper bounded by
		 something that depends on the complexity of the learning problem and $n$, the number of i.i.d. training examples.
		 (Another goal is to bound the difference between expected and empirical risks $L(\hat h) - \hat L(\hat h)$.)
		The excess risk is often within a factor of two of
		 the difference between empirical and expected risk:
		 $\sup_{h \in \sH} |L(h) - \hat L(h)|$,
		 which has a form which suggests using uniform convergence tools to bound it.
		Results on excess risk $L(\hat h) - L(h^*)$:
			Realizable, finite hypothesis class: $O\p{\frac{\log |\sH|}{n}}$
			Finite hypothesis class: $O\p{\sqrt{\frac{\log |\sH|}{n}}}$
			Shattering coefficient / VC dimension: $O\p{\sqrt{\frac{\log s(\sH, n)}{n}}} = O\p{\sqrt{\frac{\VC(\sH) \log n}{n}}}$
			$L_2$ norm constrained---kernels ($\|w\|_2 \le B_2, \|x\|_2 \le C_2$): $O\p{\frac{B_2C_2}{\sqrt{n}}}$
			$L_1$ norm constrained---sparsity ($\|w\|_1 \le B_1, \|x\|_\infty \le C_\infty$): $O\p{\frac{B_1C_\infty \sqrt{\log d}}{\sqrt{n}}}$
			!comment Neural networks---non-convexity ($\|w_j\|_2 \le B_2$, $\|v\|_2 \le B_2'$, $\|x\|_2 \le C_2$, $k$ hidden units): $O\p{\frac{B_2 B_2' C_2 \sqrt{k}}{\sqrt{n}}}$
			Non-decreasing functions (via covering numbers and chaining): $O\p{\int_0^\infty \sqrt{\frac{\log N(\epsilon, \sH, L_2(P_n))}{n}} d\epsilon} = O\p{\sqrt{\frac{\log n}{n}}}$
		Technical tools
			Tail bounds
				How much do random variables deviate from their mean?
				 We generally look for sharp concentration bounds,
				 which means that the probability of deviation by a constant decays
				 \textbf{exponentially} fast as a function of $n$:
				 $\BP[G_n - \E[G_n] \ge \epsilon] \le c^{-n \epsilon^2}$.
				When $G_n$ is an average of i.i.d. \textbf{sub-Gaussian} variables $Z_i$
				 we can bound the moment generating function and get the desired bound
				 (\textbf{Hoeffding's inequality} for bounded random variables).
				 Sub-Gaussian random variables include Gaussian and bounded random variables
				 (it is convenient to assume our loss or data is bounded),
				 but not Laplace random variables, which has heavier tails
				 (there, we need sub-exponential bounds).
				When $G_n$ is the result of applying a function with bounded differences
				 to i.i.d. variables, then \textbf{McDiarmid's inequality} gives us the same bound.
			Complexity control
				For a single hypothesis, we can directly apply the tail bound
				 to control the difference $L(h) - \hat L(h)$.
				 However, we seek \textbf{uniform convergence} over all $h \in \sH$.
				Finite hypothesis classes: $\log |\sH|$ (use simple union bound)
				For infinite hypothesis classes, the key intuition is that the complexity
				 of $\sH$ is described by \textbf{how it acts on $n$ data points}.
				 Formally, \textbf{symmetrization} (introduce ghost dataset and Rademacher variables)
				 reveals this.
				The complexity is the \textbf{shattering coefficient} $s(\sH, n)$
				 (technically of the loss class $\sA$).
				 By \textbf{Sauer's lemma}, the shattering coefficient can be upper bounded using the \textbf{VC dimension} $\VC(\sH)$.
				Rademacher complexity $R_n(\sH)$ measures how well $\sH$ can fit random binary labelings (noise).
				 Rademacher complexity is nice because of the numerous compositional properties (convex hull, Lipschitz composition, etc.)
					By \textbf{Massart's finite lemma}, we can relate $R_n(\sH)$ to the shattering coefficient.
					For $L_2$ norm constrained linear functions, we used linearity and Cauchy-Schwartz, which enables us to analyze kernels.
					For $L_1$ norm constrained linear functions, we used the fact that the $L_1$ polytope is really simple as it only has $2d$ vertices.
		Other paradigms
			We also studied two alternative pardigms for analyzing learning,
			 both of which were motivated by the need for greater nuance.
			 Let $A$ be any learning algorithm that maps training data $S$ to a hypothesis $\hat h \in \sH$.
			 The algorithm $A$ could be the ERM, but it need not be.
			Typical uniform convergence bounds yield results that depend on the complexity of the entire
			 hypothesis class $\sH$:
			 \eqn{L(A(S)) - \hat L(A(S)) \le \SomeFunction(\sH).}
			Algorithmic stability allows us to obtain a bound that depends
			 on properties of the algorithm $A$ (i.e., its stability $\beta$) rather than $\sH$.
			 We obtained bounds of the form:
			 \eqn{L(A(S)) - \hat L(A(S)) \le \SomeFunction(A).}
			PAC-Bayesian bounds allow us to incorporate the prior into the analysis
			 without sacrificing objective rigor.
			 We obtained bounds of the form:
			 \eqn{L(A(S)) - \hat L(A(S)) \le \SomeFunction(A(S)).}
			 Note that the bound depends on the output of the algorithm.
		 Of course, algorithmic stability and PAC-Bayes bounds only bound the expected
		 and empirical risks, not the excess risk, which requires the heavyweight uniform convergence.
		 Intuitively, one expects this: to bound excess risk $A(S)$ has to be related to $h^*$ somehow,
		 not just be stable or be based on a possibly incorrect prior.
	References
		Bousquet/Boucheron/Lugosi, 2008: Introduction to Statistical Learning Theory
			http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2819.pdf
		Martin Wainwright's lecture notes
			http://www.eecs.berkeley.edu/~wainwrig/stat241b
		Peter Bartlett's lecture notes
			http://www.cs.berkeley.edu/~bartlett/courses/281b-sp08
		Sham Kakade's lecture notes
			http://stat.wharton.upenn.edu/~skakade/courses/stat928
		Tomaso Poggio and Lorenzo Rosasco's lecture notes
			http://www.mit.edu/~9.520/spring11/
		Bartlett/Mendelson, 2002: Rademacher and Gaussian Complexities: Risk Bounds and Structural Results
			http://maths-people.anu.edu.au/~mendelso/papers/dt2.pdf
		Kakade/Sridharan/Tewari, 2008: On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization
			http://research.microsoft.com/en-us/um/people/skakade/papers/ml/rad_paper.pdf
		Bosquet/Elisseeff, 2002: Stability and Generalization
			http://machinelearning.wustl.edu/mlpapers/paper_files/BousquetE02a.pdf
		David McAllester's notes on PAC-Bayesian bounds
			http://ttic.uchicago.edu/~dmcallester/ttic101-07/lectures/generalization/generalization.pdf

!verbatim \newpage
Kernel methods
	!verbatim \lecture{10}
	Motivation \currlecture
		So far in this class, we have focused on studying the excess risk $L(\hat h) - L(h^*)$,
		 which measures how far our estimated predictor $\hat h$
		 is away from the best possible predictor $h^* \in \sH$ in terms of expected risk.
		 But this is only half of the story, since the expected risk that we care about
		 is the sum of the \blue{excess risk} and the \red{best expected risk of the hypothesis class}:
		 \eqn{L(\hat h) = \blue{L(\hat h) - L(h^*)} + \red{L(h^*)}.}
		 The latter term has to do with how expressive $\sH$ is.
		We have mainly focused on linear models,
		 where the prediction is a function of the inner product $\inner{w}{x}$
		 between a weight vector $w \in \R^d$ and an input $x \in \R^d$
		 (e.g., for regression, the prediction function is just $f(x) = \inner{w}{x}$).
		 However, real data often exhibit highly non-linear relationships which are important to model.
		But we can sneakily replace $\inner{w}{x}$ with $\inner{w}{\phi(x)}$,
		 where $\phi : \sX \to \R^d$ is an arbitrary feature map:
			Example: $\phi(x) = (1, x, x^2)$ for $x \in \R$
			Example: $\phi(x) = (\text{count of \emph{a} appearing in $x$}, \dots)$ for a string $x$.
		 Note that $x$ does not even need to be a real vector.
		 In general, we assume that $x \in \sX$ for some set $\sX$ of 
		 all possible inputs
		 (we won't assume any further structure on $\sX$ for now).
		Therefore, we can represent very expressive \word{non-linear} functions by making $\phi(x)$ complex,
		 but the problem is that $\phi(x)$ might have to be very high-dimensional
		 in order to attain the desired degree of expressiveness,
		 resulting in computationally expensive algorithms.
		But perhaps we can cut down on computation.
			Suppose we are trying to minimize the average squared loss over $n$ training examples:
			 \eqn{\hat L(w) = \inv{n} \sum_{i=1}^n \half (y^{(i)} - \inner{w}{\phi(x^{(i)})})^2.}
			 Optimization algorithms typically access the function only through the gradient, which is:
			 \eqn{\nabla \hat L(w)
			 &= \inv{n} \sum_{i=1}^n (y^{(i)} - \inner{w}{\phi(x^{(i)})}) \blue{\phi(x^{(i)})}.
			 }
			 An algorithm that only accumulates (scaled) gradients will ultimately produce a weight vector $w$
			 that can be written as a linear combination of the feature vectors of the data points
			 (we will revisit this property in much greater generality when we study the representer theorem):
			 \eqn{w = \sum_{i=1}^n \alpha_i \phi(x^{(i)}).}
			 Given a weight vector of this form, we can make predictions as follows:
			 \eqn{\inner{w}{\phi(x)} = \sum_{i=1}^n \alpha_i \blue{\inner{\phi(x^{(i)})}{\phi(x)}}.}
			The key is that predictions only depend on the \word{inner product} between the feature vectors. 
			 This suggests we can work with in high (even infinite!) dimensions as long as we can compute
			 this inner product efficiently.
			 The computational tradeoff is as follows:
			 we store the $\alpha_i$'s ($n$ numbers) rather than $w$ ($d$ numbers).
			 If the number of features $d$ is much larger than the number of training examples $n$,
			 then we win on space.
			 On the other hand, if we have a lot of data ($n$), we must resort to approximations, as we'll see later.
			\exampleHeading{quadraticKernel}{computing with quadratic features}
				Let the raw input be $x \in \R^b$.
				Feature map with all quadratic terms:
				 \eqn{\phi(x) = [x_1^2, \dots, x_b^2, \sqrt{2} x_1 x_2, \dots, \sqrt{2} x_1 x_b, \sqrt{2} x_2 x_3, \dots, \sqrt{2} x_2 x_b, \dots, \sqrt{2} x_{b-1} x_b],}
				 There are $O(b^2)$ dimensions.
				Explicit computation: $\inner{w}{\phi(x)}$ takes $O(b^2)$ time.
				Implicit computation: $\inner{\phi(x)}{\phi(x')} = \inner{x}{x'}^2$, which takes $O(b)$ time.
				 $\inner{w}{\phi(x)}$ requires doing this for each of $n$ data points, which takes $O(bn)$ time.
			It's important to realize that mathematically, we're still computing the same function values as before.
			 All we've done is perform a computational sleight of hand, known as the \word{kernel trick}.
			Aside: sometimes, we can compute dot products efficiently in high (even infinite) dimensions without using the kernel trick.
			 If $\phi(x)$ is sparse (as is often the case in natural language processing),
			 then $\inner{\phi(x)}{\phi(x')}$
			 can be computed in $O(s)$ time rather than $O(d)$ time,
			 where $s$ is the number of nonzero entries in $\phi(x)$.
		Since these algorithms only depend on the inner product,
		 maybe we can just cut to the chase and directly write down functions $k$
		 that correspond to inner products:
		 $k(x,x') = \inner{\phi(x)}{\phi(x')}$ for some feature map $\phi$.
		This is a key conceptual change: it shifts our perspective from
		 thinking in terms of features of single inputs to
		 thinking about notions of ``similarity'' $k(x,x')$ between two inputs $x$ and $x'$.
		 Sometimes, similarities might be more convenient from a modeling point of view.
		Therefore, kernels offers two things:
			A \emph{computationally} efficient way of working with high (and even infinite) dimensional $\phi(x)$ \word{implicitly}.
			A different perspective on features, which can be more natural from a \emph{modeling} perspective for certain applications,
			 and help us understand our model family, even if we computationally end up working with features.
	Kernels: definition and examples \currlecture
		We now define kernels formally.
		 While we have suggested that kernels are related to feature maps,
		 we hold back on establishing their formal connection until later to avoid confusion.
		\definitionHeading{kernel}{\word{kernel}}
			A function $k : \sX \times \sX \to \R$ is a positive semidefinite kernel (or more simply, a kernel)
			 iff for every finite set of points $x_1, \dots, x_n \in \sX$,
			 the \word{kernel matrix} $K \in \R^{n \times n}$ defined by $K_{ij} = k(x_i, x_j)$ is positive semidefinite.
		Let us now give some examples of kernels,
		 and then prove that they are indeed valid according to \refdef{kernel}.
		 Assume in the following that the input space is $\sX = \R^b$.
		 Note that we can visualize a kernel for $\sX = \R$ by fixing $x'$ to some value (say $1$) and plotting $k(x,1)$.
			Linear kernel: \eqn{k(x,x') = \inner{x}{x'}.}
			Polynomial kernel: \eqn{k(x,x') = (1 + \inner{x}{x'})^p.}
				Intuition: for boolean features ($x \in \{0,1\}^b$), this corresponds to forming conjunctions of the original features.
				Here, we can check that the corresponding dimensionality (number of features) is $O(b^p)$, which is exponential in $p$.
			Gaussian kernel: \eqn{k(x,x') = \exp\p{\frac{-\|x-x'\|_2^2}{2\sigma^2}}.}
				A Gaussian kernel puts a smooth bump at $x$.
				The bandwidth parameter $\sigma^2$ governs how smooth the functions should be:
				 larger $\sigma^2$ yields more smoothness.
				The corresponding dimensionality is infinite (as we'll see later), so computationally,
				 we really have no choice but to work with kernels.
				The Gaussian is in some sense the ``go to'' kernel in machine learning,
				 because it defines a very expressive hypothesis class, as we will see.
			Sequence mis-match kernel:
				The above kernels have been defined for continuous input spaces $\sX \subseteq \R^b$,
				 but we can define kernels on any type of object.
				Suppose $\sX = \Sigma^*$ is the set of all possible sequences (strings) over some alphabet $\Sigma$.
				 We want to define a kernel between two strings which measures their similarity,
				 a problem that arises in NLP and computational biology.
				 For example, consider the strings \emph{format} and \emph{fmt}.
				A string $u \in \sX$ is a subsequence of $x \in \sX$
				 if there exists a sequence of indices $\bi = (i_1, \dots, i_{|u|})$
				 such that $1 \le i_1 < \cdots < i_{|u|} \le |x|$ such that $u_j = x_{i_j}$.
				 In this case, we write $u = x(\bi)$.
				Note that $x$ has exponentially many subsequences in general.
				Now define a kernel between two sequences $x,x'$ to be a weighted number of common subsequences:
				 \eqn{k(x,x') = \sum_{u \in \Sigma^*} \sum_{(\bi,\bj): x(\bi) = x'(\bj) = u} \lambda^{|\bi| + |\bj|},}
				 for some decay parameter $0 \le \lambda \le 1$.
				 Smaller values of $\lambda$ discount longer subsequences more.
		Non-example: $k(x,x') = \1[\|x - x'\|_2 \le 1]$
			Exercise: show that $k$ is not a kernel function.
		Now we show the above kernels (linear, polynomial, Gaussian) are actually valid according to \refdef{kernel}.
		 Let $x_1, \dots, x_n \in \sX$ be any points.
		 We have to check that the kernel matrix $K$ formed by $K_{ij} = k(x_i, x_j)$ is positive semidefinite.
		 But we first start with some general principles that will allow us to easily check whether a given function $k$
		 is a kernel easily.
		General principles for checking kernels
			Base case: for any function $f : \sX \to \R$, $k(x,x') = f(x) f(x')$ is positive semidefinite.
				Proof: the kernel matrix can be written as $K = u u^\top \succeq 0$, where $u = (f(x_1), \cdots, f(x_n))$.
			Recursive case:
			 given two kernels $k_1,k_2$, we can create new kernels $k$.
			 Note that to check that $k$ is a kernel, it suffices to check that $K_1, K_2 \succeq 0 \Rightarrow K \succeq 0$,
			 where $K_1,K_2,K$ are the corresponding kernel matrices of $k_1,k_2,k$.
			Sum (recursive case): $k(x,x') = k_1(x,x') + k_2(x,x')$
				Since positive semidefiniteness is closed under addition, $K = K_1 + K_2 \succeq 0$.
			Product (recursive case): $k(x,x') = k_1(x,x') k_2(x,x')$
				$K = K_1 \circ K_2$ corresponds to elementwise product.
				Since $K_1,K_2$ are positive semidefinite, we can take their eigendecompositions:
					$K_1 = \sum_{i=1}^n \lambda_i u_i u_i^\top$
					$K_2 = \sum_{j=1}^n \tau_j z_j z_j^\top$
				Taking the elementwise product yields the following eigendecomposition, showing that $K$ is also positive semidefinite:
					$K = \sum_{i=1}^n \sum_{j=1}^n \lambda_i \tau_j (u_i \circ z_j) (u_i \circ z_j)^\top$,
					 where $\circ$ denotes elementwise products.
		Using these three principles, we can show that the linear, polynomial, and Gaussian kernels are valid.
			Linear kernel: sum over kernels defined by functions of the form $f(x) = x_i$.
			Polynomial kernel: given the linear kernel $\inner{x}{x'}$, add $1$ (which is a kernel); this
			 shows that the sum $\inner{x}{x'} + 1$ is a kernel.
			 By the product property, $(\inner{x}{x'} + 1)^2$ is also a kernel.
			 Repeat $p-1$ times to show that $(\inner{x}{x'} + 1)^p$ is a kernel.
			Gaussian kernel:
				Rewrite \eqn{k(x,x') = \exp\p{\frac{-\|x\|_2^2}{2\sigma^2}} \exp\p{\frac{-\|x'\|_2^2}{2\sigma^2}} \exp\p{\frac{\inner{x}{x'}}{\sigma^2}}.}
				The first two factors are handled by the base case.
				For the third factor, take the Taylor expansion:
				 \eqn{\exp\p{\frac{\inner{x}{x'}}{\sigma^2}} = 1 + \frac{\inner{x}{x'}}{\sigma^2} + \half \frac{\inner{x}{x'}^2}{\sigma^4} + \inv{6} \frac{\inner{x}{x'}^3}{\sigma^6} + \cdots}
				 Each term is just a homogenous polynomial kernel.
				 Summing a finite number of terms yields a kernel.
				 Kernels are closed under taking limits (since the set of positive semidefinite matrices is closed).
	Three views of kernel methods \currlecture
		We now start laying the mathematical foundation for kernel methods.
		 Specifically, we will develop three views of kernel methods, as illustrated in \reffig{kernelViews}.
			Feature map $\phi$: maps from a data point $x \in \sX$ to an element of an inner product space (the feature vector).
			 This allows us to think about properties of single data points.
			Kernel $k$: takes two data points $x,x' \in \sX$ and returns a real number.
			 This allows us to think about similarity between two data points.
			RKHS $\sH$: a set of functions $f : \sX \to \R$ equipped a norm $\|\cdot\|_\sH$ for measuring the complexity of functions.
			 This allows us to think about the prediction function $f$ itself.
		 We will define each of the three views separately, but eventually show that they are all in a sense equivalent.
		 \Fig{figures.slides/kernelViews}{0.4}{kernelViews}{The three key mathematical concepts in kernel methods.}
		First, we need a formal notion of infinite feature vectors (the range of a feature map $\phi$)
		 that generalizes $\R^d$.
		\definitionHeading{innerProductSpace}{\word{Hilbert space}}
			A Hilbert space $\sH$ is an complete\footnote{
			 Completeness means that all Cauchy sequences (in which elements get closer and closer to each other) converge to some element in the space (see \refdef{complete}).
			 Examples: set of real numbers is complete, set of rational numbers is not.} vector space with an inner product
			 $\inner{\cdot}{\cdot} : \sH \times \sH \to \R$ that satisfies the following properties:
				Symmetry: $\inner{f}{g} = \inner{g}{f}$
				Linearity: $\inner{\alpha_1 f_1 + \alpha_2 f_2}{g} = \alpha_1 \inner{f_1}{g} + \alpha_2 \inner{f_2}{g}$
				Positive definiteness: $\inner{f}{f} \ge 0$ with equality only if $f = 0$
			 The inner product gives us a norm: $\|f\|_\sH \eqdef \sqrt{\inner{f}{f}}$.
			Examples
				Euclidean space: $\R^d$, with $\inner{u}{v} = \sum_{i=1}^d u_i v_i$
				Square summable sequences: $\ell^2 = \{ (u_i)_{i \ge 1} : \sum_{i=1}^\infty u_i^2 < \infty \}$, with $\inner{u}{v} = \sum_{i=1}^\infty u_i v_i$.
				Square integrable functions on $[0,1]$: $L^2([0,1]) = \{ f : \int_0^1 f(x)^2 dx < \infty \}$, with $\inner{f}{g} = \int_0^1 f(x) g(x) dx$.\footnote{
				 Technically, $L^2([0,1])$ is not a vector space since a function $f$ which is non-zero on a measure zero set will still have $\|f\|_\sH = 0$.
				 But the quotient space (with respect to functions $f$ with $\|f\|_\sH = 0$) is a vector space.}
		\definitionHeading{featureMap}{\word{feature map}}
			Given a Hilbert space $\sH$, a \textbf{feature map} $\phi : \sX \to \sH$
			 takes inputs $x \in \sX$ to infinite feature vectors $\phi(x) \in \sH$.
		\theoremHeading{featureToKernels}{feature map defines a kernel}
			Let $\phi : \sX \to \sH$ be a feature mapping some input space $\sX$ to a Hilbert space $\sH$.
			Then $k(x,x') \eqdef \inner{\phi(x)}{\phi(x')}$ is a kernel.
		Proof:
			The key idea is that the definition of the kernel only needs to look at $n$ points,
			 which reduces everything to a finite problem.
			Let $x_1, \dots, x_n$ be a set of points, and let $K$ be the kernel matrix where
			 $K_{ij} = \inner{\phi(x_i)}{\phi(x_j)}$.
			To show that $K$ is positive semidefinite, take any $\alpha \in \R^n$.
			 We have
			 \eqn{\alpha^\top K \alpha
			 &= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \inner{\phi(x_i)}{\phi(x_j)} \\
			 &= \inner{\sum_{i=1}^n \alpha_i \phi(x_i)}{\sum_{i=j}^n \alpha_j \phi(x_j)} \\ & \ge 0,}
			 where we use linearity of the inner product.
		\theoremHeading{kernelEquiv}{kernel defines feature maps}
			For every kernel $k$ (\refdef{kernel}),
			 there exists a Hilbert space $\sH$ and a feature map $\phi : \sX \to \sH$ such that
			 $k(x,x') = \inner{\phi(x)}{\phi(x')}$.
		We will prove \refthm{kernelEquiv} later since it requires more sophisticated machinery (RKHSes).
		 But to get some intuition, let's prove it for the case where the number of inputs $\sX$ is finite.
			Let $\sX = \{x_1, \dots, x_n\}$ and define the kernel matrix $K$
			 (which defines the entire kernel).
			Since $K \in \R^{n \times n}$ is positive semidefinite, we can write it as $K = \Phi \Phi^\top$.
			 For example we can take an eigendecomposition $K = U D U^\top$ and let $\Phi = U D^{1/2}$
			 or the Cholesky decomposition $K = L L^\top$.
			Let the feature vector $\phi(x_i) \in \R^n$ be the $i$-th row of $\Phi$.
			 We can verify that $K = \Phi \Phi^\top$ (equivalently, $k(x_i, x_j) = \inner{\phi(x_i)}{\phi(x_j)}$).
			Note: the feature map is not unique, since we can also define
			 an alternate feature matrix $\Phi' = \Phi Q$ for any orthogonal matrix $Q$.
			 In this light, kernels are a more ``pure'' way of defining models,
			 because feature vectors have this rotational indeterminancy.
			If the input space $\sX$ infinite,
			 then we need to generalize our notion of feature vector from $\R^n$ to
			 an infinite dimensional space.
			 What is that space?
	Reproducing kernel Hilbert spaces (RKHS) \currlecture
		We will now introduce RKHSes (a type of Hilbert space), the third view on kernel methods.
		 RKHSes allow us to work directly on the prediction \emph{function}.
		 Concretely, in linear regression,
		 we fit a weight vector $w$, constraining or regularizing the norm $\|w\|_2$,
		 and we use $w$ to predict on a new input $x$ via $x \mapsto \inner{w}{\phi(x)}$.
		 But can we get a handle on this prediction function $x \mapsto f(x)$ directly
		 as well as a norm $\|f\|_\sH$ measuring the complexity of $f$?
		Our first reaction might be to consider a Hilbert space over functions $f : \sX \to \R$.
		 But not all Hilbert spaces are not suitable for machine learning.
			For example, consider $\sH = L^2([0,1])$.
			 Recall that every $f \in \sH$ is actually an equivalence class
			 over functions which differ on a measure zero set,
			 which means pointwise evaluations $f(x)$ at individual $x$'s is not even defined.
			This is highly distressing
			 given that the whole point is to learn an $f$ for the purpose of doing pointwise evaluations
			 (a.k.a. prediction)!
			RKHSes remedy this problem by making pointwise evaluations really nice, as we'll see.
		\definitionHeading{functional}{\word{bounded functional}}
			Given a Hilbert space $\sH$, a functional $L : \sH \to \R$ is bounded iff there exists an $M < \infty$ such that
			 \eqn{|L(f)| \le M \|f\|_\sH \text{ for all $f \in \sH$}.}
			Example: $\sH = \R^d$ with the usual inner product, $L(f) = \inner{c}{f}$ is bounded
			 (with $M = \|c\|_2$ by Cauchy-Schwartz)
		\definitionHeading{evaluationFunctional}{\word{evaluation functional}}
			Let $\sH$ be a Hilbert space consisting of functions $f : \sX \to \R$.
			For each $x \in \sX$, define the evaluation functional $L_x : \sH \to \R$ as
			 \eqn{\boxed{L_x(f) \eqdef f(x).}}
			Example: for $\sX = \R^d$ and $\sH = \{ f_c : c \in \R^d \}$ where $f_c(x) = \inner{c}{x}$ be linear functions,
			 then the evaluation functional is $L_x(f_c) = \inner{c}{x}$.
			Intuitively, the evaluation functional is a projection operator that turns a function $f$ into one component $f(x)$.
			 Hence, evaluation functionals are linear.  This will be important later.
		\definitionHeading{RKHS}{\word{reproducing kernel Hilbert space}}
			A reproducing kernel Hilbert space $\sH$ is a Hilbert space over functions $f : \sX \to \R$
			 such that for each $x \in \sX$, the \word{evaluation functional} $L_x$ is bounded.
		Non-example
			Let $\sH$ be the set of all square integrable continuous functions from $[0,1]$ to $\R$.
			Consider $f_\epsilon(x) = \max(0, 1 - \frac{|x - \half|}{\epsilon})$,
			 which is zero except for a small spike at $x = \half$ up to $f(x) = 1$.
			 Note that $\|f_\epsilon\|_\sH \to 0$ as $\epsilon \to 0$.
			Consider the evaluation functional $L_\half$.
			 Note that $L_\half(f_\epsilon) = f_\epsilon(\half) = 1$.
			 So there cannot exist an $M$ such that $L_\half(f_\epsilon) \le M \|f_\epsilon\|_\sH$ for all $\epsilon > 0$.
			So this $\sH$ is not a RKHS.
			 Note that continuity ensures that functions are actually defined pointwise,
			 not just up to a measure zero set.
			 The problem is that this $\sH$ is just too big of a set.
		 Having defined what an RKHS is, we now show the connection with kernels.  In particular:
			Each RKHS $\sH$ is associated with a unique kernel $k$ (\refthm{RKHSkernel})
			Each kernel $k$ is associated with a unique RKHS $\sH$ (\refthm{moore})
		\theoremHeading{RKHSkernel}{RKHS defines a kernel}
			Every RKHS $\sH$ over functions $f : \sX \to \R$ defines a unique kernel $k : \sX \times \sX \to \R$,
			 called the reproducing kernel of $\sH$.
		Proof (construction of the kernel)
			Note that $L_x$ is a \emph{linear} functional:
			 $L_x(c f) = c L_x(f)$ and $L_x(f + g) = L_x(f) + L_x(g)$.
			The \word{Riesz representation theorem} states that
			 all \emph{bounded linear functionals} $L$ on a Hilbert space
			 can be expressed as an inner product $L(f) = \inner{R}{f}$ for a \emph{unique} $R \in \sH$.
			Applying this theorem to the evaluation functionals $L_x$,
			 we can conclude that for each $x \in \sX$, there exists a unique \word{representer} $R_x \in \sH$
			 such that $L_x(f) = \inner{R_x}{f}$.
			 Recall that we also have $L_x(f) = f(x)$ by definition.
			 Combining yields the \word{reproducing property}:
			 \eqn{\boxed{f(x) = \inner{R_x}{f} \text{ for all $f \in \sH$}.}}
			 This is the key property: \emph{function evaluations can be expressed as inner products}.
			Now let's define a function $k$:
			 \eqnl{reproducingKernel}{\boxed{k(x,x') \eqdef R_x(x').}}
			 Applying the reproducing property one more time with $f = R_x$ yields
			 \eqn{\blue{k(x,x')} = R_{x}(x') = \blue{\inner{R_x}{R_{x'}}}.}
			 If we define a feature map $\phi(x) \eqdef R_x$, we can invoke \refthm{featureToKernels} to conclude
			 that $k(x,x')$ is a valid kernel.
			In summary, any RKHS $\sH$ gives rise to a kernel $k$ called the \word{reproducing kernel} of $\sH$.
			 The key is the Riesz representation, which turns function evaluations into inner products.
		 To complete the connection between RKHSes and kernels,
		 we need to show the converse, namely that a kernel defines an unique RKHS:
		\theoremHeading{moore}{\word{Moore-Aronszajn theorem}}
			For every kernel $k$, there exists a unique RKHS $\sH$ with reproducing kernel $k$.
		Proof sketch:
			Let $k$ be a kernel.  We will construct a RKHS $\sH$ from the functions $\{ k(x, \cdot) : x \in \sX \}$.
			First, define $\sH_0$ to contain all finite linear combinations of the form
			 \eqn{f(x) = \sum_{i=1}^n \alpha_i k(x_i, x),}
			 for all $n, \alpha_{1:n}, x_{1:n}$.
			 By construction, $\sH_0$ is a vector space (not necessarily complete though).
			 This is a very natural class of functions: just a linear combination of basis functions
			 centered around the points $x_1, \dots, x_n$.
			Second, define the inner product between $f(x) = \sum_{i=1}^n \alpha_i k(x_i, x)$
			 and $g(x) = \sum_{j=1}^{n'} \beta_j k(x_j', x)$ as follows:
			 \eqnl{h0InnerProduct}{\inner{f}{g} \eqdef \sum_{i=1}^n \sum_{j=1}^{n'} \alpha_i \beta_j k(x_i, x_j').}
			 Let's check that our definition of $\inner{\cdot}{\cdot}$
			 is an actual inner product:
				Symmetry ($\inner{f}{g} = \inner{g}{f}$): by symmetry of $k$
				Linearity ($\inner{\alpha_1 f_1 + \alpha_2 f_2}{g} = \alpha_1 \inner{f_1}{g} + \alpha_2 \inner{f_2}{g}$):
				 by definition of $f$ and $g$ (they are just a linear sum of terms).
				Positive definiteness ($\inner{f}{f} \ge 0$ with equality only if $f = 0$):
					For any $f \in \sH_0$, we have $\inner{f}{f} = \alpha^\top K \alpha \ge 0$
					 by the positive semidefinite property of kernels.
					Now we will show that $\inner{f}{f} = 0$ implies $f = 0$.
					 Let $f(x) = \sum_{i=1}^n \alpha_i k(x_i, x)$.
					 Take any $x \in \sX$ and define $c = [k(x_1, x), \dots, k(x_n, x)]^\top$.
					 Since \eqn{\p{\begin{array}{cc} K & c \\ c^\top & k(x,x) \end{array}} \succeq 0,}
					 we must have that 
					 \eqnl{proofdet}{\alpha^\top K \alpha + 2 b c^\top\alpha + b^2 k(x,x) \ge 0}
					 for all $b$.
					 Note that $\inner{f}{f} = \alpha^\top K \alpha = 0$.
					 We now argue that $c^\top\alpha = 0$.
					 If $c^\top\alpha > 0$, then as $b$ approaches $0$ from the negative side,
					 we have that the LHS of \refeqn{proofdet} is strictly negative (since the $b$ term will dominate the $b^2$ term), which is a contradiction.
					 If $c^\top\alpha < 0$, then as $b$ approaches $0$ from the positive side,
					 we get a contradiction as well.
					 Therefore, $f(x) = c^\top\alpha = 0$.
			So far we have a valid Hilbert space,
			 but we need to still check that all evaluation functionals $L_x$ are bounded
			 to get an RKHS.
			 Also, we should check that $R_x \eqdef k(x, \cdot)$ is indeed a representer
			 of function evaluationals.
			 Take any $f \in \sH_0$.  Then:
			 \eqn{f(x)
			 &= \sum_{i=1}^n \alpha_i k(x_i, x) \aside{definition of $f$} \\
			 &= \inner{f}{k(x, \cdot)} \aside{definition of inner product} \\
			 &= \inner{R_x}{f} \aside{definition of $R_x$}.}
			 Boundedness of $L_x$ follows from $|L_x(f)| = |\inner{f}{k(x,\cdot)}| \le \|f\|_\sH \|k(x, \cdot)\| = \|f\|_\sH k(x,x)$ by Cauchy-Schwartz.
			Finally, let $\sH$ be the completion of $\sH_0$ (by including all limit points of sequences in $\sH_0$).
			 This is the only part of the proof that we're punting on.
			 For details, see the references at the end of this section.
			This proves \refthm{kernelEquiv}
			 because the RKHS $\sH$ is an inner product space by construction.
		Summary
			A feature map $\phi : \sX \to \sH$: maps points in $\sX$ to some inner product space $\sH$.
			A (positive semidefinite) kernel $k : \sX \times \sX \to \R$ has the property
			 that every derived kernel matrix $K$ is positive semidefinite.
			A reproducing kernel Hilbert Space (RKHS) $\sH$ containing functions $f : \sX \to \R$ such that function evaluations are bounded linear operators.
			Equivalences
				$f(x) = \sum_{i=1}^\infty \alpha_i k(x_i, x)$, $R_x = k(x, \cdot)$: Moore-Aronszajn establishes connection between kernels and RKHSes
				$\phi(x) = R_x$: can set feature map (not unique) to map $x$ to the representer of $x$ in the RKHS
				$k(x,x') = \inner{\phi(x)}{\phi(x')}$: every kernel $k$ corresponds to some inner product (via RKHS) and vice-versa (easy)
	!verbatim \lecture{11}
	Learning using kernels \currlecture
		We have established that kernels $k$ provide a space of functions $\sH$;
		 this is the hypothesis class.\footnote{For regression, our prediction at $x$ is simply $f(x)$, where $f \in \sH$.
		 For classification and ranking problems, we need to pass the function values through some non-linear transformation.}
		 Now let's talk about learning, which is about combining the hypothesis class $\sH$ with actual data.
		Let's start with kernelized ridge regression, where we obtain examples $\{(x_i, y_i)\}_{i=1}^n$,
		 and want to find a function $f \in \sH$ that fits the data,
		 where $\sH$ is an RKHS.
		 A natural objective function is to penalize the squared loss plus a penalty for the complexity of $f$,
		 where the complexity is the RKHS norm:
		 \eqn{f^* \in \arg\min_{f \in \sH} \sum_{i=1}^n \half (f(x_i) - y_i)^2 + \frac{\lambda}{2} \|f\|_\sH^2.}
		More generally, a learning problem can be posed as the following optimization problem:
		 \eqnl{rkhsLearn}{f^* \in \arg\min_{f \in \sH} L(\{(x_i, y_i, f(x_i))\}_{i=1}^n) + Q(\|f\|_\sH^2),}
		 where
			$L : (\sX \times \sY \times \R)^n \to \R$ is an arbitrary loss function on $n$ examples.
				Example (regression): $L(\{(x_i, y_i, f(x_i))\}_{i=1}^n) = \sum_{i=1}^n \half (f(x_i) - y_i)^2$.
			$Q : [0, \infty) \to \R$ is a strictly increasing function (regularizer).
				Example (quadratic): $Q(\|f\|_\sH^2) = \frac{\lambda}{2} \|f\|_\sH^2$.
		 This optimization problem may seem daunting since it is optimizing over a potentially very large function space $\sH$.
		 But the following representer theorem reassures us that all minimizers can be written as a linear combination of the kernel functions
		 evaluated at the training points.
		\theoremHeading{representer}{representer theorem}
			Let $V$ denote the span of the representers of the training points:
			 \eqn{V \eqdef \text{span}(\{ k(x_i, \cdot) : i = 1, \dots, n \}) = \left\{ \sum_{i=1}^n \alpha_i k(x_i, \cdot) : \alpha \in \R^n \right\}.}
			Then all minimizers $f^*$ of \refeqn{rkhsLearn} satisfy $f^* \in V$.
		Proof
			FIGURE: [projection of $f^*$ onto $V$]
			The key is to use the fact that an RKHS has an inner product structure,
			 which allows us to use linear algebra.
			Define the orthogonal complement:
			 \eqn{V_\perp = \{ g \in \sH : \inner{f}{g} = 0 \text{ for all } f \in V \}.}
			Any $f \in \sH$ can be decomposed in to a part in the span of the examples and an orthogonal part:
			 \eqn{f^* = f + f_\perp,}
			 where $f \in V$ and $f_\perp \in V_\perp$.
			The idea is that the loss is unchanged by $f_\perp$ but the regularizer grows with non-zero $f_\perp$,
			 so we must have $f_\perp = 0$.
			The loss depends on $f^*$ only through $\{ f^*(x_j) : j = 1, \dots, n \}$, which can be written as:
			 \eqn{f^*(x_j) = f(x_j) + \inner{f_\perp}{k(x_j, \cdot)}.}
			 The second term is zero, so the loss doesn't depend on $f_\perp$.
			The regularizer:
			 \eqn{Q(\|f^*\|_\sH^2) = Q(\|f\|_\sH^2 + \|f_\perp\|_\sH^2).}
			 Since $Q$ is strictly monotonic and $f^*$ is a minimizer, we must have $f_\perp = 0$.
			Therefore, $f^* \in V$.
		Remark: the representer theorem does not require the loss function $L$ to be convex.
		The representer theorem tells us $\alpha$'s exist, but how to find the $\alpha$'s depends on the actual loss function and regularizer.
		 Let's now look at some examples.
		\exampleHeading{kernelRegression}{Kernelized ridge regression}
			Recall the optimization problem for regression:
			 \eqn{\min_{f \in \sH} \sum_{i=1}^n \half (f(x_i) - y_i)^2 + \frac{\lambda}{2} \|f\|_\sH^2.}
			 By the representer theorem, we have the equivalent optimization problem:
			 \eqn{\min_{\alpha \in \R^n} \sum_{i=1}^n \half \p{\sum_{j=1}^n \alpha_j k(x_i, x_j) - y_i}^2 + \frac{\lambda}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(x_i, x_j).}
			Letting $K \in \R^{n \times n}$ be the kernel matrix and
			 $Y \in \R^{n}$ denote the vector of outputs, we have:
			 \eqn{\min_{\alpha \in \R^n} \half \|K \alpha - Y\|_2^2 + \frac{\lambda}{2} \alpha^\top K \alpha.}
			 Differentiating with respect to $\alpha$ and setting to zero:
			 \eqn{K (K \alpha - Y) + \lambda K \alpha = 0.}
			 Rearranging:
			 \eqn{K (K + \lambda I) \alpha = K Y.}
			 Solving yields a solution:
			 \eqn{\boxed{\alpha = (K + \lambda I)^{-1} Y.}}
			 Note that the solution is not necessarily unique, since we could add any vector in the null space of $K$,
			 but there's no reason to consider them.
			To predict on a new example $x$, we form kernel evaluations $c \in \R^n$ where $c_i = k(x_i, x)$,
			 and then predict 
			 \eqn{y = c^\top \alpha.}
		\exampleHeading{kernelSVM}{SVM classification}
			This was done in CS229, so we won't go through it again.
			Primal ($y_i \in \{ -1, +1 \}$):
			 \eqn{\min_{f \in \sH} \sum_{i=1}^n \max\{ 0, 1 - y_i f(x_i) \} + \frac{\lambda}{2} \|f\|_\sH^2.}
			Dual (define $\tilde K_{ij} = y_i y_j K_{ij}$):
			 \eqn{\min_{\alpha \in \R^n} -\bone^\top\alpha + \alpha^\top \tilde K \alpha \quad \text{subject to} \quad 0 \le \alpha_i \le \inv{\lambda}, Y^\top\alpha = 0.}
			 The dual is computed by taking the Lagrange dual of the primal optimization problem.
		\exampleHeading{kernelPCA}{Kernel PCA}
			Review of PCA
				Recall in (featurized) PCA, we want to find directions in our data with highest variance.
				Suppose we have data points $x_1, \dots, x_n$ and a feature map $\phi : \sX \to \R^d$.
				Assume that the data points are centered at zero: $\sum_{i=1}^n \phi(x_i) = 0$.
				Define the empirical covariance matrix as follows:
				 \eqn{C \eqdef \inv{n} \Phi^\top \Phi,}
				 where the $i$-th row of $\Phi$ is $\phi(x_i)$.
				Then PCA seeks to find the principal eigenvector of $C$:
				 \eqn{C v = \lambda v.}
				 In practice, take the first $r$ eigenvectors
				 $v_1, \dots, v_r$ (principal components) as an approximation of the entire feature space.
				 Note that the $v_i$'s form an orthonormal basis (have unit norm and are orthogonal).
				The squared reconstruction error of a new point $x$ is:
				 \eqn{\left\|\sum_{i=1}^r \inner{\phi(x)}{v_i} v_i - \phi(x)\right\|_2^2.}
				 For example, if we were doing anomaly detection, if a data point $x$ has a large reconstruction error, then $x$ is an anomaly.
			Heuristic derivation of kernel PCA
				By the representer theorem (or even more simply, by inspecting the form of the covariance matrix),
				 we have $v = \sum_{i=1}^n \alpha_i \phi(x_i) = \Phi^\top\alpha$, so an equivalent characterization is to project the vectors on the data points:
				 \eqn{\Phi C v = \lambda \Phi v.}
				Using the definition of $C$ and the fact that $\Phi v = K \alpha$,
				 we have
				 \eqn{\frac1n K^2 \alpha = \lambda K \alpha.}
				 Again, any solution to the following is a valid solution to the above (but we can always add spurious vectors in the null space of $K$):
				 \eqn{\boxed{\frac1n K \alpha = \lambda \alpha.}}
				In practice, we compute the eigendecomposition of $K$ and take the first $r$ eigenvectors as the approximation.
				 For simplicity, let's assume we just take one principal component $v \in \sH$.
			Computing in infinite dimensions
				Though the derivation assumed $\phi(x) \in \R^d$, the result is the same for $\phi(x) = k(x, \cdot) \in \sH$ in general.
				We won't go through the details, but the idea is to define a covariance operator (rather than a matrix) $C : \sH \to \sH$.
				 The intuition is the same.
				Recall the principal component is now a function $v \in \sH$ with $\|v\|_\sH = 1$,\footnote{To make $v$ a unit vector, we just rescale $v$ by $\|v(x)\|_\sH^{-1}$.}
				 where $v(x) = \sum_{i=1}^n \alpha_i k(x_i, x)$.
				The squared reconstruction error of a new point $x$ is:
				 \eqn{\|v(x) v - \phi(x)\|_\sH^2 = v(x)^2 - 2 v(x)^2 + k(x, x) = k(x, x) - v(x)^2.}
				 As expected, all we need are kernel evaluations.
			Interpretation
				The point of PCA is to reduce the dimensionality of the data, so it might seem strange at first
				 we would want to use kernel PCA to first increase the number of dimensions (possibly to infinity).
				This is actually okay, because the point of kernels 
				 is to reshape the data (in non-linear ways).
				 In doing so, we can expose better directions than those present in the original data.
				For example, if we use a quadratic kernel, we are saying that we believe the data lies close to a quadratic surface.
				 Of course, with more dimensions, statistical error in the directions could increase.
			!comment In particular, given data points $x_1, \dots, x_n$, we want to find a direction $f$ that minimizes the projected reconstruction error: \eqn{\min_{\|f\|_\sH \le 1} \sum_{i=1}^n \|\inner{k(x_i, \cdot)}{f} f - k(x_i, \cdot)\|_\sH^2.}
	!comment Gaussian processes (skipped in class)
		First, we developed RKHSes so that we could talk about the space of functions $\sH$ we'd like to use to model our data.
		We then saw how the inner product and reproducing kernel structure enabled us to do learning from an optimization point of view,
		 where the goal is to recover the best $f \in \sH$.
		Now, we will take RKHSes in a \word{Bayesian} direction,
		 resulting in Gaussian processes,
		 in which we will define a prior over functions and compute posteriors over functions given our data.
		One of the most useful aspects of working with a full posterior is that we can talk about \word{uncertainty} over predictions
		 in a natural way.
		\definitionHeading{gaussianProcess}{Gaussian process}
			A \word{Gaussian process} is a collection of random variables $\{ f(x) : x \in \sX \}$,
			 such that any finite subset has a joint multivariate Gaussian distribution.
			Treat $f$ as a random function, and we write
			 \eqn{f \sim \text{GP}(0, k),}
			 where %$\mu \in \sH$ is the mean function and
			 $k : \sX \times \sX \to \R$ is the \word{covariance} (kernel) function.\footnote{In practice, one can add a mean function but we use $0$ for clarity of presentation.}
			\word{Marginalization} property: function values are jointly distributed as follows:
			 \eqn{[f(x_1), \dots, f(x_n)] \sim \sN(0, K),}
			 where $K$ is the usual kernel matrix ($K_{ij} = k(x_i, x_j)$).
		Visualization
			We can sample Gaussian processes to visualize what kind of functions are actually encoded in our prior (kernel/covariance function).
			 This can be useful even if you are not taking a Bayesian approach.
			Later, we'll examine how the spectral properties of the kernel relate to smoothness properties of the functions.
		We now consider Gaussian process regression, which is basically kernelized regression with a prior over $f \in \sH$.  Here's the model:
			Prior ($\mu(F)$):\footnote{
			 We use $\mu(F)$ to denote the Gaussian process prior measure over functions $F \subseteq \sH$.}
			 \eqn{f \sim \text{GP}(0, k).}
			Likelihood ($p(y_i \mid x_i, f)$):
			 \eqn{y_i \sim \sN(f(x_i), \sigma^2).}
			Posterior (normalized product of prior and likelihood):
			 \eqn{\mu(df \mid X, Y) \propto \mu(df) \prod_{i=1}^n p(y_i \mid x_i, f).}
		Notation
			$X \in \sX^n$ is the vector of training inputs.
			$Y \in \R^n$ is the vector of training outputs.
			$x \in \sX$: new input
			$y \in \R$: new output
			$K \in \R^n$ is the usual kernel matrix over training points: $K_{ij} = k(x_i, x_j)$.
			$c \in \R^n$ are the kernel evaluations of the training data against the new point $x$: $c_i = k(x_i, x)$.
		Suppose we want to predict $y$ at a new point $x$ given the training data $(X,Y)$.
		 The predictive distribution is:
		 \eqn{p(y \mid x, X, Y) = \int_\sH p(y \mid x, f) \mu(df \mid X, Y),}
		 where we are marginalizing out the Gaussian process $f$ with respect to the posterior
		 $\mu(\cdot \mid X, Y)$.
		In general, this kind of computation would be intractable, but for regression, we can actually get a closed form solution
		 by exploiting the marginalization property of the GP.
		 First form the augmented kernel matrix (this marginalizes out $f$):
		 \eqn{\p{\begin{array}{c} Y \\ y \end{array}} \mid X, x \sim \sN\p{0, \p{\begin{array}{cc} K + \sigma^2 I & c \\ c^\top & k(x,x) + \sigma^2 \end{array}}}.}
		Now, it's just a matter of computing the conditional distribution of ordinary multivariate Gaussians.
		 \eqn{y \mid Y, X, x \sim \sN\p{\underbrace{c (K + \sigma^2 I)^{-1} Y}_\text{mean}, \underbrace{(k(x,x) + \sigma^2) - c^\top (K + \sigma^2 I)^{-1} c}_\text{variance}}.}
		Note that before, we had called $\alpha = (K + \sigma^2)^{-1} Y$, so the mean $c^\top \alpha$ of the posterior over $y$ is exactly the same as
		 in kernelized ridge regression!\footnote{This equivalence holds since the mean and the mode coincide in regression, but is not true in general.}
		 As a bonus, we get a variance estimate which captures how confident we are about our prediction.
			Intuition: $k(x,x) + \sigma^2 I$ would be the variance over $y$ in the absence of any other data.
			 The $c^\top (K + \sigma^2 I)^{-1} c$ term is the reduction in variance, which is based on the correlation with the training points.
			To see this intuition, consider the setting where there is no noise ($\sigma^2 = 0$),
			 and we observed one point ($f(x_1)$).
			 What is the conditional distribution of $f(x)$?
			The augmented kernel matrix is:
			 \eqn{\p{\begin{array}{cc} s_1^2 & \rho s_1 s \\ \rho s_1 s & s^2. \end{array}},}
			 where $\rho$ is the correlation between $f(x_1)$ and $f(x)$.
			Then the variance of $f(x)$ is $s^2 - \rho^2 s^2 = s^2 (1 - \rho^2)$.
			 This is the marginal variance of $f(x)$ ($s^2$) scaled by $1-\rho^2$,
			 which is small when there is more correlation with $f(x_1)$.
		Note on computational tractability: Gaussian processes can also be used for \textbf{classification} as well,
		 but we no longer get nice closed form solutions
		 like we do in regression.
		 As a result, we resort to sampling or variational approximations.
		Utility of uncertainty estimates
			By maintaining a posterior, GP-based models allow us to
			 talk about the uncertainty over function values
			 (indeed, $f(x)$ is a random variable).
			One compelling use case is in online learning or optimization of a function.
			 Here, the learner does not simply get $n$ labeled examples as in batch learning.
			 Rather, the learner chooses $x \in \sX$,
			 observes $f(x)$ (we assume no noise for simplicity), and repeats.
			This is similar to the multi-armed bandit setting,
			 with the exception that $\sX$ is possibly infinite,
			 and that the $f$ has additional structure
			 given by the kernel function, which helps us reason about points we haven't seen.
			We will consider two possible objectives:
				Active learning: we care about the value of the function $f$ everywhere.
				Bayesian optimization: we only care about finding the maximum of the function $f$.
			Both objectives can be captured by an \textbf{acquisition function}
			 $A(x; x_{1:i-1}, y_{1:i-1})$, which represents the expected utility
			 gained from querying $x$.
			The general algorithm iteratively query points at maxima of the
			 acquisition function:
			\algHeading{onlineGP}{General algorithm}
				For $i = 1, \dots, n$:
					Query $x_i = \arg\max_{x \in \sX} A(x; x_{1:i-1}, y_{1:i-1})$.
					Observe $y_i = f(x_i)$.
			Active learning
				Define the acquisition function at $x$ to be
				 the variance of the function at $x$ given the previous points:
				 \eqn{A(x; x_{1:i-1}, y_{1:i-1}) = \var[f(x) \mid x_{1:i-1}, y_{1:i-1}].}
				The idea is that we want to find the $x$ such that observing $f(x)$
				 results in the most reduction in uncertainty.
				 Intuitively, this corresponds to learning the most information.
				Note that after observing $f(x)$,
				 the variance of highly correlated points will also be reduced,
				 so the next iteration, we will try to seek a point which is quite
				 different (exploration).
			Bayesian optimization
				Define the acquisition function at $x$ to be 
				 the \textbf{expected improvement} in the maximum function value
				 from observing $x$.
				 \eqn{A(x; x_{1:i-1}, y_{1:i-1}) = \E[\max \{ 0, f(x) - \max_{j < i} y_j \} \mid x_{1:i-1}, y_{1:i-1}].}
				In optimization, we want to only learn about $f(x)$ if it has a good
				 chance of improving the current maximum $\max_{j < i} y_j$.
				If $f(x)$ has high variance, but low enough expectation,
				 then there's no reason to observe $f(x)$.
				If $f(x)$ and $f(x')$ have equal function expectation
				 but $f(x)$ has much higher variance, then $x$ preferable to $x'$.
				 Note that if $f(x)$ turns out to be low, the current maximum is still maintained.
				Another popular acquisition function for optimization is GP-UCB
				 (upper confidence bound), taken from the multi-armed bandit literature,
				 where we maximize the sum of the expected value plus
				 the standard deviation to encourage exploration.
			Maximizing the acquisition function $A$ over $x \in \sX$
				Computing $\arg\max_{x \in \sX} A(x; x_{1:i-1}, y_{1:i-1})$
				 is non-trivial since $A$ is not convex and $x$ could even be discrete.
				We therefore resort to sampling or other derivative-free methods for optimizing nasty functions.
				So \refalg{onlineGP} really only make sense when the actual function evaluations $f(x)$ are \emph{much more expensive}
				 than maximizing the acquisition function or solving the GP with the resulting observations.
	Fourier properties of shift-invariant kernels \currlecture
		Having explored how kernels can be used in practice,
		 let us turn back to studying their theoretical properties.
		 Specifically, we will use Fourier analysis to get a handle
		 on what information about the data a kernel is capturing.
		 We will also see that this leads to a new basis representation of kernels.
		 In this section, we will focus on shift-invariant kernels,
		 which are kernels that don't depend on the absolute position of the data points.
		\definitionHeading{shiftInvariant}{shift-invariant kernel}
			A kernel $k : \sX \times \sX \to \R$ where $\sX \subseteq \R^b$ is \word{shift invariant} (a.k.a. stationary, translation invariant) iff
			 $k$ can be written as $k(x,x') = h(x-x')$ for some function $h$.
			Example: Gaussian kernel
			Non-example: linear kernel ($(x + a) (x' + a) \neq x x'$)
		!comment A subclass of shift-invariant kernels are those that only look at the magnitude of the difference between $x$ and $x'$:
		!comment \definitionHeading{radial}{radial kernel}
			A kernel $k$ is a \word{radial kernel} iff $k(x,x')$ depends only on $\|x-x'\|_2$.
		Our goal is to understand shift-invariant kernels in the frequency domain.
		 In particular, it will shed insight into the smoothness properties of the kernel.
		First, let's warmup with some basic facts from Fourier analysis:
			FIGURE: [complex unit circle]
			$e^{i \omega t} = \cos(\omega t) + i \sin(\omega t)$
			$e^{-i \omega t} = \cos(\omega t) - i \sin(\omega t)$
			$\cos(\omega t) = \half e^{i \omega t} + \half e^{-i \omega t}$
			Here $\omega \in \R$ is the frequency of the sinusoid.
			Note that everything thus far generalizes to $\omega \in \R^b$ and $t \in \R^b$;
			 just replace $\omega t$ with $\inner{\omega}{t}$.
		Now let's try to construct a kernel using the Fourier basis:
			First define a feature map for a fixed $\omega$:
			 $\phi(x) = e^{-i \inner{\omega}{x}}$ (note that this is complex-valued, but that's okay).
			Using this feature map, let's define a kernel:
			 \eqn{k(x,x') = \phi(x) \overline{\phi(x')} = e^{-i \inner{\omega}{x - x'}},}
			 where $\bar a$ denotes the complex conjugate of $a$.
			 This kernel deems two points to be similar if they are close modulo $2\pi/\omega$
			 (for some fixed scalar frequency $\omega$);
			 clearly this is a bit silly.
			To get more realistic kernels, we need to incorporate multiple frequencies.
			 We can do this by simply averaging over multiple kernels (recall that the sum of kernels is a kernel).
			 Specifically, let $\mu(\cdot)$ be a finite non-negative measure over frequencies.
			 Then
			 \eqnl{kernelFrequency}{k(x,x') = \int e^{-i \inner{\omega}{x - x'}} \mu(d\omega).}
			 is also a valid kernel.
			 Or in terms of $h$ (recall that $t = x - x'$):
			 \eqnl{kernelFrequency2}{h(t) = \int e^{-i \inner{\omega}{t}} \mu(d\omega).}
			The corresponding feature map consists of the following basis functions:
			 $\{ x \mapsto e^{-i \inner{\omega}{x}} : \omega \in \R^b \}$
			Intuitively $\mu$ tells us how much focus to put on various frequencies.
		\exampleHeading{constantKernel}{constant}
			Let the spectral measure $\mu = \delta_0$ place all its mass at $0$.
			Then $k(x, x') = h(t) = 1$ is the constant kernel.
		\exampleHeading{singleFreq}{single frequency}
			Suppose $\mu$ places mass only at $-\omega$ and $\omega$:
			 \eqn{\mu = \half (\delta_{-\omega} + \delta_{\omega}).}
			Then the resulting kernel is:
			 \eqn{k(x, x') = h(t) = \cos(\omega t).}
			In general, $h(t)$ defined via $\mu$ might be complex-valued,
			 but if $\mu$ is \textbf{symmetric} (that is,
			 $\mu(A) = \mu(-A)$ for all measurable sets $A$),
			 then $h(t)$ will be \textbf{real-valued}.
		\exampleHeading{sinc}{sinc}
			Let the spectral density $s$ be the 1 over $[-a,a]$
			 (that is, we only keep frequencies below $a$):
			 \eqn{s(\omega) = \1[-a \le \omega \le a].}
			Then the resulting kernel is:
			 \eqn{h(t) = \frac{2 \sin(a t)}{t}.}
			Proof: just integrate:
			 \eqn{h(t) = \int_{-a}^a e^{-i \omega t} d \omega = \inv{-it} (e^{-i a t} - e^{i a t}) = \inv{-it} (-2 i \sin(a t)).}
			Note: as $a$ increases, we cover more frequencies.
			 If $a \to \infty$, then $h(t)$ converges to a delta function at $0$,
			 which corresponds to the degenerate kernel $k(x,x) = \1[x = x']$.
		At first, it might seem that this is a funny way of defining certain types
		 of kernels, but what's remarkable is that \emph{all} shift-invariant kernels
		 can be written in the form \refeqn{kernelFrequency} for some appropriate
		 choice of $\mu$.  This statement is precisely Bochner's theorem:
		\theoremHeading{bochner}{Bochner's theorem}
			Let $k(x,x') = h(x-x')$ be a continuous shift-invariant kernel ($x \in \R^b$).
			Then there exists a unique finite non-negative measure $\mu$ (called the \word{spectral measure}) on $\R^b$ such that
			 \eqn{\boxed{h(t) = \int e^{-i \inner{t}{\omega}} \mu(d\omega).}}
			Furthermore, if $\mu$ has a density $s$,
			 \eqn{\mu(d\omega) = s(\omega) d\omega,}
			 then we call $s$ the \word{spectral density}, and $h$ is the Fourier transform of $s$.
		So far, we've defined kernels through various spectral measures $\mu$.
		 But we can also take a given kernel and compute its spectral measure to study
		 the properties of the kernel.
		 Given a candidate kernel function $k(x,x') = h(x-x')$,
		 take the Fourier transform of $h$
		 (which for symmetric functions is the inverse Fourier transform times $1/(2\pi)$)
		 to get the spectral density $s$.
		\exampleHeading{boxKernel}{box is not a kernel}
			Consider the following function:
			 \eqn{h(t) = \1[-1 \le t \le 1].}
			The inverse Fourier transform times $1/(2\pi)$ is
			 \eqn{s(\omega) = \frac{\sin(\omega)}{\pi \omega},}
			 reusing the result from above.
			But notice that $s(\omega)$ is negative in some places,
			 which means, by Bochner's theorem, that $h(t)$ is not a valid (positive semidefinite) kernel!
			 Now we have another way to check whether a shift-invariant function
			 specifies a kernel---simply take the inverse Fourier transform and see whether it's non-negative everywhere.
		!comment In between the sinc kernel and the Gaussian kernel is the exponential kernel:
			The kernel:
			 \eqn{h(t) = \exp(-\|t\|).}
			The spectral density:
			 \eqn{s(\omega) = \frac{1}{2\pi (1 + \omega^2)}.}
		\exampleHeading{gaussianKernel}{Gaussian kernel}
			Let the spectral density $s$ be the density of the multivariate Gaussian distribution
			 with variance $1/\sigma^2$:
			 \eqn{s(\omega) = \p{\frac{2\pi}{\sigma^2}}^{-d/2} \exp\p{\frac{-\sigma^2 \|\omega\|_2^2}{2}}.}
			Then the resulting kernel is the Gaussian kernel with variance $\sigma^2$ (note the inverting of the variance):
			 \eqn{h(t) = \exp\p{\frac{-\|t\|_2^2}{2\sigma^2}}.}
			Proof:
			 \eqn{h(t) = \int \p{\frac{2\pi}{\sigma^2}}^{-d/2} \exp\p{\frac{(-\sigma^2 \|\omega\|_2^2 - 2 i \inner{\omega}{t} - \sigma^{-2} i^2 \|t\|_2^2) + \sigma^{-2} i^2 \|t\|_2^2}{2}} d\omega.}
			 Complete the square and note that the Gaussian distribution (with mean $i t/\sigma$) integrates to 1.
			Intuition: the larger $\sigma^2$ is,
			 one can see from $s(\omega)$ that high frequency components are dampened.
			 Consequently, the smoother the kernel $h(t)$ is.
		\exampleHeading{rqk}{rational quadratic kernel}
			Motivation: with Gaussian kernels,
			 how do we set the variance $\sigma^2$?
			Putting on our Bayesian hats, let's define a prior over $\tau = \sigma^{-2}$.
			Let $k_\tau(x,x')$ be the Gaussian kernel with hyperparameter $\tau$.
			Recalling that the sum of kernels is a kernel, we have that
			 \eqn{\int k_\tau(x,x') p(\tau) d\tau}
			 is also a kernel for any $p(\tau)$.
			For mathematical convenience,
			 let's put a $\text{Gamma}(\alpha, \beta)$ prior on $\tau$.
			By conjugacy, we can integrate a Gamma distribution against a Gaussian,
			 which yields a student-t distribution.
			Ignoring normalization constants, the kernel is the \textbf{rational quadratic kernel} (derivation omitted):
			 \eqn{h(t) = \p{1 + \frac{\beta t^2}{2 \alpha}}^{-\alpha}.}
			When $\alpha = 1$ and $\beta = 2$, we have:
			 \eqn{h(t) = \frac{1}{1 + t^2}.}
			Compared with the Gaussian kernel:
				The area near zero is steeper, so the function can change rapidly.
				The tails decay slower, function values can have longer range dependencies.
			 This flexbility comes from integrating over values of $\sigma^2$.
			Note that as $\alpha,\beta \to \infty$ with $\alpha = \beta$,
			 the rational quadratic approaches the Gaussian kernel with $\sigma^2 = 1$.
		!comment \theoremHeading{mercer}{Mercer's theorem} (from functional analysis; Mercer, 1909)
			Assumption: compactness
			Suppose $k$ is square integrable (i.e., $k \in L^2(\sX \times \sX)$).
			 Must have finite trace (limiting).
			Suppose $k$ is positive semidefinite: $\int \int f(x) k(x,x') f(x') dx dx' \ge 0$ for all square integrable $f \in L^2(\sX)$.
			Then there exists functions $u_i : \sX \to \R$ with associated coefficients $\lambda_i \ge 0$ for $i = 1, 2, \dots$ such that
			 \[ k(x, x') = \sum_i \lambda_i u_i(x) u_i(x') \quad \text{ for all } x \in \sX. \]
			 If we define the feature vector $\phi_i(x) = \sqrt{\lambda_i} \phi_i(x)$,
			Remarks
				Because of boundedness, we only need a countable number of features $\phi$.
				Note that this doesn't give us an explicit construction of $\phi$.
	!verbatim \lecture{12}
	Efficient computation \currlecture
		We saw how kernel methods could be used for learning:
		 Given $n$ points $x_1, \dots, x_n$, we form the $n \times n$ kernel matrix $K$,
		 and optimize an objective function whose variables are $\alpha \in \R^n$.
		 For example, in kernel ridge regression, we have $\alpha = (K + \lambda I)^{-1} Y$,
		 which requires $O(n^3)$ time.
		 When we have large datasets (e.g., $n = 10^6$), this is prohibitively expensive.
		 On the other hand, when the feature vector $\phi(x)$ is high-dimensional
		 (especially infinite-dimensional),
		 then scaling with $n$ is the lesser of two evils.
		 But can we do better?
		 Note that even merely computing the full kernel matrix $K$ takes $O(n^2)$ time,
		 which is already too large, so we will probably have to cut some corners.
		We will introduce two types of kernel approximations:
			Random features: We will write the kernel function as an integral,
			 and using Monte Carlo approximations of this integral.
			 These approximations are of the kernel function and are data-independent.
			Nyström method:
			 We will sample a subset of the $n$ points and use these points
			 to approximate the kernel matrix.
			 These approximations are of the kernel matrix and are data-dependent.
		Gaussian kernel intuitions
			Let's get some intuition about when approximations might be a sensible thing to do
			 on a concrete scenario.
			 Recall the Gaussian kernel:
			 \eqn{k(x,x') = \exp\p{\frac{-\|x-x'\|_2^2}{2 \sigma^2}}.}
			If the points $x_1, \dots, x_n$ are far apart (relative to the bandwidth of $\sigma^2$),
			 then the kernel matrix $K$ will be roughly the identity matrix.
			 In this case, we can't possibly hope for a low-rank approximation to capture everything.
			On the other hand, if the points are tightly clustered into $m$ clusters,
			 then the kernel matrix (with sorted columns/rows)
			 looks like a block diagonal matrix with $m$ blocks,
			 where each block is a rank $1$ all-ones matrix.
			 Here, you would expect a rank $m$ approximation to be effective.
			In reality, the situation is somewhere in between.
			 Of course, kernel methods are exactly useful when the data are fairly complex,
			 so we shouldn't expect these approximations to provide magical savings,
			 unless the data is very redundant.
		Random Fourier features (Rahimi/Recht, 2008)
			Our starting point is Bochner's theorem (\refthm{bochner}),
			 which allows us to write shift-invariant kernels in terms of an integral
			 over some spectral measure $\mu$:
			 \eqn{k(x,x') = \int \phi_\omega(x) \overline{\phi_\omega(x')} \mu(d \omega),}
			 where $\phi_\omega(x) = e^{-i \inner{\omega}{x}}$
			 is a single (Fourier) feature.
			The key idea is to replace the integral with a finite sum over $m$ elements.
			 For simplicity, assume that $\mu$ is a probability distribution.
			 If it is not, then we can normalize it and then multiply the result by $\mu(\mathbb C^b)$.
			 Let $\omega_1, \dots, \omega_m$ be drawn i.i.d.~from $\mu$.
			 Then, define the approximate kernel as:
			 \eqn{\hat k(x, x') = \inv{m} \sum_{i=1}^m \phi_{\omega_i}(x) \overline{\phi_{\omega_i}(x)}.}
			 This kernel corresponds to having the following random feature map:
			 \eqn{\hat\phi(x) = \inv{\sqrt{m}} [\phi_{\omega_1}(x), \dots, \phi_{\omega_m}(x)] \in \mathbb C^m.}
			As a concrete example, consider the Gaussian kernel, which has a Gaussian spectral density
			 (recall $\mu(d \omega) = s(\omega) d\omega$) with the inverse variance:
			 \eqn{k(x,x') &= \exp\p{\frac{-\|x-x'\|_2^2}{2 \sigma^2}}, \\
			 s(\omega) &= \p{\frac{2\pi}{\sigma^2}}^{-b/2} \exp\p{\frac{-\sigma^2 \|\omega\|_2^2}{2}}.}
			 This means that each $\omega_i \sim \sN(0, \sigma^{-2} I)$ is drawn from a
			 Gaussian.
			Algorithm
				The practical upshot of random Fourier features on the Gaussian kernel
				 is that it is dirt simple.
				Before you get data, draw $\omega_1, \dots, \omega_m \sim \sN(0, \sigma^{-2} I)$,
				 which defines the random feature map $\hat\phi$.
				 This feature map is fixed once and for all.
				In training/test, given a new data point $x$, we can apply the feature map $\hat\phi(x)$,
				 which simply involves $m$ Gaussian projections.
			Note that the approximate kernel is unbiased ($\E[\hat k(x,x')] = k(x,x')$),
			 so as $m \to \infty$, we have that $\hat k(x,x')$ converges to $k(x,x')$
			 for a fixed $x, x'$.  We want this to work well on average for all the data that we're going to see,
			 which smells almost like uniform convergence.
			 The following theorem quantifies this:
			\theoremHeading{randomFeatures}{random features (Rahimi/Recht, 2008)}
				Let $k$ be a shift-invariant kernel on $x \in \R^b$.
				Let 
				 \eqn{\sF \eqdef \pc{ x \mapsto \int \alpha(\omega) \phi_\omega(x) \mu(d \omega) : \forall \omega, |\alpha(\omega)| \le C }}
				 be the subset of functions in the RKHS $\sH$ with bounded Fourier coefficients $\alpha(\omega)$.
				Let
				 \eqn{\hat\sF \eqdef \pc{ x \mapsto \inv{m} \sum_{i=1}^m \alpha(\omega_i) \phi_{\omega_i}(x) : \forall \omega, |\alpha(\omega)| \le C }}
				 be the subset that is spanned by the random feature functions,
				 where $\omega_{1:k}$ be drawn i.i.d.~from $\mu$.
				Let $p^*$ be any distribution over $\sX = \R^b$.
				Define the inner product with respect to the data-generating distribution
				 (this is not the RKHS inner product):
				 \eqn{\inner{f}{g} \eqdef \E_{x \sim p^*}[f(x) g(x)].}
				Let $f^* \in \sF$ be any true function.
				Then with probability at least $1-\delta$, there exists $\hat f \in \hat \sF$ that
				 \eqn{\|\hat f - f^*\| \le \frac{C}{\sqrt{m}} \p{1 + \sqrt{2 \log (1/\delta)}}.}
			Proof of \refthm{randomFeatures}:
				This proof uses fairly standard tools: McDiarmid's inequality and Jensen's inequality.
				 The function we're applying involves taking a norm of a function,
				 but we just need the bounded differences condition to hold.
				Fix $f^* \in \sF$ with coefficients $\alpha(\omega)$.
				Construct $\hat f$ with the same coefficients,
				 and note that $\hat f \in \hat \sF$ and $\E[\hat f] = f^*$.
				Define 
				 \eqn{D(\omega_{1:m}) = \|\hat f - f^*\|.}
				 Note that $D$ satisfies the bounded differences inequality:
				 letting $\omega_{1:m}^i = \omega_{1:m}$ except on the $i$-th component,
				 where it is $\omega_i'$:
				 \eqn{D(\omega_{1:m}) - D(\omega_{1:m}^i)
				 &\le \|\hat f - f^*\| - \|\hat f^i - f^*\| \\
				 &\le \|\hat f - \hat f^i\| \aside{triangle inequality} \\
				 &\le \inv{m} \|\alpha(\omega_i) \phi_{\omega_i} - \alpha(\omega_i') \phi_{\omega_i'}\| \\
				 &\le \frac{2 C}{m}.
				 }
				 Note that the last line follows
				 because $|\alpha(\omega_i)| \le C$ and
				 $\phi_{\omega_i}(x) = e^{-i\inner{\omega_i}{x}}$ and $|e^{-i a}| = 1$ for all $a$.
				We can bound the mean by passing to the variance:
				 \eqn{\E[D(\omega_{1:m})]
				 &\le \sqrt{\E[D(\omega_{1:m})^2]} \aside{Jensen's inequality} \\
				 &=   \sqrt{\E\pb{\left\|\inv{m} \sum_{i=1}^m (\alpha(\omega_i) \phi_{\omega_i} - f^*)\right\|^2}} \aside{expand} \\
				 &=   \sqrt{\inv{m^2} \sum_{i=1}^m \E\pb{\left\|\alpha(\omega_i) \phi_{\omega_i} - f^*\right\|^2}} \aside{variance of i.i.d.~sum} \\
				 &\le \frac{C}{\sqrt{m}} \aside{use $|\alpha(\omega_i)| \le C$}.
				 }
				Applying McDiarmid's inequality (\refthm{mcdiarmid}),
				 we get that
				 \eqn{\BP\pb{D(\omega_{1:m}) \ge \frac{C}{\sqrt{m}} + \epsilon} \le \exp\p{\frac{-2 \epsilon^2}{\sum_{i=1}^m (2C/m)^2}}.}
				 Rearranging yields the theorem.
			Remark: the definition of $\alpha$ here differs from the Rahimi/Recht paper.
			Corollary:
				Suppose we had a loss function $\ell(y, v)$ which is $1$-Lipschitz in the second argument.
				 (e.g., the hinge loss).
				 Define the expected risk in the usual way:
				 \eqn{L(f) \eqdef \E_{(x,y) \sim p^*}[\ell(y, f(x))].}
				 Then the approximation ratio is bounded:
				 \eqn{L(\hat f) - L(f^*)
				 &\le \E[|\ell(y, \hat f(x)) - \ell(y, f^*(x))|] \aside{definition, add $|\cdot|$} \\
				 &\le \E[|\hat f(x) - f^*(x)|] \aside{fix $y$, $\ell$ is Lipschitz} \\
				 &\le \|\hat f - f^*\| \aside{concavity of $\sqrt{\cdot}$}.}
			So far, we have analyzed approximation error due to having a finite $m$,
			 but assuming an infinite amount of data.
			 Separately, there is the estimation error due to having $n$ data points:
			 \eqn{L(\hat f_\text{ERM}) - L(\hat f) \le O_p\p{\frac{C}{\sqrt{n}}},}
			 where $\hat f_\text{ERM}$ minimzes the empirical risk over the random hypothesis class $\hat \sF$.
			 So, the total error, which includes approximation error and estimation error is
			 \eqn{L(\hat f_\text{ERM}) - L(f^*) = O_p\p{\frac{C}{\sqrt{n}} + \frac{C}{\sqrt{m}}}.}
			 This bound suggests that the approximation and estimation errors are balanced when $m$
			 and $n$ are on the same order.
			 One takeaway is that we shouldn't over-optimize one without the other.
			 But one might also strongly object and say that if $m \approxeq n$,
			 then we aren't really getting any savings!
			 This is indeed a valid complaint,
			 and in order to get stronger results,
			 we would need to impose more structure on the problem.
		!comment Fastfood approximations (Le/Sarlós/Smola, 2013)
			Using the random Fourier features,
			 we have cut the running time from $O(n^3)$
			 to $O(n m b)$, where $m$ allows one to balance the approximation error with runtime.
			 Now we will show how to use further tricks to obtain $O(n m \log b)$,
			 which can be a substantial savings since $b$ could be in the thousands.
			Recall that in this case, we computed a feature vector for an input $x \in \R^b$,
			 $\phi(x) = [e^{-i\inner{\omega_1}{x}}, \dots, e^{-i \inner{\omega_m}{x}}]$,
			 which involves $m$ dot products of $b$-dimensional vectors.
			For simplicity, we will focus on Gaussian kernels,
			 where $\omega_1, \dots, \omega_m \sim \sN(0, \sigma^{-2})$.
			 The desired computation is just left-multiplying $x$ by a $m \times b$ matrix
			 $\Omega$ with i.i.d. Gaussian entries.
			 Can we avoid doing $m$ dot products?
			For simplicity, assume $m = b$.
			 The key idea is to define the following matrix $V$ instead of $\Omega$:
			 \eqn{V \eqdef \inv{\sigma \sqrt{b}} S H G \Pi H B,}
			 where
				$B$ is a diagonal matrix whose diagonal entries are uniform over $\{-1,+1\}$.
				$H$ is a Walsh-Hadamard matrix $H_d$ where
				 \eqn{H_2 = \pb{\begin{array}{rr} 1 & 1 \\ 1 & -1 \end{array}} \quad
				 H_{2k} = \pb{\begin{array}{rr} H_k & H_k \\ H_k & -H_k \end{array}},}
				 which is a random transformation.
				 The key is that $H_d x$ can be computed in $O(b \log b)$ time.
				$\Pi$ is a random permutation matrix, which ensures the two copies of $H$ have incoherent columns.
				$G$ is a diagonal matrix whose diagonal entries are drawn $G_{ii} \sim \sN(0, 1)$.
				$S$ is a diagonal scaling matrix that ensures the lengths of the rows of $H G \Pi H B$
				 are independent.
				 Specifically, set
				 \eqn{S_{ii} = s_i \|G\|_F^{-\half}, \quad s_i \sim (2\pi)^{-b/2} A_{b-1}^{-1} r^{b-1} e^{-\frac{r^2}{2}}.}
			\theoremHeading{fastfood}{Fastfood is unbiased}
				Define the Fastfood feature vector
				 \eqn{\phi(x) = [e^{-i (Vx)_1}, \dots, e^{-i (Vx)_m}].}
				Then
				 \eqn{\E\pb{\inner{\phi(x)}{\overline{\phi(x')}}} = k(x, x') = \exp\p{\frac{-\|x-x\|_2^2}{2 \sigma^2}}.}
		Dot product kernels (Kar/Karnick, 2012)
			We have seen that shift-invariant kernels admit an integral representation,
			 which allows us to use Monte Carlo to approximate it.
			 What about non-shift invariant kernels such as polynomial kernels, such as the following?
			 \eqn{k(x,x') = \inner{x}{x'}^p.}
			Although random Fourier features will not work,
			 we can still try to write the kernel as an expectation.
			 The key is that if we draw a Rademacher variable $\omega \in \{-1,+1\}^b$ (uniform),
			 randomly projecting $x$ onto $\omega$ yields an unbiased estimate of the inner product:
			 \eqn{\inner{x}{x'} = \E[\inner{\omega}{x} \inner{\omega}{x'}].}
			 Of course, this isn't useful by itself, but it does reduce $x$ to a scalar $\inner{\omega}{x}$,
			 which is useful.
			To generalize to polynomial kernels,
			 we simply do the above construction $p$ times and multiply it all together.
			 For the quadratic kernel, let $\omega_1$ and $\omega_2$ be two independent Rademacher vectors.
			 Then:
			 \eqn{\inner{x}{x'}^2
			 &= \E[\inner{\omega_1}{x} \inner{\omega_1}{x'}] \E[\inner{\omega_2}{x} \inner{\omega_2}{x'}] \\
			 &= \E[\inner{\omega_1}{x} \inner{\omega_2}{x} \inner{\omega_1}{x'} \inner{\omega_2}{x'}],}
			 where the first line follows from the earlier calculation,
			 and the second line follows from independence of $\omega_1$ and $\omega_2$.
			 Note that $\inner{\omega_1}{x} \inner{\omega_2}{x}$ is still just a number.
			More generally, if the kernel is a analytic function of the dot product,
			 then it admits the following Taylor expansion around $0$:
			 \eqn{k(x,x') = f(\inner{x}{x'}), \quad f(z) = \sum_{j=0}^\infty a_j z^j.}
			To construct a random feature,
				Choose $J$ with probability proportional to $a_j$ (if we can't sample from $a_j$ exactly, then we can use importance weighting).
				Choose $\omega_1, \dots, \omega_J$ Rademacher vectors,
				 and let
				 \eqn{\phi_{\omega_{1:J}}(x) = \prod_{j=1}^J \inner{\omega_{j}}{x}.}
			If we do this $m$ times to form a $m$-dimensional feature vector,
			 then we have a Monte Carlo estimate of the kernel $k$.
			 Note that in the process, we have to draw an expected $m b \E[J]$
			 Rademacher variables.
			At this point, we have only showed that we have an unbiased estimate of the kernel.
			 We still need to show that the variance isn't too large.
			 See the paper in the references below for that.
		Arc-cosine kernel (Cho/Saul, 2009)
			The random features perspective is very suggestive of the computation in neural networks.
			 A one layer neural network computes a function
			 \eqn{f(x) = \sum_{j=1}^m \alpha_j \sigma(\omega_j \cdot x),}
			 where the parameters $\alpha_{1:m}$ and $\omega_{1:m}$ are optimized via
			 (stochastic) gradient descent (backpropagation),
			 and $\sigma$ is a non-linear function such as a hard-threshold ($\sigma(z) = \1[z \ge 0]$)
			 or rectified linear unit ($\sigma(z) = \1[z \ge 0] z$).
			For comparison, the random features view of kernels defines a function class
			 where $\omega_{1:m}$ is chosen randomly rather than optimized,
			 while $\alpha_{1:m}$ are optimized.
			 Typically, neural network weights are initialized randomly, sometimes according to a Gaussian
			 with the appropriate variance.
			 The fact that random features approximates a kernel suggests
			 that even the random initialization is quite sensible starting point
			 (provided the final layer $\alpha_{1:m}$ are optimized).
			What if we take $\sigma$ to be a rectified linear unit and let the number of hidden units $m \to \infty$?
			 Does this limiting quantity have a nice form?
			 To answer this question, let us define a more general family.
			Define the random basis function:
			 \eqn{\phi_\omega(x) = \1[\omega \cdot x \ge 0] (\omega \cdot x)^q.}
				For $q = 0$, we obtain the threshold function.
				For $q = 1$, we obtain the rectified linear unit (ReLU).
			As $m \to \infty$, we obtain the following kernel:
			 \eqn{k(x, x') = 2 \int \phi_\omega(x) \phi_\omega(x') p(\omega) \omega.}
			This kernel can be shown to have the following closed form solution:
			 \eqn{k(x, x') = \inv{\pi} \|x\|^q \|x'\|^q J_q(\theta),}
			 where
			 \eqn{\theta = \arccos\p{\frac{x \cdot x'}{\|x\| \|x'\|}}}
			 is the angle between $x$ and $x'$
			 and $J_q(\theta)$ captures the angular dependence.
			 In other words, this \emph{arc-cosine kernel} decouples the magnitude from the angle.
			In general $J_q$ is a complex function, but we can consider two simple cases:
			 \eqn{
			 J_0(\theta) &= \pi - \theta \\
			 J_1(\theta) &= \sin(\theta) + (\pi - \theta) \cos(\theta).
			 }
			The punchline is that even if we are using neural networks,
			 we can use kernel methods to better understand them from a representational point of view.
		Nyström method (Williams/Seeger, 2000)
			In the above, we have constructed random features,
			 which were independent of the data.
			 A technique that predates these,
			 which can work better when the spectrum of the kernel matrix
			 is to form a low-rank approximation.
			 This method applies more generically to approximating large PSD matrices.
			Given a kernel matrix $K \in \R^{n \times n}$,
			 we will sample a subset of the indices $I \subseteq \{1, \dots, n\}$ with $|I| = m$,
			 and let $J = \{1, \dots, n\} \backslash I$ be the other indices.
			 We then evaluate the kernel on points in $I$ paired with all other points,
			 for a total of $O(|I| n)$ evaluations.
			 Then we can define the approximate kernel matrix:
			 \eqnl{nystrom}{
			 K =
			 \p{
			 \begin{array}{cc}
			 K_{II} & K_{IJ} \\
			 K_{JI} & K_{JJ}
			 \end{array}
			 }
			 \approxeq
			 \p{
			 \begin{array}{cc}
			 K_{II} & K_{IJ} \\
			 K_{JI} & \red{K_{JI} K_{II}^{\dagger} K_{IJ}},
			 \end{array}
			 }
			 = \tilde K
			 }
			 or more compactly:
			 \eqn{\tilde K \eqdef K_{\cdot I} K_{II}^{\dagger} K_{I \cdot}.}
			 Note that the difference $K_{JJ} - \red{K_{JI} K_{II}^{\dagger} K_{IJ}}$ is the Schur complement of $K_{JJ}$.
			 If we interpret $K$ as a covariance matrix of a Gaussian $Z$,
			 then this is the conditional variance $\var[Z_J \mid Z_I]$.
			Note that if $K$ is rank $m$ and and $K_{II}$ also contains linearly independent columns
			 (so that it captures the subspace of $K$),
			 then the Schur complement is zero, and the Nyström method is exact.
			 If not, then the error stems from simply not being able to capture the low rank solution
			 by having a rank $m$ matrix plus an error from doing column sampling
			 (which doesn't yield the eigenvectors).
			 We can think of this as projecting the kernel matrix $K$ on to the subspace of the data points in $I$.
			How do we choose $I$?  Two popular choices are uniform sampling and sampling proportional to $K_{ii} = k(x_i, x_i)$,
			 which corresponds to the squared magnitude of $x_i$.
			 Intuitively, the weighted sampling focuses more energy on points which are more important.
			The following theorem formalizes the error bound:
			\theoremHeading{nystrom}{Nyström with non-uniform sampling (Drineas/Mahoney, 2005)}
				Suppose we choose $I$ by sampling (with replacement),
				 choosing $i \in \{1, \dots, n\}$ with probability $K_{ii} / \sum_{j=1}^n K_{jj}$.
				Let $\tilde K_m$ be the best rank $m$ approximation of $K$.
				Let $\tilde K$ be defined as in \refeqn{nystrom}, but where we replace
				 $K_{II}$ with the best rank $m$ approximation of $K_{II}$.
				Then with probability at least $1-\delta$,
				 \eqn{\|K - \tilde K\|_F^2 \le \|K - \tilde K_m\|_F^2 + 4 (1 + \sqrt{8 \log (1/\delta)}) \tr(K)^2 \sqrt{\frac{m}{|I|}}.}
			Proof: follows from algebraic manipulation and concentration.
			 Note that the theorem statement is a correct version of Drineas and Mahoney's Theorem 3,
			 where we just combined equation 31 with Lemma 9.
			The theorem suggests that we should take $|I| > m$,
			 which gives us more opportunities to cover the column space of $\tilde K_m$.
	Universality (skipped in class)
		We have explored several different kernels,
		 and we can (and should) certainly choose one based on domain knowledge.
		But one can ask: is there a general purpose kernel $k$,
		 in the sense that $k$ can be used to solve \emph{any} learning problem
		 given sufficient data?
		 The notion of general purpose is defined as follows.
		\definitionHeading{universality}{universal kernel}
			Let $\sX$ be a locally compact Hausdorff space
			 (e.g., $\R^b$ or any discrete set, but not infinite-dimensional spaces in general).
			Let $k : \sX \times \sX \to \R$ be a kernel.
			We say that $k$ is a \word{universal kernel}
			 (specifically, a $c_0$-universal kernel) iff
			 the RKHS $\sH$ with reproducing kernel $k$ is dense in $C_0(\sX)$,
			 the set of all continuous bounded functions on $\sX$
			 (with respect to the uniform norm).
			 In other words, for any function $f \in C_0(\sX)$ and $\epsilon > 0$,
			 there exists some $g \in \sH$ such that $\sup_{x \in \sX} \|f-g\| \le \epsilon$.
		The premise is that the target function we want to learn is in $C_0(\sX)$,
		 so by using a universal kernel, we are defining an RKHS which
		 can approximate any function in $C_0(\sX)$ as well as we want.
		The following theorem characterizes universal kernels in terms of their
		 Fourier properties:
		\theoremHeading{universalKernel}{Carmeli, 2010}
			Let $k$ be a shift-invariant kernel with spectral measure $\mu$ on $\sX = \R^d$.
			If the support of $\mu$ is all of $\R^b$, then $k$ is a universal kernel.
		 Intuition: in order to represent any $C_0(\sX)$ function,
		 we must not have any gaps in our spectrum.
		Example: the Gaussian kernel is universal; the sinc kernel is not universal.
		The final piece of the puzzle is \word{universal consistency},
		 which means that that a learning algorithm will actually achieve
		 the best possible error as the number of training examples tends to infinity.
		 Steinwart showed that using SVMs with a universal kernel
		 guarantees universal consistency.
		 Of course, universality is only about how well we can represent the target function;
		 it says nothing about readily we can actually estimate that function based on finite data.
	RKHS embedding of probability distributions (skipped in class)
		So far, we've showed that kernels can be used for
		 estimating functions for regression, classification,
		 dimensionality reduction (PCA), etc.
		 Now we will show how kernels can be used to represent and answer questions about
		 probability distributions without having to explicitly estimate them.
		As a motivating example, consider the problem of testing whether
		 two probability distributions $P$ and $Q$ are the same
		 by only observing expectations under the distributions.
		Given a distribution $P$, we can look at various \word{moments} of the distribution $\E_{x \sim P}[f(x)]$
		 for various functions $f$.
		 For example, if $f(x) = x$, then we get the mean.
		 Such a $f$ only gives us partial information about $P$:
		 if two distributions differ in their mean, then we know they are different,
		 but if they have the same mean, we cannot conclude that they are same.
		More generally, assume $P$ and $Q$ are defined on some locally compact Hausdorff space $\sX$ (e.g., $\R^b$).
		 Define the \word{maximum mean discrepancy} (MMD) as follows:
		 \eqn{\boxed{D(P, Q, \sF) \eqdef \sup_{f \in \sF} \left( \E_{x \sim P}[f(x)] - \E_{x \sim Q}[f(x)]\right),}}
		 for some set of functions $\sF$.
		 Shorthand: $\E_P[f]$ means $\E_{x \sim P}[f(x)]$.
		Can we find $\sF$ so that
		 \eqn{D(P, Q, \sF) = 0 \Leftrightarrow P = Q?}
		 Note that $P=Q$ always implies $D(P, Q, \sF)$, but the other direction requires some work.
		If we knew $P$ and $Q$ were Gaussian,
		 then it suffices to take $\sF = \{ x \mapsto x, x \mapsto x^2 \}$,
		 since the first two moments define a Gaussian distribution.
		 However, what about general $P$ and $Q$?
		 We need a much larger class of functions $\sF$:
		\theoremHeading{c0}{Dudley, 1984}
			If $\sF = C_0(\sX)$ (all continuous bounded functions),
			 then $D(P, Q, \sF) = 0$ implies $P = Q$.
		However, $C_0(\sX)$ is a large and difficult set to work with.
		 Fortunately, it suffices to take $\sF$ to be any set that is dense in $C_0(\sX)$,
		 in particular an RKHS:
		\theoremHeading{c1}{Steinwart, 2001}
			Let $\sF = \{ f \in \sH : \|f\|_\sH \le 1 \}$, where $\sH$ is the RKHS
			 defined by a universal kernel $k$.
			Then $D(P, Q, \sF) = 0$ implies $P = Q$.
		Proof:
			Let $P \neq Q$ be two distinct distributions.
			Then there exists $f \in C_0(\sX)$ such that $|\E_P[f] - \E_Q[f]| = \epsilon > 0$.
			Since $\sH$ is universal (i.e., $\sH$ is dense in $\sC_0(\sX)$ with respect to the uniform norm),
			 there exists $g \in \sH$ with $g \neq 0$ such that
			 \eqn{\|f - g\|_\infty \eqdef \sup_{x \in \sX} |f(x) - g(x)| \le \epsilon/3.}
			This means
			 $|\E_P[f] - \E_P[g]| \le \epsilon/3$ and
			 $|\E_Q[f] - \E_Q[g]| \le \epsilon/3$.
			By the triangle inequality, $|\E_P[g] - \E_Q[g]| \ge \epsilon/3 > 0$.
			Rescale $g$: let $u = g/\|g\|_\sH \in \sF$.
			We still have $D(P, Q, \sF) \ge |\E_P[u] - \E_Q[u]| > 0$.
		Computing $D(P, Q, \sF)$
			We've established that $D(P, Q, \sF)$ contains sufficient information for testing whether two distributions are equal.
			 But how do we actually compute the max over $\sF$?  This seems daunting at first sight.
			 Fortunately, we can compute $D(P, Q, \sF)$ in closed form by exploiting properties of the RKHS.
			First, a general statement.  By the reproducing property and linearity of the inner product,
			 we can express expected function value as an inner product:
			 \eqn{\E_{x \sim P}[f(x)] = \E_{x \sim P}[\inner{k(\cdot, x)}{f}] = \inner{\underbrace{\E_{x \sim P}[k(x, \cdot)]}_{\eqdef \blue{\mu_P}}}{f}.}
			 Here, $\mu_P \in \sH$ is the \word{RKHS embedding} of the probability distribution $P$.
			We can now write the MMD solution as follows:
			 \eqn{D(P, Q, \sF) = \sup_{f \in \sF} \inner{\mu_P - \mu_Q}{f} = \|\mu_P - \mu_Q\|_\sH,}
			 where the sup is obtained by setting $f$ to be a unit vector in the direction of $\mu_P - \mu_Q$.
			Unpacking the square of the last expression and rewriting in terms of kernel evaluations:
			 %\eqn{\|\mu_P - \mu_Q\|_\sH^2 = \E_{P \times P}[k(x,x')] - 2 \E_{P \times Q}[k(x,y)] + \E_{Q \times Q}[k(y,y')].}
			 \eqn{\|\mu_P - \mu_Q\|_\sH^2 = \E_{P \times P}[k(x,x')] - \E_{P \times Q}[k(x,y)] - \E_{Q \times P}[k(y,x)] + \E_{Q \times Q}[k(y,y')].}
		Of course, in practice, we only have samples from $P, Q$:
		 let $x_1, \dots, x_n \sim P$ and $y_1, \dots, y_n \sim Q$ all be drawn independently.
		We can obtain an empirical estimate of $D(P, Q, \sF)$
		 as a U-statistic (a U-statistic is a function which is an average over some function applied to all pairs of points):
		 \eqn{\hat D_n(P, Q, \sF) = \inv{\binom{n}{2}} \sum_{i < j} [k(x_i, x_j) - k(x_i, y_j) - k(y_i, x_j) + k(y_i, y_j)].}
		 This estimate is unbiased, since the expectation of each term is $D(P, Q, \sF)$.
		Let the null hypothesis be that $P = Q$.  Under the null hypothesis, as $n \to \infty$, we know that $\hat D_n(P, Q, \sF) \cvP 0$,
		 but in order to use $\hat D_n$ as a test statistic for hypothesis testing,
		 we need to know its (approximate) distribution.
		 (Recall that $\hat D_n(P, Q, \sF)$ is a random variable that is a function of the data points.)
		There are two ways to go about this:
			We can derive finite sample complexity bounds to bound the deviation of $\hat D(P, Q, \sF)$ from its mean $D(P, Q, \sF)$.
			We can show that $\hat D(P, Q, \sF)$ is asymptotically normal with some variance, and use the normal as an approximation
			 of the distribution.
		In the next section, we will develop the tools to analyze random variables such as these.
	Summary \currlecture
		We began by noting that some algorithms (e.g., gradient descent) do not require arbitrary inner products
		 between weight vectors and feature vectors,
		 but only \textbf{inner products between feature vectors}.
		This motivated the use of \textbf{kernels} (defined to be positive semidefinite functions),
		 which can provide both computational (ability to implicitly compute inner products between infinite-dimensional feature vectors)
		 and modeling advantages (thinking in terms of similarities between two inputs).
		Taking a step back, we saw that all that matters at the end of the day are functions evaluated at various inputs.
		 This motivated the definition of \textbf{reproducing kernel Hilbert spaces} (RKHS),
		 in which two important properties hold: (i) function evaluations were bounded (meaningful),
		 and (ii) there is a nice inner product structure.
		We showed that the three distinct viewpoints above (features, kernels, functions)
		 are actually all equivalent (due to the Moore-Aronszajn theorem).
		The \textbf{representer theorem}
		 shows that the optimum over an appropriately regularized function space $\sH$
		 is attained by a function in the span of the training data.
		 This allows us to derive kernelized SVMs, kernelized regression, kernel PCA,
		 RKHS embeddings of probability distributions.
		\textbf{Bochner's theorem},
		 allows us to study the Fourier properties of shift-invariant kernels,
		 relating universality and smoothness properties of
		 a kernel to the frequencies that the kernel passes through.
		Bochner's theorem allowed us to obtain computationally efficient ways to approximate
		 kernel methods by writing kernels as an integral over dot products of \textbf{random features}.
		 This leads to efficient algorithms.
		!comment \textbf{Gaussian processes} allow us to work with functions over an RKHS
		 but have uncertainty estimates over function values.
		 Uncertainty estimates are critical for active learning and Bayesian optimization.
	References
		Hofmann/Scholkopf/Smola, 2008: Kernel Methods in Machine Learning
			http://arxiv.org/PS_cache/math/pdf/0701/0701907v3.pdf
		Drineas/Mahoney, 2005: On the Nystrom Method for Approximating a Gram Matrix for Improved Kernel-Based Learning
			http://www.cs.yale.edu/homes/mmahoney/pubs/kernel_JMLR.pdf
		Rahimi/Recht, 2008: Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning
			http://www.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf
		Yang/Li/Mahdavi/Jin/Zhou, 2012: Nystrom Method vs Random Fourier Features: A Theoretical and Empirical Comparison
			http://papers.nips.cc/paper/4588-nystrom-method-vs-random-fourier-features-a-theoretical-and-empirical-comparison.pdf
		Kar/Karnick, 2012: Random Feature Maps for Dot Product Kernels
			http://jmlr.csail.mit.edu/proceedings/papers/v22/kar12/kar12.pdf
		!comment Skip
			Rasmussen/Williams, 2006: Gaussian Processes for Machine Learning
				http://www.gaussianprocess.org/gpml/chapters/
			Gretton/Borgwardt/Rasch/Schölkopf/Smola, 2008: A Kernel Method for the Two-Sample Problem
				http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchetal08.pdf

!verbatim \newpage
!verbatim \lecture{13}
Online learning
	Introduction \currlecture
		Thus far, we have analyzed algorithms (maximum likelihood, ERM) in the \emph{statistical setting},
		 where we assume the the training and test data are both drawn i.i.d.~from some distribution $p^*$.
		 We even boasted that we need not make any assumption about what $p^*$ is.
		 In this unit, we will weaken the assumptions even more and assume that data
		 can be generated completely \emph{adversarily}.
		 In addition, we will move to the online setting where training and test are interleaved.
		 Thus we make two shifts to the learning setup:
			Batch to online
			Statistical to adversarial
		We will first discuss the online learning framework, focusing on prediction.
		 Then, we will cast online learning as online convex optimization
		 and develop several algorithms and prove regret bounds for these algorithms.
		 Finally, we will look at multi-armed bandit problems, where
		 the learner obtains partial feedback.
		 Throughout this section, there will be very little probability
		 (since we will be working in the adversarial setting),
		 but we will draw quite a bit from convex analysis.
		Framework
			Prediction task: we want to map inputs $x \in \sX$ to outputs $y \in \sY$.
			The online learning setting can be thought of as the following game between a learner and nature:
			 \Fig{figures.slides/onlineLearningGame}{0.3}{onlineLearningGame2}{Online learning game.}
				Iterate $t = 1, \dots, T$:
					Learner receives input $x_t \in \sX$
					Learner outputs prediction $p_t \in \sY$
					Learner receives true label $y_t \in \sY$
					(Learner suffers loss $\ell(y_t, p_t)$)
					(Learner updates model parameters)
			More formally, the learner is a function $\sA$
			 that returns the current prediction given the full history:\footnote{
			 For now, assume deterministic algorithms.
			 Later, we'll consider stochastic algorithms, which will be important against adversaries.
			 }
			 \eqn{p_{t+1} = \sA(x_{1:t}, p_{1:t}, y_{1:t}, x_{t+1}).}
			 Nature can be defined similarly.
		\exampleHeading{spamFiltering}{online binary classification for spam filtering}
			Inputs: $\sX = \{0,1\}^d$ are boolean feature vectors (presence or absence of a word).
			Outputs: $\sY = \{+1,-1\}$: whether a document is spam or not spam.
			Zero-one loss: $\ell(y_t, p_t) = \1[y_t \neq p_t]$ is whether the prediction was incorrect.
		Remarks
			The typical training phase (setting model parameters)
			 and testing phase (making predictions for evaluation) are \textbf{interleaved} in online learning.
			Note that the the online learning setting leaves completely open the time and memory
			 usage of the algorithms that operate in this setting.  Technically, we could just train an SVM
			 on all the data that we've seen so far, and predict on the next example.
			 However, the spirit of online learning suggests that the amount of work an algorithm does per iteration
			 should not grow with $t$.
			 In practice, online learning algorithms update parameters after each example,
			 and hence tend to be \textbf{faster} than traditional batch optimization algorithms such as Newton's method.
			The real world is complex and constantly-changing,
			 but online learning algorithms have the potential to \textbf{adapt}
			 (although we will not analyze their adapative capabilities in this course).
			In some applications such as spam filtering,
			 the inputs could be generated by an \textbf{adversary}.
			 In our analyses, we will make no assumptions about the input/output sequence.
		Evaluation
			Now comes the most important part:
			 How we measure the quality of an online learner $\sA$?
			 While seemingly simple and obvious,
			 how we answer this question has a \emph{defining impact} on the behavior of $\sA$.
			The first attempt is to just write down the cumulative loss of the learner:
			 \eqn{\sum_{t=1}^T \ell(y_t, p_t).}
			 However, if we are in the adversarial setting,
			 no algorithm can do better than the maximum regret $T$ (for the zero-one loss)
			 since the adversary can always choose $y_t \neq p_t$.
			 What do you do when your performance is awful?
			 You show that others are doing even worse than you.
			Less flippantly, let $\sH$ be a set of experts,
			 where each \word{expert} is a function $h : \sX \to \sY$ that predicts $h(x)$ on input $x \in \sX$.
			 Note that the learner can adapt (change over time) whereas the experts are fixed.
			\definitionHeading{regret}{\word{regret}}
				The regret of a learner with respect to an expert $h$ is the
				 cumulative difference between the loss incurred by the learner and the loss incurred by expert $h$:
				 \eqn{\Regret(h) \eqdef \sum_{t=1}^T [\ell(y_t, p_t) - \ell(y_t, h(x_t))].}
				 Note that $\Regret(h)$ depends on $\sH$, $T$, the sequences $x_{1:T},y_{1:T}$,
				 and of course the algorithm $\sA$ itself;
				 but we're eliding the dependence on all of these things in the notation.
				The regret with respect to a class of experts $\sH$ is the maximum regret
				 \eqn{\Regret \eqdef \max_{h \in \sH} \Regret(h).}
				 Equivalently, we can write:
				 \eqn{\Regret = \underbrace{\sum_{t=1}^T \ell(y_t, p_t)}_\text{learner} - \underbrace{\min_{h \in \sH} \sum_{t=1}^T \ell(y_t, h(x_t))}_\text{best expert}.}
			The best expert is a role model for the learner.
			 While it is technically possible for the learner to do better than all the experts
			 since it can adapt over time (leading to negative regret),
			 this generally won't happen,
			 and therefore, we will be content with trying to achieve small positive regret.
			We are interested in particular in the maximum possible regret over all sequences $x_{1:T},y_{1:T}$;
			 in more colorful language, an adversary chooses the inputs and outputs as to maximize regret.
			Having a comparison set of experts gives us some hope to compete against an adversary.
			 Intuitively, if the adversary tries to screw over the learner, it will probably screw over the experts too.
		In the next few lectures, we will prove results of the following form for various online learning algorithms $\sA$:
		 for all $\sH, T, x_{1:T}, y_{1:T}$, we have:
		 \eqn{\Regret \le \SomeFunction(\sH, T).}
		 Usually, we want the regret to be sublinear in $T$,
		 which means that the average regret per example goes to zero.
	Warm-up \currlecture
		!tmpformat N
		We will give two examples, one where online learning is impossible and one where it is possible.
		\exampleHeading{coverExample}{negative result}
			Assume binary classification: $y \in \{-1,+1\}$
			Assume zero-one loss: $\ell(y_t, p_t) = \1[y_t \neq p_t]$
			Assume the learner is fully \textbf{deterministic}.
			Claim: for all learners $\sA$, there exists an $\sH$ and input/output sequence $x_{1:T},y_{1:T}$ such that:
			 \eqn{\boxed{\Regret \ge T/2} \aside{awful!}}
			\textbf{Key point}: adversary (having full knowledge of learner) can choose $y_t$ to be always
			 different from $p_t$.
			Learner's cumulative loss: $T$ (make mistake on every example).
			We're not done yet, because remember regret is the difference between learner's cumulative loss and the best expert's,
			 so we have to check how well the experts do in this case.
			Consider two experts, $\sH = \{ h_{-1}, h_{+1} \}$, where $h_y$ always predicts $y$.
			The sum of the cumulative losses of the experts equals $T$
			 because $\ell(y_t, h_{-1}(x_t)) + \ell(y_t, h_{+1}(x_t)) = 1$,
			 so one of the experts must achieve loss $\le T/2$.
			Therefore, the difference between $T$ and $\le T/2$ is $\ge T/2$.
			It is perhaps not surprising that no learner can do well
			 because nature is too powerful here (it can choose any sequence with full
			 knowledge of what the learner will do).
			 Not even measuring regret with respect to only two experts is enough.
			 So we will need assumptions to get positive results.
		\exampleHeading{majorityExample}{positive result (learning with expert advice)}
			Assume zero-one loss: $\ell(y_t, p_t) = \1[y_t \neq p_t]$.
			Clearly, we need to make \emph{some} assumptions to make progress.
			 Here, we will make a fairly strong assumption just to get some intuition.
			\assumptionHeading{realizableMajority}{\word{realizable}}
			 Assume the best expert $h^* \in \sH$ obtains zero cumulative loss
			 ($\ell(y_t, h^*(x_t)) = 0$ for all $t = 1, \dots, T$);
			 or equivalently, since we're dealing with zero-one loss, $y_t = h^*(x_t)$ for all $t$.
			 This assumption places a joint restriction on $\sH, x_{1:T}, y_{1:T}$.
			We will design an algorithm that queries the experts on each example $x_t$,
			 and try to combine their predictions in some way;
			 this setting is called \word{learning with expert advice}.
			\algHeading{majority}{\word{majority algorithm}}
				Maintain a set $V_t \subseteq \sH$ of valid experts (those compatible with the first $t-1$ examples).
				On each iteration $t$,
				 predict $p_t$ to be the majority vote over
				 predictions of valid experts $\{ h(x_t) : h \in V_t \}$.
				Keep experts which were correct:
				 $V_{t+1} = \{ h \in V_t : y_t = h(x_t) \}$.
			Analysis:
				On each mistake, at least half of the experts are eliminated.
				So $1 \le |V_{T+1}| \le |\sH| 2^{-M}$, where $M$ is the number of mistakes
				 (also equal to $\Regret$ since the best expert has zero loss).
				Note that the lower bound is due to the realizability assumption.
				$M$ is the exactly the regret here.
				Take logs and rearrange:
				 \eqn{\boxed{\Regret \le \log_2 |\sH|.}}
			Notes:
				This is a really strong result: note that the regret is constant;
				 after a finite number of iterations, we cease to make any more mistakes forever.
				However, realizability (that some expert is perfect) is too strong of an assumption.
				Also, the regret bound is useless here if there are an infinite number of experts ($|\sH| = \infty$).
	Online convex optimization \currlecture
		In order to obtain more general results,
		 we move to a framework called online convex optimization.
		 \word{Convexity} will give us considerable leverage.
		 Afterwards, we'll connect online convex optimization with online learning.
		Let $S \subseteq \R^d$ be a convex set
		 (e.g., representing the set of allowed weight vectors).
			Example: $S = \{ u : \|u\|_2 \le B \}$.
		FIGURE: [convex function with subgradients]
		\definitionHeading{convex}{\word{convexity}}
		 A function $f : S \to \R$ is convex iff for all
		 points $w \in S$, there is some vector $z \in \R^d$ such that
		 \eqnl{convex}{\boxed{f(u) \ge f(w) + z \cdot (u - w) \quad \text{ for all } u \in S.}}
		 This says that at any point $w \in S$,
		 we can find a linear approximation (RHS of \refeqn{convex})
		 that lower bounds the function $f$ (LHS of \refeqn{convex}).
		\definitionHeading{subgradient}{\word{subgradient}}
		 For each $w \in S$, the set of all $z$ satisfying \refeqn{convex}
		 are known as the subgradients at $w$:
		 \eqn{\boxed{\partial f(w) \eqdef \{ z : f(u) \ge f(w) + z \cdot (u - w) \text{ for all } u \in S \}.}}
		 If $f$ is differentiable at $w$, then there is one subgradient
		 equal to the gradient: $\partial f(w) = \{ \nabla f(w) \}$.
		Convexity is remarkable because it allows you to say something (in particular, lower bound)
		 the global behavior (for all $u \in S$) of a function $f$ by something local and simple
		 (a linear function).  An important consequence of $f$ being convex is that
		 if $0 \in \partial f(w)$, then $w$ is a global minimum of $f$.
		Checking convexity
			How do you know if a function is convex?  You can try to work it out
			 from the definition or if the function is twice-differentiable, compute
			 the Hessian and show that it's positive semidefinite.
			But often, we can check convexity by decomposing the function using
			 a few rules.  This is by no means an exhaustive list,
			 but these are essentially all the important cases that we'll need in this class.
				Linear functions: $f(w) = w \cdot z$ for any $z \in \R^d$
				Quadratic functions: $f(w) = w^\top A w$ for positive semidefinite $A \in \R^{d \times d}$
				Negative entropy: $f(w) = \sum_{i=1}^d w_i \log w_i$ on $w \in \Delta_d$.
				Sum: $f + g$ if $f$ and $g$ are convex
				Scale: $c f$ where $c \ge 0$ and $f$ is convex
				Supremum: $\sup_{f \in \sF} f$, where $\sF$ is a family of convex functions
			Example: hinge loss (skipped in class)
				Function: $f(w) = \max\{0, 1 - y (w \cdot x)\}$.
				Subgradient:
					$\partial f(w) = \{0\}$ if $y (w \cdot x) > 1$ ($w$ achieves margin at least $1$)
					$\partial f(w) = \{-y x\}$ if $y (w \cdot x) < 1$
					$\partial f(w) = \{-y x \alpha : \alpha \in [0, 1]\}$ if $y (w \cdot x) = 1$
		The setup for online convex optimization is similar to online learning:
			Iterate $t = 1, \dots, T$:
				Learner chooses $w_t \in S$
				Nature chooses convex loss function $f_t : S \to \R$
			 Formally, the learner $\sA$ chooses $w_{t+1}$ depending on the past:
			 \eqn{w_{t+1} = \sA(w_{1:t}, f_{1:t}).}
			Regret is defined in the way you would expect (cumulative difference of losses):
			 \eqn{\Regret(u) &\eqdef \sum_{t=1}^T [f_t(w_t) - f_t(u)]. \\
			 \Regret &\eqdef \max_{u \in S} \Regret(u).}
			The set $S$ plays two roles:
			 it is the set of experts with which we define our regret,
			 and it is also the set of parameters that our learner is going to consider.
			 For simplicity, we assume these two are the same,
			 although in general, they do not have to be.
		We now turn to our original goal of doing online learning.
		 We will show some examples of reducing online learning
		 to online convex optimization.  In particular, given an input/output sequence
		 input $x_{1:T},y_{1:T}$, we will construct a sequence of convex functions $f_{1:T}$
		 such that the low regret incurred by a learner on the online convex optimization problem
		 implies low regret on the online learning problem.
		 Let OL be the online learner who has access to OCO, a online convex optimizer.
			\exampleHeading{linearRegression}{linear regression}
				FIGURE: [two boxes, OL and OCO with arrows between]
				Assume we are doing linear regression with the squared loss,
				 $\ell(y_t, p_t) = (p_t - y_t)^2$.
				On each iteration $t = 1, \dots, T$:
					OL receives an input $x_t \in \R^d$ from nature.
					OL asks OCO for a weight vector $w_t \in \R^d$.
					OL sends the prediction $p_t = w_t \cdot x_t$ to nature.
					OL receives the true output $y_t$ from nature.
					OL relays the feedback to OCO via the loss function
					 \eqn{f_t(w) = (w \cdot x_t - y_t)^2.}
				 Note that $(x_t,y_t)$ is baked into $f_t$, which changes each iteration.
				 Since the squared loss is convex, $f_t$ is convex.
				 One can check easily based on matching definitions
				 that the regret of OCO is exactly the same as regret of OL.
			\exampleHeading{expertAdvice}{learning with expert advice}
				Now let's turn to a problem where the convexity structure isn't as apparent.
				Assume we have a finite number (this is important) of experts $\sH$,
				 which the learner can query.  Let
				 \eqn{\sH = \{ h_1, \dots, h_d \}.}
				Assume we are doing binary classification with the zero-one loss (this is not important---any bounded loss function will do),
				 $\ell(y_t, p_t) = \1[y_t \neq p_t]$.
				Note that neither the loss function nor the set of experts $\sH$ is convex;
				 in fact, this doesn't really make sense, since the domains are discrete.
				Nonetheless, we can convexify the problem using \word{randomization}.
				 Specifically, we allow the learner to produce a probability distribution $w_t \in \Delta_d$
				 ($\Delta_d \subseteq \R^d$ is the $(d-1)$-dimensional simplex)\footnote{
				 Formally, think of a probability distribution over a random variable $J \in \{1, \dots, d\}$, with $\BP[J = j] = w_j$.}
				 over the $d$ experts $\sH$, sample an expert from this distribution, and predict according to that expert.
				 The expected zero-one loss is then a convex
				 (in fact, linear) function of $w_t$.
				Formally, the reduction is as follows:
					OL receives an input $x_t$ from nature.
					OL asks OCO for a weight vector $w_t \in \Delta_d$, which represents a distribution over $d$ experts.
					OL samples an expert $j_t \sim w_t$ and send prediction $p_t = h_{j_t}(x_t)$ to nature.
					OL receives the true output $y_t$ from nature.
					OL relays the feedback to OCO via the loss function
					 \eqn{f_t(w) = w \cdot z_t,} where $z_t$ is the vector of losses incurred by each expert:
					 \eqn{z_t = [\ell(y_t, h_1(x_t)), \dots, \ell(y_t, h_d(x_t))] \in \{0,1\}^d.}
					 Again, $f_t$ bakes the loss and the data into the same function,
					 and one can check that the expected regret of OL is the same as the regret of OCO.
					 Note that we assume no structure on $x_t$ and $y_t$---they could be arbitrarily complicated;
					 we are only exposed to them via the the experts and the loss function.
				This convexify-by-randomization trick applies more generally
				 to any loss function and any output space $\sY$.
				 The key is that the set of experts is finite,
				 and the learner is just choosing a convex combination of those experts.
				Yet another way to convexify non-convex losses without randomization
				 is to use an upper bound
				 (e.g., hinge loss or logistic loss upper bounds the zero-one loss).
				 However, minimizing the convex upper bound does not guarantee
				 minimizing the zero-one loss.
		!comment Relationship to convex optimization
			Before we talk about how to solve online convex optimization,
			 let's see what the implications are for ordinary convex optimization,
			 which is what one would typically turn to in the batch learning setting,
			 where one gets all the data up front.
			Recall that in convex optimization, we are given a single convex function $f$
			 and asked to minimize it, that is, compute:
			 \eqn{w^* \in \arg\min_{w \in S} f(w).}
			 Often in machine learning, the function $f$ we're minimizing is
			 a sum of convex functions, one for each of $n$ data points:
			 \eqn{f(w) = \inv{n} \sum_{i=1}^n g_i(w),}
			 where each $g_i$ is convex.
			 For example, $g_i$ could be the hinge loss on training example $(x_i,y_i)$:
			 $g_i(w) = \max \{0, 1 - y_i (w \cdot x_i) \}$.
			We can use a online convex optimization algorithm to minimize $f$ as follows.
			 At each time step $t$, choose a random $g_i$ and feed it to the learner.
			 Formally, let $f_t = g_{i_t}$, where $i_t$ is drawn uniformly from $\{ 1, \dots, n \}$.
			 An important property of this construction is that $f_t$ is equal to $f$ in expectation:
			 \eqnl{relatefft}{f(w) = \E[f_t(w)].}
			After $T$ iterations, take the average over all the weight vectors that the learner predicted:
			 \eqn{\bar w = \inv{T} \sum_{t=1}^T w_t.}
			We claim now that $\bar w$ is good with respect to $w^*$.
			 By the definition of regret:
			 \eqn{\Regret(w^*) = \sum_{t=1}^T [f_t(w_t) - f_t(w^*)].}
			 After dividing by $T$, rearranging, and taking expectations:
			 \eqnl{expectedRegretf}{\E\pb{\inv T \sum_{t=1}^T f_t(w_t)} = \E\pb{\inv T \sum_{t=1}^T f_t(w^*)} + \frac{\E[\Regret(w^*)]}{T}.}
				Let's analyze the LHS of \refeqn{expectedRegretf}.
				 First, by definition of $\bar w$ and Jensen's inequality (uniform distribution over $\{1, \dots, T\}$),
				 we have:
				 \eqn{f(\bar w) \le \inv{T} \sum_{t=1}^T f(w_t).}
				 Taking expectations, and using linearity of expectation, and the fact that $f = \E[f_t]$:
				 \eqn{\E[f(\bar w)] \le \E\pb{\inv{T} \sum_{t=1}^T f(w_t)} = \E\pb{\inv{T} \sum_{t=1}^T f_t(w_t)}.}
				Let's now analyze the first term of the RHS of \refeqn{expectedRegretf}.
				 Since $f = \E[f_t]$,
				 \eqn{f(w^*) = \E\pb{\inv{T} \sum_{t=1}^T f_t(w^*)}.}
				Therefore, we can conclude
				 \eqn{\boxed{\E[f(\bar w)] \le f(w^*) + \frac{\E[\Regret(w^*)]}{T}.}}
			You should think of $\E[\Regret(w^*)]$ as either $O(1)$, $O(\log T)$, or $O(\sqrt{T})$,
			 so that $\E[\Regret(w^*)]$ going to zero means convergence to the global minimum of $f$
			 (solving the original convex optimization problem).
			By optimization standards,
			 $O(1/\sqrt{T})$ or even $O(1/T)$ is very slow.
			 Standard (batch) optimization algorithms such as gradient descent
			 (with the appropriate step size)
			 can, though not in all cases,
			 obtain linear (confusingly, also called exponential or geometric) convergence ($O(e^{-c(T/n)})$)
			 or even superlinear convergence ($O(e^{-c(T/n)^2})$).
			There are two reasons why you might deliberately not choose an algorithm with these rates:
				First, note the difference in the dependence on $n$ in the batch and online settings.
				 Each iteration of an \word{online} algorithm processes only one example,
				 whereas each iteration of an \word{batch} algorithm processes all $n$ examples to compute $\nabla f(w)$,
				 which is $n$ times more expensive.
				 When we have large datasets, then online algorithms can be faster.
				Second, for machine learning applications, there is no need to optimize
				 $f$ to death since there is noise in the data anyway,
				 and we'll show later that the statistical error is $O(1/\sqrt{T})$,
				 so one could even argue that running an online algorithm over the data
				 once ($T = n$) is theoretically good enough.
	Follow the leader (FTL) \currlecture
		!tmpformat N
		We first start out with a natural algorithm called follow the leader (FTL),
		 which in some sense is the analog of the majority algorithm for online learning.
		 We'll see that it works for quadratic functions but fails for linear functions.
		 This will give us intuition about how to fix up our algorithm.
		\algHeading{FTL}{\word{follow the leader (FTL)}}
			Let $f_1, \dots, f_T$ be the sequence of loss functions played by nature.
			The learner chooses the weight vector $w_t \in S$ that minimizes the cumulative loss so far on the previous $t-1$ iterations:
			 \eqnl{FTL}{\boxed{w_t \in \arg\min_{w \in S} \sum_{i=1}^{t-1} f_i(w).}}
			 (If there are multiple minima, choose any one of them.  This is not important.)
			Aside: solving this optimization problem at each iteration
			 is expensive in general (which would seem to destroy the spirit of online learning),
			 but we'll consider special cases with analytic solutions.
			Note: We can think of FTL as an empirical risk minimizer
			 where the training set is the first $t-1$ examples.
		We want to now study the regret of FTL.
		 Regret compares the learner (FTL) with any expert $u \in S$,
		 but this difference can be hard to reason about.
		 So to get started, we will use the following result (\reflem{FTL})
		 to replace $u$ in the bound by (i) something easier to compare $w_t$ to and
		 (ii) at least as good as any $u \in S$.
		\lemmaHeading{FTL}{compare FTL with one-step lookahead cheater}
			Let $f_1, \dots, f_T$ be any sequence of loss functions.
			Let $w_1, \dots, w_T$ be produced by FTL according to \refeqn{FTL}.  For any $u \in S$:
			 \eqn{\boxed{\Regret(u) \eqdef \sum_{t=1}^T [f_t(w_t) - f_t(u)] \le \sum_{t=1}^T [f_t(w_t) - \blue{f_t(w_{t+1})}].}}
			Note: This is saying our regret against the best fixed $u$ is no worse
			 than comparing against the one-step lookahead $w_{t+1}$
			 that peeks at the current function $f_t$ (which is cheating!).
			Note: The RHS terms $f_t(w_t) - f_t(w_{t+1})$ measure how \emph{stable}
			 the algorithm is; smaller is better.
			 Stability is an important intuition to develop.
		Proof of \reflem{FTL}:
			Subtracting $\sum_t f_t(w_t)$ from both sides,
			 it suffices to show
			 \eqn{\sum_{t=1}^T f_t(w_{t+1}) \le \sum_{t=1}^T f_t(u)}
			 for all $u \in S$.
			 Intuitively, this says that $w_{t+1}$
			 (which takes the minimum over the first $t$ functions)
			 is better than using a fixed $u$ for all time.
			Proof by induction:
				Assume the inductive hypothesis on $T-1$:
				 \eqn{\sum_{t=1}^{T-1} f_t(w_{t+1}) \le \sum_{t=1}^{T-1} f_t(u) \quad \text{for all } u \in S.}
				Add $f_T(w_{T+1})$ to both sides:
				 \eqn{\sum_{t=1}^T f_t(w_{t+1}) \le \sum_{t=1}^{T-1} f_t(u) + f_T(w_{T+1}) \text{ for all } u \in S.}
				In particular, this holds for $u = w_{T+1}$, so we have:
				 \eqn{\sum_{t=1}^T f_t(w_{t+1}) \le \sum_{t=1}^T f_t(w_{T+1}).}
				Since $w_{T+1} \in \arg\min_u \sum_{t=1}^T f_t(u)$ by definition of FTL, we have:
				 \eqn{\sum_{t=1}^T f_t(w_{t+1}) \le \sum_{t=1}^T f_t(u) \text{ for all } u \in S,}
				 which is the inductive hypothesis for $T$.
		Note that \reflem{FTL} doesn't actually require on convexity of $f_t$,
		 but rather only stability of the iterates $\{w_t\}$ as measured by $f_t$.
		 As we'll see later, strong convexity is the main way we will achieve this stability.
		 For now, let's consider two examples to gain some intuition:
		 one where the $\{w_t\}$ are stable (\refex{quadraticOptimization})
		 and one where they are not (\refex{linearOptimization}).
		\exampleHeading{quadraticOptimization}{quadratic optimization: FTL works}
			Assume nature always chooses quadratic functions:
			 \eqn{f_t(w) = \half \|w - z_t\|_2^2,}
			 where the points are bounded: $\|z_t\|_2 \le L$ for all $t = 1, \dots, T$.
			FTL (minimizing over $S = \R^d$) has a closed form solution, which is just the average of the previous points:
			 \eqn{w_t = \inv{t-1} \sum_{i=1}^{t-1} z_i.}
			Bound one term of the RHS of \reflem{FTL} (intuitively the difference is only one term):
			 \eqn{f_t(w_t) - f_t(w_{t+1})
			 &= \half \|w_t-z_t\|_2^2 - \half \|(1-1/t)w_t + (1/t)z_t - z_t\|_2^2 \\
			 &= \half (1 - (1-1/t)^2) \|w_t-z_t\|_2^2 \\
			 &\le \blue{(1/t)} \|w_t-z_t\|_2^2 \\
			 &\le (1/t) 4L^2.
			 }
			Side calculation: summing $1/t$ yields $\log T$:
			 \eqn{\sum_{t=1}^T (1/t) \le 1 + \int_1^{T} (1/t) dt = \log(T) + 1.}
			Summing over $t$ yields:
			 \eqn{\boxed{\Regret \le \sum_{t=1}^T [f_t(w_t) - f_t(w_{t+1})] \le 4L^2 (\log(T) + 1).}}
			The important thing here is that the difference between $w_t$ and $w_{t+1}$
			 (measured in terms of loss) is really small (only $1/t$),
			 which means that FTL for quadratic functions is really stable.
			 This makes sense because averages are stable: adding an extra data point should only affect the running average by $O(1/t)$.
		\exampleHeading{linearOptimization}{linear optimization: FTL fails}
			We will construct an evil example to make FTL fail.
			Let $S = [-1, 1]$ be FTL's possible predictions.
			 This is a nice bounded one-dimensional convex set,
			 so we're not even trying hard to be pathological.
			Consider linear functions $f_t(w) = w z_t$ in $d=1$ dimension,
			 where \eqn{(z_1, z_2, \dots) = (-0.5, 1, -1, 1, -1, 1, -1, \dots).}
			The minimizer computed by FTL will be attained at an extreme point,
			 causing oscillating behavior.
			 \eqn{(w_1, w_2, \dots) = (0, 1, -1, 1, -1, 1, -1, 1, \dots).}
			FTL obtains $T-1$ cumulative loss (get loss 1 on every single example except the first).
			Expert $u = 0$ obtains $0$ cumulative loss (not necessarily even the best).
			Therefore, the regret is pretty depressing:
			 \eqn{\boxed{\Regret \ge T-1}.}
		What's the lesson?
			For these quadratic functions,
			 $w_t$ and $w_{t+1}$ must get closer (low regret).
			For these linear functions,
			 $w_t$ and $w_{t+1}$ do not get closer (high regret).
			It seems then that FTL works when functions $f_t$ offer
			 ``stability'' (e.g., quadratic) but fail when they do not (e.g., linear).
			We will reveal the more general principle (strong convexity) later work.
	!verbatim \lecture{14}
	Follow the regularized leader (FTRL) \currlecture
		It would be nice if nature just handed us quadratic-looking $f_t$'s,
		 but in general, we're not the ones calling the shots there.
		 But we do control the learner,
		 so the key idea is to \emph{add some regularization} of our own
		 to stablize the learner.
		\algHeading{FTRL}{\word{follow the regularized leader (FTRL)}}
			Let $\psi : S \to \R$ be a function called a \word{regularizer}
			 (this defines the learner).
			Let $f_1, \dots, f_T$ be the sequence of loss functions played by nature.
			On iteration $t$, the learner chooses the weight vector that minimizes the regularizer plus the losses on the first $t-1$ examples:
			 \eqnl{FTRL}{\boxed{w_t \in \arg\min_{w \in S} \blue{\psi(w) +} \sum_{i=1}^{t-1} f_i(w).}}
			Note: FTL is just FTRL with $\psi = 0$.
		Quadratic $\psi$, linear $f_t$
			For the remainder of the section,
			 just to build the right intuition in a transparent way,
			 let's specialize to quadratic regularizers $\psi$ and linear loss functions $f_t$:
			 \eqnl{quadraticLinear}{\psi(w) = \frac{1}{2 \eta} \|w\|_2^2, \quad\quad f_t(w) = w \cdot z_t.}
			Then the FTRL optimization problem \refeqn{FTRL} is:
			 \eqnl{quadraticLinearFTRL}{w_t = \arg\min_{w \in S} \left\{ \frac{1}{2\eta} \|w\|_2^2 - w \cdot \theta_t \right\},}
			 where \eqn{\theta_t = -\sum_{i=1}^{t-1} z_i} is the negative sum of the gradients $z_t$.
			 Interpret $\theta_t$ as the direction that we want to move in to reduce the loss,
			 but now, unlike in FTL, we're held back by the quadratic regularizer.
			If $S = \R^d$, then FTRL has a closed form solution:
			 \eqnl{wTheta}{w_t = \eta \theta_t,}
			 a scaled down version of $\theta_t$.
			 We can write $w_t = -\eta z_1 - \eta z_2 - \cdots - \eta z_{t-1}$ and
			 equivalently think of the weights as being updated incrementally according to the following recurrence:
			 \eqnl{wRecursive}{\boxed{w_{t+1} = w_{t} - \eta z_{t}.}}
			 From this perspective, the recurrence in \refeqn{wRecursive} looks 
			 awfully like an online subgradient update where $z_t$ is the gradient
			 and $\eta$ is the step size.
			If $S \neq \R^d$, then FTRL requires a projection onto $S$.
			 We rewrite \refeqn{quadraticLinearFTRL} by completing the square (add $\frac{\eta}{2} \|\theta_t\|_2^2$):
			 \eqn{\boxed{w_t \in \arg\min_{w \in S} \frac{1}{2\eta} \|w - \eta \theta_t\|_2^2 = \Pi_S(\eta \theta_t),}}
			 which is a Euclidean projection of $\eta \theta_t$ onto set $S$.
			 This is called a \word{lazy projection} since $\theta_t$ still accumulates unprojected gradients,
			 and we only project when we need to obtain a weight vector $w_t$ for prediction.
			 This is also known as Nesterov's \word{dual averaging} algorithm.
		Regularizers in online and batch learning
			It's worth pausing to examine the difference
			 in the way regularization enters online learning versus batch learning,
			 with which you're probably more familiar.
			In batch learning (e.g., in training an SVM or ridge regression),
			 one typically seeks to optimize
			 a function which is the sum of the training loss plus the regularizer.
			 Notably, the regularizer is part of the objective function.
			In online learning here, our objective in some sense is the regret,
			 which makes no mention of the regularizer.
			 The regularizer lies purely inside the learner's head,
			 and is used to defines the updates.
			 In our example so far, the regularizer (in the context of FTRL)
			 gives birth to the online gradient algorithm in \refeqn{wRecursive}.
		Now we will analyze the regret FTRL for quadratic regularizers and linear losses.
		\theoremHeading{FTRL}{regret of FTRL}
			Let $S \subseteq \R^d$ be a convex set (of weight vectors).
			Let $f_1, \dots, f_T$ be any sequence of linear loss functions:
			 $f_t(w) = w \cdot z_t$ for some $z_t \in \R^d$.
			Let $\psi(w) = \frac{1}{2 \eta} \|w\|_2^2$
			 be a quadratic regularizer for any $\eta > 0$.
			Then the regret of FTRL (as defined in \refalg{FTRL}) with respect to any $u \in S$ is as follows:
			 \eqnl{FTRLregret}{\boxed{\Regret(u) \le \inv{2 \eta} \|u\|_2^2 + \frac{\eta}{2} \sum_{t=1}^T \|z_t\|_2^2.}}
		Interpretation: the step size $\eta$ allows us to trade off two terms:
			First term: ``squared bias'' due to regularization.
			 To compete with $u$, the learner's iterates $w_t$ must somehow
			 get close to $u$.
			 Smaller $\eta$ means more regularization (remember $w_t = \eta \theta_t$), which
			 means it's harder to get there.
			Second term: ``variance'' due to changing $z_t$.
			 A smaller $\eta$ means that successive weight vectors are closer
			 and thus stabler (recall $w_{t+1} - w_t = -\eta z_t$).
			 With a small $\eta$,
			 we can hope to avoid the bad scenario in \refex{linearOptimization}.
		FIGURE: [ellipse with $u$ at edge, draw iterates $w_t$ trying to reach $u$]
		Corollary:
			To make the bound look cleaner:
				Let $\|z_t\|_2 \le L$.
				Let $\|u\|_2 \le B$ for all experts $u \in S$.
			 Now \refeqn{FTRLregret} can be rewritten as:
			 \eqn{\Regret(u) \le \frac{B^2}{2 \eta} + \frac{\eta T L^2}{2}.}
			Side calculation:
				Suppose we want to minimize
				 some function with the following form:
				 $C(\eta) = a/\eta + b \eta$.
				Take the derivative: $-a/\eta^2 + b = 0$,
				 resulting in $\eta = \sqrt{a/b}$ and $C(\eta) = 2\sqrt{ab}$,
				 which is just twice the geometric average of $a$ and $b$.
			Letting $a = B^2/2$ and $b = TL^2/2$,
			 we get $\eta = \frac{B}{L\sqrt{T}}$ and
			 \eqnl{BLT}{\boxed{\Regret \le B L \sqrt{T}.}}
			 Note that the average regret goes to zero as desired,
			 even though not as fast as for quadratic functions ($\log T$).
		Proof of weakened version of \refthm{FTRL}
			We will prove \refthm{FTRL} later using Bregman divergences,
			 but just to give some intuition without requiring too much technical machinery,
			 we will instead prove a slightly weaker result
			 (note that the second term is looser by a factor of $2$):
			 \eqn{\boxed{\Regret(u) \le \inv{2 \eta} \|u\|_2^2 + \blue{\eta} \sum_{t=1}^T \|z_t\|_2^2.}}
			The key idea is to reduce FTRL to FTL.
			 Observe that FTRL is the same as FTL where
			 the first function is the regularizer.
			Let us then apply \reflem{FTL} to the sequence of functions
			 $\psi, f_1, \dots, f_T$ (when applying the theorem,
			 note that the indices are shifted by 1).
			 This results in the bound:
			 \eqn{[\psi(w_0) - \psi(u)] + \sum_{t=1}^T [f_t(w_t) - f_t(u)] \le [\psi(w_0) - \psi(w_1)] + \sum_{t=1}^T [f_t(w_t) - f_t(w_{t+1})].}
			Canceling $\psi(w_0)$, noting that $\psi(w_1) \ge 0$, and rearranging, we get:
			 \eqnl{FTRLregretRewritten}{\Regret(u)
			 \eqdef \sum_{t=1}^T [f_t(w_t) - f_t(u)]
			 \le \frac{1}{2 \eta} \|u\|_2^2 + \sum_{t=1}^T [f_t(w_t) - f_t(w_{t+1})].}
			Now let's bound one term of the RHS sum:
			 \eqn{f_t(w_t) - f_t(w_{t+1})
			 &= z_t \cdot (w_t - w_{t+1}) \aside{since $f_t(w) = w \cdot z_t$} \\
			 &\le \|z_t\|_2 \|w_t - w_{t+1}\|_2 \aside{Cauchy-Schwartz} \\
			 &= \|z_t\|_2 \|\Pi_S(\eta \theta_{t}) - \Pi_S(\eta \theta_{t+1})\|_2 \aside{since $w_t = \Pi_S(\eta\theta_t)$} \\
			 &\le \|z_t\|_2 \|\eta \theta_{t} - \eta \theta_{t+1}\|_2 \aside{projection decreases distance} \\
			 &= \eta \|z_t\|_2^2 \aside{since $\theta_{t+1} = \theta_t - z_t$}.}
			 Plugging this bound back into \refeqn{FTRLregretRewritten} completes the proof.
	Online subgradient descent (OGD) \currlecture
		So far, we have proven regret bounds for FTRL with linear loss functions.
		 However, many loss functions we care about in practice (e.g., squared
		 loss, hinge loss) are not linear.
		Even if did derive a regret for FTRL with more general losses,
		 there would still be a computational problem:
		 FTRL (see \refeqn{FTRL}) requires minimizing over all the loss
		 functions seen so far, which in general is computationally impractical,
		 especially for an online learning algorithm.
		 For linear loss functions, on the other hand,
		 we could optimize $w_t$ easily by maintaining
		 $\theta_t$ as a ``sufficient statistics'' (by linearity).
		Our strategy to handling general losses efficiently
		 is by appealing to the linear machinery we already have.
		 The key idea is to \emph{run FTRL on a linear approximation of $f_t$}.
		What linear approximation $w \mapsto w \cdot z_t$ should we use?
		 Let's use the subgradient of $f_t$ at the current weights $w_t$:
		 take any $z_t \in \partial f_t(w_t)$.
		 Just to highlight the simplicity of the algorithm,
		 here it is:
		\algHeading{ogd}{\word{Online subgradient descent (OGD)}}
			Let $w_1 = 0$.
			For iteration $t = 1, \dots, T$:
				Predict $w_t$ and receive $f_t$.
				Take any subgradient $z_t \in \partial f_t(w_t)$.
				If $S = \R^d$, perform the update:
				 \eqn{w_{t+1} = w_t - \eta z_t.}
				If $S \subseteq \R^d$, project cumulative gradients onto $S$:
				 \eqn{w_{t+1} = \Pi_S(\eta \theta_{t+1}), \quad \theta_{t+1} = \theta_t - z_t.}
		To emphasize: OGD on $f_t$ is nothing more than FTRL on
		 quadratic regularizers and linear subgradient approximations of $f_t$.
		Analyzing regret
			From our earlier analysis of FTRL (\refthm{FTRL}),
			 we already have a bound on the regret on the linearized losses:
			 \eqn{\sum_{t=1}^T [w_t \cdot z_t - u \cdot z_t].}
			We are interested on controlling the actual regret with respect to $f_t$:
			 \eqn{\sum_{t=1}^T [f_t(w_t) - f(u)].}
			How do we relate these two?
			 Here's where \emph{convexity} comes in a crucial way.
			Since $z_t \in \partial f_t(w_t)$ is a subgradient,
			 we have by the definition of subgradient:
			 \eqn{f_t(u) \ge f_t(w_t) + z_t \cdot (u - w_t).}
			 Rearranging, we get a direct comparison for each term of the regret:
			 \eqnl{convexityFTRL}{\boxed{f_t(w_t) - f_t(u) \le (w_t \cdot z_t) - (u \cdot z_t).}}
			The upshot is that the bounds we got for linear losses
			 apply without modification to general losses!
			 Intuitively, linear functions are the hardest to optimize using online convex optimization.
			FIGURE: [draw convex $f_t$ with linearized]
		Remarks:
			OGD works for any convex loss function $f_t$ (so does FTRL, but we only analyzed it for linear losses).
			OGD is in some sense the first practical, non-toy algorithm we've developed.
			Gradient-based methods are most commonly thought of as a procedure
			 for optimizing a global objective,
			 but this analysis provides a different perspective:
			 that of doing full minimization of linearized losses with quadratic regularization.
			The minimization viewpoint opens way to many other algorithms,
			 all of which can be thought of as using different regularizers or using better approximations
			 of the loss functions, while maintaining efficient parameter updates.
		\exampleHeading{perceptron}{Online SVM}
			Let us use our result on OGD to derive a regret bound for learning SVMs in an online manner.
			We will just apply OGD on the hinge loss for classification ($x_t \in \R^d, y_t \in \{+1,-1\}$):
			 \eqn{f_t(w) = \max \{ 0, 1 - y_t (w \cdot x_t) \}.}
			 The algorithm (assume $S = \R^d$, so we don't need to project):
				If $y_t (w_t \cdot x_t) \ge 1$ (classify correctly with margin 1): do nothing.\footnote{This algorithm is very similar to the Perceptron algorithm; the only difference is that Perceptron just requires any positive margin, not necessarily 1.}
				Else: $w_{t+1} = w_t + \eta y_t x_t$.
			Analysis:
				Assume the data points are bounded: $\|x_t\|_2 \le L$.
				 Then $z_t \in \partial f_t(w_t)$ also satisfies that bound $\|z_t\|_2 \le L$.
				Assume that expert weights are bounded: $\|u\|_2 \le B$.
				The regret bound from \refthm{FTRL} is as follows:
				 \eqn{\boxed{\Regret \le B L \sqrt{T}.}}
		\exampleHeading{quadraticExpertAdvice}{Learning with expert advice}
			Now let us consider learning with expert advice.
				We maintain a distribution $w_t \in \Delta_d$ over $d$ experts
				 and predict by sampling an expert from that distribution.
				Assume the zero-one loss: $\ell(y_t, p_t) = \1[y_t \neq p_t]$.
				The loss function is linear: $f_t(w_t) = w_t \cdot z_t$,
				 where \eqn{z_t = [\ell(y_t, h_1(x_t)), \cdots, \ell(y_t, h_d(x_t))] \in \{0,1\}^d} is the loss vector.
			Bound on set of experts ($B$):
			 the experts live in the simplex $S = \Delta_d$,
			 which has its $2$-norm bounded by $B = 1$ (attained at a corner).
			Bound on loss gradient ($L$):
				The Lipschitz constant is bounded by the norm of the gradient $z_t$,
				 which is at most $\sqrt{d}$.
				Therefore, the regret bound we get is
				 \eqn{\boxed{\Regret \le B L \sqrt{T} = \sqrt{d T}.}}
			Note that we are depending on the square root of the number of experts $d$
			 rather than the log of the number of experts in our first online learning bound for learning with expert advice.
			 Can we obtain a $\log d$ dependence without assuming realizability?
			 We will find out in the next section.
	Online mirror descent (OMD) \currlecture
		!tmpformat N
		So far, we have analyzed FTRL for quadratic regularizers, which leads to
		 (lazy projected) gradient-based algorithms.
		 Quadratic regularization is imposing a certain prior knowledge, namely that there is a good parameter vector $w$ in a small $L_2$ ball.
		 But perhaps we know some dimensions to be more important than others.
		 Then we might want to use a non-spherical regularizer.
		 Or in the case of learning with expert advice,
		 we know that $w \in \Delta_d$ (a probability distribution),
		 so negative entropy might be more appropriate.
		 In this section, we will develop a general way of obtaining
		 regret bounds for general regularizers,
		 and make explicit the glorious role that strong convexity plays.
		 We make make extensive use of \word{Fenchel duality} and \word{Bregman divergences}.
		The goal for this lecture is to analyze FTRL (\refalg{FTRL}) for arbitrary
		 convex loss functions and regularizers.
		 The resulting algorithm is this:
		\algHeading{mirrorDescent}{\word{online mirror descent (OMD)}}
			Let $\psi : \R^d \to \R$ be the regularizer (this defines the learner).
			Let $f_1, \dots, f_T$ be the sequence of loss functions played by nature.
			On each iteration $t = 1, \dots, T$, the learner chooses weights $w_t$ to minimize the regularized (linearized) loss:
			 \eqnl{omd}{w_t \in \arg\min_{w \in \R^d} \{ \psi(w) - w \cdot \theta_t \},}
			 where
			 $z_t \in \partial f_t(w_t)$ is the $t$-th subgradient,
			 and $\theta_t = -\sum_{i=1}^{t-1} z_i$ is the negative sum of the
			 first $t-1$ subgradients.
		Technical note:
		 we will let the domain of weight vectors be unconstrained ($S = \R^d$).
		 We can always fold a constraint into the regularizer
		 by setting $\psi(w) = \infty$ if $w$ violates the constraint.
		To recap the terminology:
			OMD on $f_t$ is equivalent to FTRL on the linearizations
			 $w \mapsto w \cdot z_t$.
			OGD is OMD with the quadratic regularizer $\psi(w) = \frac{1}{2\eta} \|w\|_2^2$
		Examples of regularizers:
			Quadratic regularizer: $\psi(w) = \frac{1}{2 \eta} \|w\|_2^2$.
			Non-spherical quadratic regularizer: $\psi(w) = \frac{1}{2 \eta} w^\top A w$ for $A \succeq 0$.
			Entropic regularizer: $\psi(w) = \inv{\eta} \sum_{j=1}^d w_j \log w_j$ if $w \in \Delta_d$
			 (this is the negative entropy defined on the probability simplex),
			 and $\infty$ otherwise.
			Note: the difference between the two regularizers is that the entropic regularizer slopes up violently
			 when $w$ approaches the boundary of the simplex $\Delta_d$
			 (the function value is finite but the gradient goes to infinity).
		We now need to build up some tools to help us analyze OMD.
		 First, we introduce Fenchel duality, an important tool in optimization:
			\definitionHeading{fenchel}{Fenchel conjugate}
				The \word{Fenchel conjugate}\footnote{Also known as the convex conjugate or Legendre-Fenchel transformation.} of
				 a function (not necessarily convex) $\psi : \R^d \to \R$ is
				 \eqn{\boxed{\psi^*(\theta) \eqdef \sup_{w \in \R^d} \{ w \cdot \theta - \psi(w) \}.}}
			Intuition
				For scalars $w, \theta \in \R$,
				 given a fixed $\theta$ (interpreted as a slope),
				 $-\psi^*(\theta)$ is the position where the supporting hyperplane of $\psi$ with slope $\theta$ hits the vertical axis.
					FIGURE: [draw $\psi$]
				One can think of sweeping $\theta$ and reading out $\psi^*(\theta)$;
				 this information in some sense encodes the epigraph of $\psi$ if $\psi$ is convex.
			Useful facts:
				$\psi^*$ is always convex (even if $\psi$ is not),
				 since it's just a supremum over a collection of linear functions
				 $\theta \mapsto w \cdot \theta - \psi(w)$.
				$\psi^*(\theta) \ge w \cdot \theta - \psi(w)$ for all $w \in \R^d$
				 (\word{Fenchel-Young inequality}).
				 This follows directly from the definition of $\psi^*$.
				If $r(w) = a \psi(w)$ with $a > 0$, then $r^*(\theta) = a \psi^*(\theta/a)$.
				 This is useful because once we've computed the Fenchel conjugate for one
				 function, we know it for different scalings.  In fact,
				 Fenchel duality has a very nice algebra that makes computing
				 Fenchel conjugates modular.
				$\psi^{**} = \psi$ iff $\psi$ is convex (and lower semi-continuous).
				 In this case, we have
				 \eqn{\psi(w) = \psi^{**}(w) = \sup_{\theta \in \R^d} \{ w \cdot \theta - \psi^*(\theta) \}.}
				If $\psi$ is differentiable,
				 then
				 \eqn{\nabla \psi^*(\theta) = \arg\max_{w \in \R^d} \{ w \cdot \theta - \psi(w) \}.}
				 This is because the gradient of the supremum of a collection of functions
				 at $\theta$ is the gradient of the function that attains the max at $\theta$.
			Mirroring
				Comparing this with the the OMD update \refeqn{omd},
				 we see that $w_t = \nabla \psi^*(\theta_t)$,
				 and $-\psi^*(\theta_t)$ is the corresponding value of the regularized loss.
				Since $w_t$ attains the supremum of the Fenchel conjugate $\psi^*$,
				 the optimality conditions (differentiate with respect to $w$)
				 tell us that $\theta_t = \nabla \psi(w_t)$.
				We have a one-to-one mapping between weights $w$ and negative cumulative subgradients $\theta$,
				 linked via the gradients of $\psi$ and $\psi^*$,
				 which are inverses of one another:
				 \eqnl{omdFenchel}{\boxed{w_t = \nabla \psi^*(\theta_t), \quad \theta_t = \nabla \psi(w_t).}}
				This provides a very elegant view of what online mirror descent is doing.
				 OMD takes a series of gradient updates $\theta_{t+1} = \theta_t - z_t$,
				 generating $\theta_1, \theta_2, \dots, \theta_T$.
				 These steps are \emph{mirrored} via Fenchel duality in
				 the sequence $w_1, w_2, \dots, w_T$.
				FIGURE: [mapping]
			\exampleHeading{quadraticFenchel}{Quadratic regularizer}
				Let $\psi(w) = \frac{1}{2 \eta} \|w\|_2^2$.
				Then $\psi^*(\theta) = \frac{\eta}{2} \|\theta\|_2^2$,
				 attained by $w = \nabla \psi^*(\theta) = \eta \theta$.
				In this case, $w$ and $\theta$ are simple rescalings of each other.
			\exampleHeading{entropyFenchel}{Entropic regularizer}
				Let $\psi(w) = \inv{\eta} \sum_{j=1}^d w_j \log w_j$ for $w \in \Delta_d$ (negative entropy).
				Then $\psi^*(\theta) = \inv{\eta} \log \left(\sum_{j=1}^d e^{\eta \theta_j} \right)$ (log partition function),
				 attained by $w_j = \nabla \psi^*(\theta) = \frac{e^{\eta \theta_j}}{\sum_{k=1}^d e^{\eta \theta_k}}$.
				Aside: this is maximum entropy duality in exponential families,
				 where $w$ and $\theta$ represent the mean and canonical parametrization of a multinomial distribution.
	!verbatim \lecture{15}
	Regret bounds with Bregman divergences \currlecture
		Motivation
			Having reinterpreted OMD as a mapping using a conjugate function,
			 let's turn to proving a regret bound.
			 Recall that in order to prove bounds, we needed to ensure that $w_t$ and $w_{t+1}$ don't change too much
			 according to some criteria.
			For quadratic regularization, this criteria was based on Euclidean distance.
			We now generalize this by using Bregman divergences, which generalizes
			 the notion of distance based on the regularizer.
			\definitionHeading{bregman}{Bregman divergence}
				Let $f$ be a continuously-differentiable convex function.
				The \word{Bregman divergence} between $w$ and $u$ is the difference at $w$ between $f$ and a linear approximation around $u$:
				 \eqn{\boxed{D_f(w \| u) \eqdef f(w) - f(u) - \nabla f(u) \cdot (w - u).}}
				Intuitively, the divergence captures the the error of the linear approximation of $f$ based on the gradient $\nabla f(u)$.
				FIGURE: [show gap between $f$ and its linear approximation]
			Property: $D_f(w \| u) \ge 0$ (by definition of convexity).
			Note: Bregman divergences are not symmetric and therefore not a distance metric.
			\exampleHeading{quadraticBregman}{Quadratic regularizer}
				Let $f(w) = \frac{1}{2} \|w\|_2^2$.
				Then $D_f(w \| u) = \inv{2} \|w - u\|_2^2$ (squared Euclidean distance).
			\exampleHeading{entropyBregman}{Entropic regularizer}
				Let $f(w) = \sum_{j=1}^d w_j \log w_j$ for $w \in \Delta_d$ (negative entropy).
				Then $D_f(w \| u) = \KL{w}{u} = \sum_j w_j \log (w_j / u_j)$ for $w \in \Delta_d$ (KL divergence).
			Property (scaling): $D_{a f}(w \| u) = a D_f(w \| u)$.
		\theoremHeading{omd}{regret of OMD using Bregman divergences}
			OMD (\refalg{mirrorDescent}) obtains the following regret bound:
			 \eqnl{OMDregret}{\boxed{\Regret(u) \le [\psi(u) - \psi(w_1)] + \sum_{t=1}^T D_{\psi^*}(\theta_{t+1} \| \theta_t).}}
			Furthermore, if all $f_t$'s are linear,
			 the inequality is actually an equality if $u$ is the best expert.
			 So this bound is pretty air tight.
		Proof of \refthm{omd}:
			We assume all the loss functions are linear;
			 recall in our analysis of OGD that
			 linear functions are the worst case.
			The key step is to find a potential function
			 which allows us to monitor progress.
			 The pivoting quantity is $\psi^*(\theta_{T+1})$,
			 the negative regularized loss of the best fixed expert.
			 This will allow us to relate the learner ($w_t$) to the expert ($u$).
			Recall:
				Learner's loss is $\sum_{t=1}^T w_t \cdot z_t$.
				Expert's loss is $\sum_{t=1}^T u \cdot z_t$.
				The regret is the difference between the two.
			The expert:
			 \eqn{\psi^*(\theta_{T+1})
			 &\ge u \cdot \theta_{T+1} - \psi(u) \\
			 &= \sum_{t=1}^T [-u \cdot z_t] - \psi(u)}
			 by the Fenchel-Young inequality.
			 Note that we have equality if $u$ is the best expert,
			 by definition of $\psi^*$.
			The learner:
			 \eqn{\psi^*(\theta_{T+1})
			 &= \psi^*(\theta_1) + \sum_{t=1}^T [\psi^*(\theta_{t+1}) - \psi^*(\theta_t)] \aside{telescoping sums} \\
			 &= \psi^*(\theta_1) + \sum_{t=1}^T [\nabla \psi^*(\theta_t) \cdot (\theta_{t+1} - \theta_t) + D_{\psi^*}(\theta_{t+1} \| \theta_t)] \aside{Bregman definition} \\
			 &= \psi^*(\theta_1) + \sum_{t=1}^T [-w_t \cdot z_t + D_{\psi^*}(\theta_{t+1} \| \theta_t)] \aside{definition of OMD \refeqn{omdFenchel} and $\theta_t$}.}
			 Note that $\psi^*(\theta_1) = -\psi(w_1)$ since $\theta_1 = 0$.
			Combining last equations for the expert and learner and rearranging yields the result.
			The regret bound \refeqn{OMDregret} is a generalization of \refeqn{FTRLregret},
			 where $\psi(w) = \frac{1}{2 \eta} \|w\|_2^2$ is quadratic regularizer,
			 and $D_{\psi^*}(\theta_{t+1} \| \theta_t) = \frac{\eta}{2} \|z_t\|_2^2$.
			However, a general Bregman divergence usually doesn't have such a nice form,
			 and thus it is useful to bound it using a nicer quantity:
			 a norm of some kind.
	Strong convexity and smoothness \currlecture
		First, let's review some intuition behind norms.
			$L_p$ norms decrease as $p$ increases:
			 \eqn{\|w\|_1 \ge \|w\|_2 \ge \|w\|_\infty.}
			!comment $L_p$ balls increase in size as $p$ increases: \eqn{\{ w : \|w\|_1 \le B \} \subseteq \{ w : \|w\|_2 \le B \} \subseteq \{ w : \|w\|_\infty \le B \}.}
			The difference between the norms can be huge.
			 For example, take $w = (1, \dots, 1) \in \R^d$.
			 Then $\|w\|_1 = d$, $\|w\|_2 = \sqrt{d}$, $\|w\|_\infty = 1$.
			Recall that the dual norm of a norm $\|\cdot\|$ is
			 \eqn{\|x\|_* = \sup_{\|y\| \le 1} (x \cdot y).}
			 Thinking of $y$ as a linear operator on $x$,
			 the dual norm measures the gain when we measure perturbations in $x$ using $\|\cdot\|$.
			The norms $\|\cdot\|_p$ and $\|\cdot\|_q$ are dual to each other when $\inv{p} + \inv{q} = 1$.
			 Two common examples are:
			 \eqn{\|\cdot\|_1 \quad\text{is dual to}\quad \|\cdot\|_\infty.}
			 \eqn{\|\cdot\|_2 \quad\text{is dual to}\quad \|\cdot\|_2.}
		We will now use these squared norms to control Bregman divergences
		 provided the Bregman divergences have an important special structure called \emph{strong convexity}:
		\definitionHeading{stronglyConvex}{strong convexity/smoothness}
			A function $f$ is $\alpha$-strongly convex with respect to a norm $\|\cdot\|$ iff for all $w,u$:
			 \eqn{D_f(w \| u) \ge \frac{\alpha}{2} \|w - u\|^2.}
			A function $f$ is $\alpha$-strongly smooth with respect to a norm $\|\cdot\|$ iff for all $w,u$:
			 \eqn{D_f(w \| u) \le \frac{\alpha}{2} \|w - u\|^2.}
		Intuitions
			Strong convexity means that $f$ must be growing at least quadratically.
			Strong smoothness means that $f$ must be growing slower than some quadratic function.
			Example: the quadratic regularizer $\psi(w) = \frac{1}{2} \|w\|_2^2$ is
			 both $1$-strongly convex
			 and $1$-strongly smooth
			 with respect to the $L_2$ norm,
			 since $D_\psi(w \| u) = \frac{1}{2} \|w - u\|_2^2$.
		Duality links strong convexity and strong smoothness, as the following result shows.
		\lemmaHeading{convexSmooth}{strong convexity and strong smoothness}
			The following two statements are equivalent:
				$\psi(w)$ is $1/\eta$-strongly convex with respect to a norm $\|\cdot\|$.
				$\psi^*(\theta)$ is $\eta$-strongly smooth with respect to the dual norm $\|\cdot\|_*$.
			Sanity check
			 the quadratic regularizer:
			 $\psi(w) = \frac{1}{2\eta} \|w\|_2^2$
			 and $\psi^*(\theta) = \frac{\eta}{2} \|\theta\|_2^2$.
		With these tools, we can finally make some progress on our regret bound from \refthm{omd},
		 rewriting the Bregman divergences in terms of norms.
		\theoremHeading{omdNorms}{regret of OMD using norms}
			Suppose $\psi$ is a $\frac{1}{\eta}$-strongly convex regularizer.
			Then the regret of online mirror descent is
			 \eqnl{OMDregret2}{\boxed{\Regret(u) \le [\psi(u) - \psi(w_1)] + \frac{\eta}{2} \sum_{t=1}^T \|z_t\|_*^2.}}
		Proof of \refthm{omdNorms}
			By \reflem{convexSmooth},
			 since $\psi$ is $1/\eta$-strongly convex,
			 the Fenchel conjugate $\psi^*$ is $\eta$-strongly smooth.
			By definition of strong smoothness and the fact that $\theta_{t+1} = \theta_t - z_t$,
			 \eqnl{bregmanDualNorm}{\boxed{D_{\psi^*}(\theta_{t+1} \| \theta_t) \le \frac{\eta}{2} \|z_t\|_*^2.}}
			Plugging this bound into \refeqn{OMDregret} gives us the result.
		Remarks:
			If we plug in the quadratic regularizer $\psi(w) = \frac{1}{2\eta} \|w\|_2^2$,
			 then we get back the original result \refeqn{FTRLregret}.
			However, \refeqn{OMDregret2} generalizes to other regularizers.
			We get to measure the size of $z_t$ using the dual norm $\|\cdot\|_*$ rather than the default $L_2$ norm,
			 which will help us get tighter bounds for the learning from expert advice problem.
		Learning form expert advice
			Let's now use our tools to improve the regret bound that we got for learning from expert advice.
			Recall that when we used the quadratic regularizer,
			 we got a regret bound of $\sqrt{d T}$ because $\|z_t\|_2$ could be as big as $\sqrt{d}$.
			To reduce this, let's just use another norm: $\|z_t\|_\infty \le 1$;
			 no dependence on $d$!
			But this means that the regularizer $\psi$ has to be strongly convex with respect to the $L_1$ norm,
			 but this is harder because $\|\cdot\|_1 \ge \|\cdot\|_2$.
			Failure:
			 the quadratic regularizer $\psi(w) = \frac{1}{2 \eta} \|w\|_2^2$ is
			 only $\frac{1}{\eta d}$-strongly convex with respect to the $L_1$ norm:
			 \eqn{D_\psi(w \| u) \ge \frac{1}{2 \eta} \|w - u\|_2^2 \ge \frac{1}{2 \eta d} \|w - u\|_1^2.}
			 Sanity check:
			 $\|(1, \dots, 1)\|_2^2 = d$ but
			 $\|(1, \dots, 1)\|_1^2 = d^2$.
			 So if we try to use this in the regret bound, we get $\frac{1}{2 \eta} + \frac{T \eta d}{2}$,
			 which is still $\sqrt{d T}$ (for optimal $\eta$).
			Failure: the $L_1$ regularizer $\psi(w) = \|w\|_1$ is
			 neither strongly convex nor strongly smooth with respect to the any norm
			 for any $\alpha$: it doesn't grow fast enough for large $w$ and grows too fast for small $w$.
			Success: entropic regularizer
			 $\psi(w) = \frac{1}{\eta} \sum_j w_j \log w_j$ for $w \in \Delta_d$
			 is $1/\eta$-strongly convex with respect to the $L_1$ norm.
			 This requires some algebra and an application of Cauchy-Schwartz
			 (see Example 2.5 in Shai Shalev-Shwartz's online learning tutorial).
			FIGURE: [compare quadratic and entropic regularizer for $d=2$]
		\exampleHeading{exponentiatedGradient}{exponentiated gradient (EG)}
			Entropic regularizer: $\psi(w) = \inv{\eta} \sum_{j=1}^d w_j \log w_j$ for $w \in \Delta_d$.
			Recall $\psi^*(\theta) = \inv{\eta} \log \sum_{j=1}^d e^{\eta \theta_j}$ and $\nabla \psi_j^*(\theta) = \frac{e^{\eta \theta_j}}{\sum_{k=1}^d e^{\eta \theta_k}}$.
			OMD updates:
			 \eqn{\boxed{w_{t,j} \propto e^{\eta \theta_{t,j}}.}}
			 The equivalent recursive formula:
			 \eqn{\boxed{w_{t+1,j} \propto w_{t,j} e^{-\eta z_{t,j}}.}}
			 Interpretation: we maintain a distribution over the $d$ experts.
			 We use the gradient $z_t$ to reweight the experts, normalizing afterwards to ensure a proper distribution.
			Recap: the exponentiated gradient (EG) algorithm
			 is just online mirror descent using the entropic regularizer.
		\exampleHeading{EG}{EG for learning with expert advice}
			Consider learning with expert advice (\refex{expertAdvice}).
			We use the expected zero-one loss:
			 $f_t(w) = w \cdot z_t$, where $z_t$ is the loss vector.
			We have that the dual norm of the gradients are bounded $\|z_t\|_\infty \le 1$.
			The minimum and maximum values of the regularizer:
				$\max_w \psi(w) = 0$ (minimum entropy)
				$\min_w \psi(w) = \frac{\log (1/d)}{\eta} = \frac{-\log d}{\eta}$ (maximum entropy)
			Then
			 \eqn{\Regret = \frac{\log(d)}{\eta} + \frac{\eta T}{2}.}
			Setting $\eta = \sqrt{\frac{2 \log d}{T}}$ yields:
			 \eqnl{EG}{\boxed{\Regret = \sqrt{2 \log(d) T}.}}
		To compare with quadratic regularization (OGD):
			Quadratic: $[\max_u \psi(u) - \min_u \psi(u)] \le \frac{1}{2 \eta}$, $\|z_t\|_2 \le \sqrt{d}$
			Entropic: $[\max_u \psi(u) - \min_u \psi(u)] \le \frac{\log d}{\eta}$, $\|z_t\|_\infty \le 1$
		Discussion
			Online mirror descent (OMD) allows us to use different regularizers based on our prior knowledge about the expert vector $u$ and the data $z_t$.
			 As we see with EG, tailoring the regularizer can lead to better bounds.
			!comment In OGD, we were just assuming the weights lived in a ball $\|u\|_2 \le 1$ (this is too loose by a factor of $\sqrt{d}$ because we know $u$ lives in the simplex, which is than the $L_2$ ball).
			Using the $L_2$ norm means that we use the bound $\|z_t\|_2 \le \sqrt{d}$.
			 To get rid of the dependence on $d$ here, we use the $L_\infty$ norm
			 with $\|z_t\|_\infty \le 1$.
			However, this means that $\psi$ must be strongly convex with respect to the $L_1$ norm.
			 The quadratic regularizer isn't strong enough
			 (only $\frac{1}{\eta d}$-strongly convex with respect to $L_2$),
			 so we need the entropic regularizer, which is $1$-strongly convex with respect to $L_1$.
			Curvature hurts us because $[\psi(u) - \psi(w_1)]$ is now larger,
			 but the simplex is small, so $[\psi(u) - \psi(w_1)]$
			 only grows from $1$ (with the quadratic regularizer) to $\log d$ (with the entropic regularizer).
			So the tradeoff was definitely to our advantage.
	Local norms \currlecture
		Recall that that the general online mirror descent (OMD) analysis (\refthm{omd}) yields:
		 \eqn{\Regret(u) \le [\psi(u) - \psi(w_1)] + \sum_{t=1}^T D_{\psi^*}(\theta_{t+1} \| \theta_t).}
		 Using the fact that $\psi$ is $1/\eta$-strongly convex with respect to some norm $\|\cdot\|$,
		 we can upper bound the Bregman divergence by the following \refeqn{OMDregret2}:
		 \eqnl{Dzbound}{D_{\psi^*}(\theta_{t+1} \| \theta_t) \le \frac{\eta}{2} \|z_t\|_*^2.}
		If we use the entropic regularizer for $\psi$
		 with norm $\|\cdot\| = \|\cdot\|_1$ and dual norm $\|\cdot\|_* = \|\cdot\|_\infty$,
		 we get our key $\sqrt{2 \log(d) T}$ regret bound for EG.
		In this section, we will use the local norms technique to improve \refeqn{Dzbound}.
		 This will allow us to do two things:
			Recover the strong $O(\log d)$ bound for the realizable setting.
			Allow us to analyze the multi-armed bandit setting.
		Why we should do better:
			Consider an example where there are two experts:
			 one which is perfect ($z_{t,1} = 0$ for all $t$)
			 and one which is horrible ($z_{t,2} = 1$ for all $t$).
			The EG algorithm will quickly downweight the bad expert exponentially fast:
			 \eqn{
			 w_{t,1} \propto 1 \quad\quad
			 w_{t,2} &\propto e^{-\eta (t-1)}.
			 }
			So as $t \to \infty$, we have basically put all our weight on the first expert,
			 and should be suffering no loss.
			But $\|z_t\|_\infty^2 = 1$, so in the regret bound we still pay $\frac{\eta T}{2}$,
			 which is simply a travesty.
			We would hope that $\|z_t\|_\infty^2 = \max_{1 \le j \le d} z_{t,j}^2$
			 should be replaced with something that is sensitive to how much
			 weight we're placing on it, which is $w_{t,j}$.
			 Indeed the following theorem fulfills this dream:
		\theoremHeading{egLocalNorms}{exponentiated gradient (analysis using local norms)}
			Assume nature plays a sequence of linear loss functions $f_t(w) = w \cdot z_t$,
			 where $z_{t,j} \ge 0$ for all $t = 1, \dots, T$ and $j = 1, \dots, d$.
			Then the exponentiated gradient (EG) algorithm (\refex{EG})
			 achieves the following regret bound:
			 \eqnl{egLocalNorms}{\boxed{\Regret(u) \le [\psi(u) - \psi(w_1)] + \eta \blue{\sum_{t=1}^T \sum_{j=1}^d w_{t,j} z_{t,j}^2}.}}
		This bound \refeqn{egLocalNorms} is better than the bound
		 in \refthm{omdNorms} because we are taking an average
		 (with respect to the distribution $w_t \in \Delta_d$) instead of a max:
		 \eqn{\sum_{j=1}^d w_{t,j} z_{t,j}^2 \le \max_{1 \le j \le d} z_{t,j}^2 \eqdef \|z_t\|_\infty^2.}
		 This allows some components of the loss subgradients $z_{t,j}$ to be large
		 provided that the corresponding weights $w_{t,j}$ are small.
		\exampleHeading{egRealizable}{exponentiated gradient in the realizable setting}
			Assume the loss vector is bounded: $z_t \in [0,1]^d$.
			Assume there exists an expert $u \in \Delta_d$ with zero cumulative loss (realizability).
			Recall the regret bound from using local norms:
			 \eqn{\Regret(u)
			 &\eqdef \sum_{t=1}^T (w_t \cdot z_t) - \underbrace{\sum_{t=1}^T (u \cdot z_t)}_{= 0} \le \frac{\log d}{\eta} + \eta \sum_{t=1}^T \underbrace{\sum_{j=1}^d w_{t,j} z_{t,j}^2}_{\le w_t \cdot z_t}}.
			Note that $\sum_{j=1}^d w_{t,j} z_{t,j}^2 \le w_t \cdot z_t$, because all quantities are non-negative
			 and $a^2 \le a$ for $a \in [0,1]$.
			Rearranging, we get:
			 \eqn{\Regret(u) \le \frac{\log d}{\eta (1-\eta)}.}
			Minimize the bound with $\eta = \frac12$ to obtain:
			 \eqn{\boxed{\Regret(u) \le 4 \log d.}}
			Recall that the majority algorithm (which aggressively zeros out the weights of components as soon as they err)
			 also obtained the very low $O(\log d)$ regret (see \refex{majorityExample}),
			 so it's really nice to see that EG obtains the same regret guarantee.
			If the problem isn't realizable,
			 the majority algorithm isn't even correct (it will eliminate all the experts),
			 whereas EG will gracefully fall back to $O(\sqrt{\log(d) T})$ regret.
		Now that you are hopefully somewhat convinced that the theorem is useful,
		 let's prove it.
		Proof of \refthm{egLocalNorms}:
			This proof mostly involves starting with the Bregman divergence,
			 and performing some low-level algebraic manipulation.
			 Perhaps the most useful high-level take-away is whenever
			 we're trying to massage some expression with $\log$'s and $\exp$'s,
			 it's useful to try to approximate the functions using linear and quadratic
			 approximations (think Taylor approximations).
			Specifically, we will use the following two facts:
				Fact 1: $e^{-a} \le 1 - a + a^2$ for $a \ge 0$
				 (this actually holds for smaller $a$, but let's keep it simple)
				Fact 2: $\log (1 - a) \le -a$
			Recall the Fenchel conjugate of the entropic regularizer is the log-partition function:
			 \eqn{\psi^*(\theta) = \frac1\eta \log \sum_{j=1}^d e^{\eta \theta_j}.}
			By the definition of Bregman divergences (this was used in the proof of \refthm{omd}), we have:
			 \eqn{D_{\psi^*}(\theta_{t+1} \| \theta_t) = \psi^*(\theta_{t+1}) - \psi^*(\theta_t) - \underbrace{\nabla \psi^*(\theta_t)}_{w_t} \cdot \underbrace{(\theta_{t+1} - \theta_t)}_{-z_t}.}
			The rest is just applying the two facts and watching stuff cancel:
			 \eqn{D_{\psi^*}(\theta_{t+1} \| \theta_t)
			 &= \psi^*(\theta_{t+1}) - \psi^*(\theta_t) + w_t \cdot z_t \\
			 &= \frac1\eta \log \p{\frac{\sum_{j=1}^d e^{\eta \theta_{t+1,j}}}{\sum_{j=1}^d e^{\eta \theta_{t,j}}}} + w_t \cdot z_t \aside{definition of $\psi^*$} \\
			 &= \frac1\eta \log \p{\sum_{j=1}^d w_{t,j} e^{-\eta z_{t,j}}} + w_t \cdot z_t \aside{definition of $\theta_t$} \\
			 &\le \frac1\eta \log \p{\sum_{j=1}^d w_{t,j} [1 - (\eta z_{t,j} - \eta^2 z_{t,j}^2)]} + w_t \cdot z_t \aside{fact 1} \\
			 &= \frac1\eta \log \p{1 - \sum_{j=1}^d w_{t,j} (\eta z_{t,j} - \eta^2 z_{t,j}^2)} + w_t \cdot z_t \aside{$w_t \in \Delta_d$} \\
			 &\le \frac1\eta \sum_{j=1}^d w_{t,j} (-\eta z_{t,j} + \eta^2 z_{t,j}^2) + w_t \cdot z_t \aside{fact 2} \\
			 &= \eta \sum_{j=1}^d w_{t,j} z_{t,j}^2 \aside{algebra}.
			 }
	!verbatim \lecture{16}
	Adaptive optimistic mirror descent \currlecture
		Motivation
			Recall that the local norm bound technique for exponentiated gradient (EG) yields the following bound \refeqn{egLocalNorms}:
			 \eqnl{egOldRegret}{\Regret(u) \le \frac{\log d}{\eta} + \eta \sum_{t=1}^T \sum_{j=1}^d \red{w_{t,j}} z_{t,j}^2,}
			 improving the $\|z_t\|_\infty^2$ from the standard mirror descent analysis (\refex{EG})
			 to the local norm $\|z_t\|_{\diag(w_t)}^2$.
			 (Note that this was just an improvement in the analysis, not the algorithm.)
			But there are two weaknesses of this regret bound.
			 To see the first weakness, consider the following example.
			!comment \exampleHeading{egBestExpertGapOne}{bad expert}
				We have two experts:
					Expert 1 is perfect ($z_{t,1} = 0$).
					Expert 2 is really bad ($z_{t,2} = 1$).
				The EG algorithm plays $w_t \propto [1, \exp(-\eta(t-1))]$.
				The regret is $\sum_{t=1}^T \frac{1}{1 + \exp(\eta(t-1))} \ge \frac{1}{1 + e}$ for $t \le \frac{1}{\eta}$.
			\exampleHeading{egBestExpertGapConfused}{perfect and confused expert}
				We have two experts:
					Expert 1 is perfect ($z_{t,1} = 0$).
					Expert 2 is just confused and alternates between loss of $-1$ and $+1$ ($z_{t,2} = (-1)^{t-1}$).
				Playing either expert 1 or expert 2 alone is optimal,
				 yielding a cumulative loss of $0$ (for expert 2, for even $T$).
				 But expert 2 has higher variance and is thus intuitively riskier,
				 so maybe we should avoid it.
				However, the EG algorithm does not:
				 \eqn{
				 w_t \propto \begin{cases}
				 [1, 1] & \text{if $t$ is odd} \\
				 [1, \exp(-\eta)] & \text{if $t$ is even}.
				 \end{cases}
				 }
				 EG doesn't penalize expert 2 on the odd rounds at all and barely penalizes it on the even rounds,
				 because expert 2 redeems itself on the even rounds.
				On this example, assuming the step size $\eta \le 1$,
				 EG incurs a regret (not just a regret bound) of:
				 \eqn{\text{EG regret}
				 &= \frac{T}{2} \p{\half (1) + \frac{\exp(-\eta)}{1 + \exp(-\eta)} (-1)} \\
				 &= \frac{T}{2} \p{\half - \frac{1}{1 + \exp(\eta)}} \\
				 &\ge \frac{T}{2} \p{\half - \frac{1}{2 + 2 \eta}} \aside{$\exp(\eta) \le 1 + 2 \eta$ for $\eta \le 1$} \\
				 &\ge \frac{T}{2} \eta.
				 }
				Typically, $\eta = \Theta(\frac{1}{\sqrt{T}})$, in which case, the regret is $\Omega(\sqrt{T})$.
				If $\eta$ is too large, then EG ends up swinging back and forth between the two experts,
				 and incurring a lot of regret, just as in the FTL example with linear loss functions (\refex{linearOptimization}).
				 On the other hand, $\eta$ can't be too small either, or else we don't end up being able to switch
				 to a good expert quickly enough (consider the example where expert 2 gets a loss of $1$ always).
			The second weakness is that if we add $5$ to the losses of all the experts on all $T$ iterations,
			 then the regret bound suffers something like an extra $25 \eta T$.
			 But morally, the problem hasn't changed!
			In what follows, we will produce two improvements to the EG algorithm.
			 There are two key ideas:
				\word{Adaptivity}: instead of using a fixed regularizer $\psi$, we will use a regularizer $\psi_t$ that depends on the current iteration.
				 This way, we can adapt the regularizer to the loss functions that we've seen so far.
				\word{Optimism}: we will incorporate hints $m_1, \dots, m_T$, which are a guess of the actual loss sequence $z_1, \dots, z_T$.
				 By incorporating these hints, our algorithm can do much better when the hints are correct,
				 but still be robust enough to perform well when the hints are not correct.
				 As a concrete example, think of $m_t = z_{t-1}$ (a recency bias),
				 which hints that the subgradients aren't changing very much.
			 Using these two ideas, we define an algorithm called \textbf{adaptive optimistic mirror descent (AOMD)},
			 which obtain the following regret bound for learning with expert advice (for $\eta \le \frac{1}{4}$):
			 \eqnl{egNewRegret}{\Regret(u) \le \frac{\log d}{\eta} + \eta \sum_{t=1}^T \sum_{j=1}^d \red{u_{j}} \green{(z_{t,j} - m_{t,j})}^2.}
			 Compared to \refeqn{egOldRegret}, \refeqn{egNewRegret} depends on the expert $u_j$ rather than the learner $w_{t,j}$;
			 and it also depends on the squared errors $(z_{t,j} - m_{t,j})^2$ rather than $z_{t,j}^2$.
			 Unlike local norms, this requires an actual change to the algorithm---a change which is inspired by the theory.
		Recall that the normal mirror descent updates are given as follows:
		 \eqn{w_t = \arg\min_{w \in \R^d} \pc{\psi(w) + \sum_{i=1}^{t-1} w \cdot z_i} = \nabla \psi^*(\theta_t).}
		Define the following \textbf{adaptive} mirror descent updates:
		 \eqn{w_t = \arg\min_{w \in \R^d} \pc{\psi_{\red{t}}(w) + \sum_{i=1}^{t-1} w \cdot z_i} = \nabla \psi^*_{\red{t}}(\theta_t),}
		 where all we've done is replaced $\psi$ with $\psi_t$.
		To put \textbf{optimism} in, suppose we have a sequence of \word{hints}
		 \eqn{m_1, \dots, m_T,}
		 which are guesses of $z_1, \dots, z_T$.
		 These have the property that $m_t$ only depends on $z_{1:t-1}$.
		 For example, the simplest one could be $m_t = z_{t-1}$ (recency bias),
		 which means that we believe the subgradients do not change very much.
		 Note that if $m_t = z_t$, then we would be cheating and using the one-step lookahead.
		 The \textbf{adaptive optimistic mirror descent} updates are as follows:
		 \eqn{\boxed{w_t = \arg\min_{w \in \R^d} \pc{\psi(w) + \sum_{i=1}^{t-1} w \cdot z_i + \red{w \cdot m_t}} = \nabla \psi^*(\tilde \theta_t),}}
		 where $\tilde \theta_t \eqdef \theta_t - \red{m_t}$ is the guess of $\theta_{t+1}$.
		\theoremHeading{optimisticRegret}{regret of adaptive optimistic mirror descent}
			Let $m_1, \dots, m_T$ be any sequence of hints.  Assume $m_1 = 0$.
			Suppose we have a sequence of regularizers $\psi_1, \dots, \psi_T$ satisfying the following loss-bounding property:
			 \eqnl{aomdLossBounding}{\psi^*_{t+1}(\theta_{t+1}) \le \psi^*_t(\tilde \theta_t) - w_t \cdot (z_t - m_t).}
			Then
			 %\eqn{\boxed{\Regret(u) \le \frac{\psi_{T+1}(u) - \psi(w_1)}{\eta}.}}
			 \eqn{\boxed{\Regret(u) \le \psi_{T+1}(u) - \psi(w_1).}}
		Note: in our previous mirror descent analysis, we had two terms, one coming from the regularizer,
		 and one coming from the stability of our estimates.
		 In contrast, \refeqn{aomdLossBounding} ``pushes the loss into the regularizer'',
		 which gives us a bit more flexibility.
		Proof of \refthm{optimisticRegret}:
		 \eqn{
		 u \cdot \theta_{T+1} - \psi_{T+1}(u)
		 &\le \psi_{T+1}^*(\theta_{T+1}) \aside{Fenchel-Young} \\
		 &= \psi^*_1(\theta_1) + \sum_{t=1}^T [\psi_{t+1}^*(\theta_{t+1}) - \psi_{t}^*(\theta_{t})] \aside{telescoping} \\
		 &= \psi^*_1(\theta_1) + \sum_{t=1}^T [\psi_{t+1}^*(\theta_{t+1}) - \psi_{t}^*(\tilde\theta_{t}) - \underbrace{\nabla \psi^*(\tilde\theta_t)}_{w_t} \cdot m_t] \aside{convexity of $\psi_{t}^*$} \\
		 &= \psi^*_1(\theta_1) + \sum_{t=1}^T -w_t \cdot z_t \aside{loss-bounding property} \\
		 &= -\psi_1(w_1) + \sum_{t=1}^T -w_t \cdot z_t \aside{since $m_1 = 0$}.
		 }
		 Recall that $\theta_{T+1} = -\sum_{t=1}^T z_t$.
		 Algebra completes the proof.
		Now what kind of regularizer should we use to satisfy the loss-bounding property?
			First attempt: let's try just using a fixed regularizer $\psi_t(w) = \psi(w)$.
			 This means that we need
			 \eqn{\psi^*(\theta_{t+1}) \le \psi^*(\tilde \theta_t) - w_t \cdot (z_t - m_t).}
			 But in fact, since $\psi^*$ is convex, and recalling $w_t = \nabla \psi^*(\tilde \theta_t)$
			 and $\theta_{t+1} - \tilde\theta_t = -(z_t - m_t)$
			 we actually have:
			 \eqn{\psi^*(\theta_{t+1}) \ge \psi^*(\tilde \theta_t) - w_t \cdot (z_t - m_t),}
			 which is the opposite of what we want!
			FIGURE: [graph of $\psi^*$ $\theta_t, \tilde\theta_t, \theta_{t+1}$]
			Second attempt: let's try to provide a second-order correction:
			 \eqn{\psi^*(\theta_{t+1} - \red{\eta (z_t - m_t)^2}) \le \psi^*(\tilde \theta_t) - w_t \cdot (z_t - m_t).}
			 The $\eta (z_t - m_t)^2$ term makes the LHS smaller.
			 However, condition doesn't look like the loss-bounding precondition we need for \refthm{optimisticRegret}.
			 To make the condition match,
			 the key is to \emph{fold the second-order term into the regularizer}.
			To that end, define the vector $a_t$ to be the squared magnitude deviations between $z_t$ and $m_t$:
			 \eqn{a_t \eqdef (a_{t,1}, \dots, a_{t,d}), \quad \boxed{a_{t,j} \eqdef (z_{t,j} - m_{t,j})^2.}}
			 This motivates us to consider the following adaptive regularizer:
			 \eqnl{adaptiveReg}{\boxed{\psi_t(w) \eqdef \psi(w) + \eta \sum_{i=1}^{t-1} w \cdot a_i.}}
			 In the expert advice setting where $w$ is a distribution over experts,
			 we are penalizing experts that have large deviations from the hints, which is in line
			 with our intuitions that we want experts which match our hints (kind of like our prior).
			Algorithmically, AOMD with this choice of $\psi_t$ is:
			 \eqn{w_t
			 &= \arg\min_w \pc{\psi(w) + \eta \sum_{i=1}^{t-1} w \cdot a_i - w \cdot \tilde\theta_t} \\
			 &= \arg\min_w \pc{\psi(w) - w \cdot \tilde\beta_t} \\
			 &= \nabla \psi^*(\tilde \beta_t),
			 }
			 where we define the second-corrected vectors (without and with $m_t$-lookahead):
			 \eqn{\beta_t \eqdef \theta_t - \eta \sum_{i=1}^{t-1} a_i, \quad \tilde \beta_t \eqdef \beta_t - m_t.}
			 Recall that $\tilde\theta_t = \theta_t - m_t$.
		\theoremHeading{optimisticRegretCorrections}{second-order corrections}
			Consider running AOMD using the adaptive regularizer $\psi_t$ defined in \refeqn{adaptiveReg}.
			Assume the second-order-corrected loss-bounding property holds:
			 \eqn{\psi^*(\beta_{t+1}) \le \psi^*(\tilde \beta_t) - w_t \cdot (z_t - m_t).}
			Then we have the following regret bound:
			 \eqn{\boxed{\Regret(u) \le \psi(u) - \psi(w_1) + \green{\eta \sum_{t=1}^T u \cdot a_t}.}}
		Proof of \refthm{optimisticRegretCorrections}
			We need to translate the second-order-corrected loss-bounding property on $\psi$
			 the loss-bounding property on $\psi_t$.
				Since
				 $\psi_t(w) = \psi(w) + w \cdot (\eta \sum_{i=1}^{t-1} a_i)$ by definition,
				 Fenchel conjugacy yields:
				 \eqn{\psi_t^*(x) = \psi^*\p{x - \eta \sum_{i=1}^{t-1} a_i}.}
				 In other words, $\psi_t^*$ is just $\psi^*$ shifted by the second-order corrections.
				Therefore,
				 \eqn{\psi^*_{t+1}(\theta_{t+1}) = \psi^*(\beta_{t+1}), \quad, \psi^*_{t}(\tilde\theta_{t}) = \psi^*(\tilde\beta_{t}).}
			Note that $\beta_1 = 0$, so $\psi^*(\beta_1) = -\psi(w_1)$.
			Then:
			 \eqn{\Regret(u)
			 &\le \psi_{T+1}(u) - \psi(w_1) \aside{from \refthm{optimisticRegret}} \\
			 &\le \psi(u) + \eta \sum_{t=1}^T u \cdot a_t - \psi(w_1) \aside{definition of $\psi_t$}.
			 }
		Now let us apply this result to the expert advice setting.
		\theoremHeading{egOptimistic}{adaptive optimistic gradient descent (expert advice)}
			Consider $\|z_t\|_\infty \le 1$ and $\|m_t\|_\infty \le 1$.
			Let $\psi(w) = \frac{1}{\eta} \sum_{j=1}^d w_j \log w_j$ be the standard entropic regularizer,
			 which corresponds to the following algorithm:
			 \eqn{w_{t,j} \propto \exp(\tilde\beta_{t,j}) = \exp\p{\eta \theta_{t,j} - \underbrace{\eta m_{t,j}}_\text{optimism} - \underbrace{\eta^2 \sum_{i=1}^{t-1} (z_{i,j} - m_{i,j})^2}_\text{adaptivity}}.}
			Assume $0 < \eta \le \frac{1}{4}$.
			Then adaptive optimistic mirror descent (AOMD) obtains the following regret bound:
			 \eqn{\boxed{\Regret(u) \le \frac{\log d}{\eta} + \eta \sum_{t=1}^T \sum_{j=1}^d u_j (z_{t,j} - m_{t,j})^2.}}
		Proof sketch: do algebra on $\psi^*(\beta_{t+1})$ to show the condition of \refthm{optimisticRegretCorrections};
		 see \citet{steinhardt2014eg}.
		\exampleHeading{egPathLength}{path length bound}
			Set $m_t = z_{t-1}$ be the average subgradients in the past.
			Algorithmically, this corresponds to counting the last $z_{t-1}$ twice in addition to penalizing deviations.
			FIGURE: [plot expert paths over time]
			We obtain
			 \eqn{\Regret \le \frac{\log d}{\eta} + \eta \sum_{t=1}^T (z_{t,j^*} - z_{t-1,j^*})^2,}
			 where $j^* \in [d]$ is best expert (the one with the lowest cumulative loss).
			On \refex{egBestExpertGapConfused},
			 expert 1 is a best expert (obtains cumulative loss $0$).
			 We have that the path length for expert 1 is $(z_{t,1} - z_{t-1,1})^2 = 0$,
			 so we obtain:
			 \eqn{\Regret \le \frac{\log d}{\eta}.}
			 The bound only holds for $\eta \le \frac{1}{4}$.
			 By setting $\eta = \frac{1}{4}$, we that the regret is a mere constant:
			 \eqn{\Regret \le 4 \log d.}
			Note that unlike EG (which had $\Omega(\sqrt{T})$ regret),
			 we don't get distracted by expert 2, who has very low loss as well,
			 but has longer path length and is thus discounted.
		At a high-level, what we're doing with the hints $m_t$ is penalizing experts
		 whose losses $z_{t,j}$ don't agree with our ``prior'' $m_{t,j}$.
		 Of course, the hints have only a secondary $\Theta(\eta)$ effect,
		 whereas the actual losses have a primary $\Theta(1)$ effect.
		This section is mostly based on \citet{steinhardt2014eg},
		 who combines optimistic mirror descent \citep{rakhlin2013online}
		 and adaptive mirror descent \citep{orabona2015generalized}.
	Online-to-batch conversion \currlecture
		So far, we have been focusing on the \textbf{online} setting,
		 where the learner receives one example at a time and is asked to make a prediction on each one.
		 Good online learners have low \textbf{regret}.
		Sometimes it is more natural to operate in the \textbf{batch} setting,
		 where the learner gets a set of training examples, learns a model,
		 and then is asked to predict on new test examples.
		 Good batch learners have low \textbf{expected risk}.
		In this section, we will show that low regret implies low expected risk
		 by explicitly reducing the online setting to the batch setting.
		Batch setting
			Assume we have a unknown data-generating distribution $p^*(z)$, where we use $z = (x,y) \in \sZ$ to denote an input-output pair.
			Assume our hypothesis class is a convex set of weight vectors $S \subseteq \R^d$.
			As in online learning, we define a convex loss function $\ell(z, w) \in \R$;
			 for example, the squared loss for linear regression would be $\ell((x,y), w) = (w \cdot x - y)^2$.
			 Assume $\ell(z,w)$ is convex in $w$ for each $z \in \sZ$.
			The \word{expected risk} of a weight vector $w \in S$ is defined as follows:
			 \eqn{\boxed{L(w) = \E_{z \sim p^*}[\ell(z,w)].}}
			Define the weight vector that minimizes the expected risk:
			 \eqn{w^* \in \arg\min_{w \in S} L(w).}
			The batch learner gets $T$ i.i.d. training examples
			 $(z_1, \dots, z_T)$, where each $z_t \sim p^*$.
			 The goal is to output some estimate $w \in S$ that hopefully has low $L(w)$.
			!comment Define the empirical risk of a weight vector $w \in S$ as follows: \eqn{\hat L(w) = \inv{T} \sum_{t=1}^T \ell(z_t,w).}
		 Assuming we have an online learner as a black box, we will construct a batch learner as follows:
		\algHeading{onlineToBatch}{online-to-batch conversion}
			Input: $T$ training examples $z_1, \dots, z_T$.
			Iterate $t = 1, \dots, T$:
				Receive $w_t$ from the online learner.
				Send loss function $f_t(w) = \ell(z_t, w)$ to the online learner.
			Return the average of the weight vectors: $\bar w = \inv{T} \sum_{t=1}^T w_t$.
		Remarks about randomness
			In contrast to the online learning (adversarial) setting,
			 many of the quantities we are working with now have distributions associated with them.
			For example, $f_t$ is a random function that depends on $z_t$.
			Each $w_t$ is a random variable which depends on (i.e., is in the sigma-algebra of) $z_{1:t-1}$.
			Note that $L(w^*)$ is not random.
		\theoremHeading{onlineToBatch}{Online-to-batch conversion}
			Recall the usual definition of regret (which depends on $z_{1:T}$):
			 \eqn{\Regret(u) = \sum_{t=1}^T [f_t(w_t) - f_t(u)].}
			Online-to-batch conversion obtains the following expected expected risk:
			 \eqn{\boxed{\E[L(\bar w)] \le L(w^*) + \frac{\E[\Regret(w^*)]}{T}.}}
		Interpretation: the expected expected risk of the online-to-batch conversion $L(\bar w)$
		 is bounded by the best possible expected risk (attained by $w^* \in S$)
		 plus the online regret.
		Note that $\E[L(\bar w)]$ has two expectations here:
		 the $\E[\cdot]$ is an expectation over possible training datasets,
		 and $L$ contains an expectation over test examples.
		Proof:
			The first key insight is that $f_t(w_t)$ provides an unbiased estimate of the expected risk of $w_t$.
			 \eqnl{onlineBatchUnbiased}{\E[f_t(w_t) \mid w_t] = L(w_t).}
			 This is true because all the examples are independent,
			 so $w_t$ (deterministic function of $z_{1:t-1}$) is independent of $f_t$ (deterministic function of $z_t$).
			The second key insight is that averaging can only reduce loss.
			 Since $\ell(z,\cdot)$ is convex, and an average of convex functions is convex, $L$ is convex.
			 By Jensen's inequality:
			 \eqnl{onlineBatchConvexity}{L(\bar w) \le \inv{T} \sum_{t=1}^T L(w_t).}
			Now, the rest is just putting the pieces together.
			 Putting \refeqns{onlineBatchConvexity}{onlineBatchUnbiased} together:
			 \eqn{L(\bar w) \le \inv{T} \sum_{t=1}^T \E[f_t(w_t) \mid w_t].}
			 Adding and subtracting $L(w^*)$ to the RHS, noting that $L(w^*) = \E[f_t(w^*)]$:
			 \eqn{L(\bar w) \le L(w^*) + \inv{T} \sum_{t=1}^T (\E[f_t(w_t) \mid w_t] - \E[f_t(w^*)]).}
			Taking expectations on both sides:
			 \eqn{\E[L(\bar w)] \le L(w^*) + \inv{T} \sum_{t=1}^T (\E[f_t(w_t) - f_t(w^*)]).}
			The second term of the RHS is upper bounded by the regret, so we have:
			 \eqn{\E[L(\bar w)] \le L(w^*) + \frac{\E[\Regret(w^*)]}{T}.}
		Remarks
			If you run online subgradient descent once over your training data,\footnote{In practice,
			 it helps to run the online learning algorithm multiple times over the training data.}
			 you should expect $O(\sqrt{T})$ regret by our previous analyses.
			The resulting average weight vector will, \emph{in expectation},\footnote{You can get high probability bounds too using martingales,
			 but we won't discuss those here.}
			 have an expected risk which is within $O(1/\sqrt{T})$
			 of the best weight vector.
			If the batch learner returns the last weight vector $w_{T+1}$,
			 then our analysis above doesn't apply.
			In general, averaging is a useful concept for stabilizing learning algorithms.
	Adversarial bandits: expert advice \currlecture
		In online learning with expert advice,
		 on each iteration, after we choose one of the $d$ experts/actions,
		 nature reveals the loss vector $z_t$ for every single action.
		However, in many applications such as clinical trials or advertisement placement,
		 packet routing, we only get to observe the loss of the action we took,
		 not the losses of the actions we didn't take.
		This setting is known as the multi-armed bandit problem,
		 which is a type of online learning problem with \word{partial feedback}.
		This problem is much more difficult.
		 Intuitively, the learner should choose actions that we expect to yield low loss,
		 but it must choose which actions to explore to get more information about the losses.
		 Thus, the multi-armed bandit problem
		 exposes the challenges of the classic exploration/exploitation tradeoff.\footnote{
		 Reinforcement learning requires managing the exploration/exploitation tradeoff,
		 but is even more difficult because actions take the learner
		 between different states in a way that is unknown to the learner.}
		Setup
			FIGURE: [matrix with loss functions]
			There are $d$ possible actions (corresponding to the arms of a row of slot machines).
			Let $z_t \in [0,1]^d$ denote the vector of losses of the $d$ actions at iteration $t$.
			For each iteration $t = 1, \dots, T$:
				Learner chooses a distribution $w_t \in \Delta_d$ over actions,
				 and samples an action $a_t \sim w_t$.
				Learner observes the loss of \emph{only that particular action} $\blue{z_{t,a_t}}$ and no others.
			The expected regret with respect to an expert $u \in \Delta_d$
			 is defined in the same way as it was for online learning with expert advice:
			 \eqn{\E[\Regret(u)] = \sum_{t=1}^T [w_t \cdot z_t - u \cdot z_t].}
			 Note that taking the max over randomized experts $u \in \Delta_d$ is equivalent to
			 taking the max over single actions $a \in [d]$, since the maximum over a linear function
			 is attained at one of the vertices.
			!comment \eqn{\Regret = \max_{a \in [d]} \sum_{t=1}^T [z_{t,a_t} - z_{t,a}].}
		Estimating the loss vector
			Recall that in online learning, we would observe the entire loss vector $z_t$.
			 In that case, we could use the exponentiated gradient (EG) algorithm
			 ($w_{t+1,j} \propto w_{t,j} e^{-\eta z_{t,j}}$)
			 to obtain a regret bound of $\Regret \le \sqrt{2 \log(d) T}$ (see \refex{EG}).
			In the bandit setting, we don't observe $z_t$, so what can we do?
			 Let's try to estimate it with some $\hat z_t$ that (i) we can observe
			 and (ii) is equal to $z_t$ in expectation (unbiased) in that for all $a = 1, \dots, d$:
			 \eqn{\E_{a_t \sim w_t}[\hat z_{t,a} \mid w_t] = z_{t,a}.}
			Given these two constraints, it's not hard to see that the only choice for $\hat z_t$ is:
			 \eqn{\boxed{\hat z_{t,a} = \begin{cases} \frac{z_{t,a}}{w_{t,a}} & \text{if } a = a_t \\ 0 & \text{otherwise}. \end{cases}}}
			 Note that dividing $w_{t,a}$ compensates for sampling $a_t \sim w_t$.
		\algHeading{banditEG}{Bandit-EG}
			Run EG with the unbiased esimate $\hat z_t$ rather than $z_t$.
			 Really, that's it.
		\theoremHeading{banditEG}{Bandit-EG analysis}
			Bandit-EG obtains expected regret (expectation taken over learner's randomness) of
			 \eqnl{banditEG}{\boxed{\E[\Regret(u)] \le 2 \sqrt{\blue{d} \log(d) T}.}}
		Comparison of Bandit-EG (bandit setting) and EG (online learning setting):
			Compared with the bound for EG in the full information online learning setting \refeqn{EG},
			 we see that this bound \refeqn{banditEG} is worse by a factor of $\sqrt{d}$.
			In other words, the number of iterations $T$ for Bandit-EG needs to be $d$ times larger
			 in order to obtain the same average regret as EG.
			This is intuitive since in the bandit setting, each iteration reveals $(1/d)$-th the amount of information compared with the online learning setting.
		Proof of \refthm{banditEG}:
			If EG is run with the unbiased estimate $\hat z_t$ ($z_t = \E[\hat z_t \mid w_t]$),
			 then the expected regret is simply:
			 \eqn{\E[\Regret(u)] \le \frac{\log d}{\eta} + \eta \sum_{t=1}^T \E_{a_t \sim w_t}\left[\sum_{a=1}^d w_{t,a} \blue{\hat z_{t,a}^2} \mid w_t\right].}
			 Note that the random variable $\hat z_t$ only depends on the previous actions through the random variable $w_t$.
			So now, we just have to compute the expected local norm:
			 \eqn{\E_{a_t \sim w_t}\left[\sum_{a=1}^d w_{t,a} \hat z_{t,a}^2 \mid w_t \right] = \sum_{a=1}^d w_{t,a}^2 \p{\frac{z_{t,a}^2}{w_{t,a}^2}} \le \blue{d},}
			 since $z_t \in [0, 1]^d$.
			Note that it is crucial that we use local norms here; the generic bound \refeqn{OMDregret2}
			 of $\|z_t\|_\infty^2$ is not good enough because $\|z_t\|_\infty$ is unbounded.
			Minimizing the regret bound with respect to $\eta$,
			 we get $2 \sqrt{d \log (d) T}$ with $\eta = \sqrt{\log(d) / (d T)}$.
		Note that we have only gotten results in expectation (over randomness of the learner $a_t \sim w_t$).
		 To get high probability results, we also need to control the variance of $\hat z_t$.
		 To do this, we modify the EG algorithm by
		 smoothing $w_t$ with the uniform distribution over actions.
		 This results in the standard Exp3 algorithm.
	Adversarial bandits: online gradient descent \currlecture
		In the online convex optimization setting,
		 we are presented with a sequence of functions $f_1, \dots, f_T$.
		 Importantly, we could compute subgradients $z_t \in \partial f_t(w_t)$,
		 which allowed us to update our weight vector using online gradient descent (OGD).
		 In the bandit setting, suppose we can only query function values of $f_t$.
		 Can we still minimize regret?
		Intuitively, something should be possible.
		 In one dimension, the gradient of $f_t$ at a point $w$ can be approximated by
		 $\frac{f_t(w + \delta) - f_t(w - \delta)}{2 \delta}$.
		 But this requires two function evaluations, which is not permitted in the bandit setting:
		 you only get to administer one treatment to a patient, not two!
		 Can we do it with one function evaluation?  Can we generalize to high dimensions?
		 The answer to both questions is yes.
		 The original idea is from \citet{flaxman2005online};
		 we follow the exposition in \citet{shalev2011online}.
		For a function $f$, define a smoothed version of $f$ as follows:
		 \eqn{\tilde f(w) = \E_{v \sim \Uball}[f(w + \delta v)],}
		 where $\Uball$ is the uniform distribution over the unit ball
		 (the set of vectors $v$ with $\|v\|_2 = 1$).
		 There are two important properties:
			We can compute the gradient of $\tilde f$ (this is the key step):
			 \eqn{\boxed{\nabla \tilde f(w) = \E_{v \sim \Usphere}\pb{\frac{d}{\delta} f(w + \delta v) v}.}}
			 This is due to Stokes' theorem, but the intuition can be seen in 1D due to the Fundamental Theorem of calculus:
			 \eqn{\tilde f(w) = \E[f(w + \delta v)] = \frac{\int_{-\delta}^\delta f(w + t) dt}{2 \delta} = \frac{F(w + \delta) - F(w - \delta)}{2 \delta},}
			 where $F$ is the antiderivative of $f$ and thus
			 \eqn{\tilde f'(w) = \frac{f(w + \delta) - f(w - \delta)}{2 \delta}.}
			 In the general setting,
			 if we draw $v \sim \Usphere$ from the unit sphere,
			 then $\frac{d}{\delta} f(w + \delta v) v$ is an unbiased estimate of $\nabla \tilde f(w)$.
			We have that $\tilde f$ well approximates $f$:
			 \eqn{|\tilde f(w) - f(w)| \le |\E[f(w + \delta v) - f(w)]| \le L \delta,}
			 where $L$ is the Lipschitz constant of $f$.
		 Now we are ready to define the algorithm:
		\algHeading{banditOGD}{Bandit-OGD}
			For $t = 1, \dots, T$:
				Set $w_t = \arg\min_{w \in S} \|w - \eta \theta_t\|_2$ (same as OGD)
				Pick random noise $v_t \sim \Usphere$
				Predict $\tilde w_t = w_t + \delta v_t$
				Compute $z_t = \frac{d}{\delta} f_t(\tilde w_t) v_t$
				Update $\theta_{t+1} = \theta_t - z_t$
		\theoremHeading{bogdRegret}{regret bound bandit online gradient descent}
			Let $f_1, \dots, f_T$ be a sequence of convex functions.
			Let $B = \max_{u \in S} \|u\|_2$ be the magnitude of an expert.
			Let $F = \max_{u \in S, t \in [T]} f_t(u)$ be the maximum function value.
			Then Bandit-OGD obtains the following regret bound:
			 \eqn{\E[\Regret(u)]
			 & = \E\pb{\sum_{t=1}^T [f_t(\tilde w_t) - f_t(u)]} \\
			 &\le 3 L \delta T + \frac{B^2}{2 \eta} + \frac{\eta}{2} T d^2 (F/\delta + L)^2,}
			 where the expectation is over the learner's random choice (nature is still adversarial).
		Let us set $\delta$ and $\eta$ to optimize the bound.
			First, we optimize the step size $\eta$:
			 setting $\eta = \frac{B}{d (F/\delta + L) \sqrt{T}}$ yields a regret bound of
			 \eqn{3 L \delta T + B d (F/\delta + L) \sqrt{T}.}
			Second, we optimize the amount of smoothing $\delta$:
			 setting $\delta = \sqrt{\frac{B d F}{3 L}} T^{-1/4}$ yields
			 a final regret bound of
			 \eqn{\E[\Regret]
			 &\le \sqrt{12 B d F L T^{3/4} + B d L \sqrt{T}} \\
			 & = \boxed{O(\sqrt{B d F L} T^{3/4}),}}
			 where in the last equality, we're thinking of the dependence on $T$ as primary.
			Note that our regret bound of $O(T^{3/4})$ has a worse dependence than the $O(T^{1/4})$ that
			 we're used to seeing.
			 This comes from the fact that we're only getting an unbiased estimate of the gradient
			 of the a \emph{smoothed function} $\hat f$, not $f$.
			 And as usual, we have a $\sqrt{d}$ dependence on the dimension since we are in the bandit
			 setting and only get $1/d$-the the information of a full gradient.
		Proof of \refthm{bogdRegret}
			Our analysis of plain OGD \refthm{FTRL} yields the following regret bound
			 when applied to $w_t$ and linear functions $x \mapsto x \cdot z_t$:
			 \eqn{\sum_{t=1}^T [w_t \cdot z_t - u \cdot z_t] \le \frac{1}{2 \eta} \|u\|_2^2 + \frac{\eta}{2} \sum_{t=1}^T \|z_t\|_2^2.}
			First, taking expectations on both sides,
			 using the fact that $\tilde f_t$ is convex with $\nabla \tilde f_t(w_t) = \E[z_t]$ for the LHS,
			 expanding the definition of $z_t$ on the RHS:
			 \eqnl{bogdRegretStepTwo}{\sum_{t=1}^T \E[\tilde f_t(w_t) - \tilde f_t(u)] \le \frac{1}{2 \eta} \|u\|_2^2 + \frac{\eta}{2} \sum_{t=1}^T \E[\|(d/\delta) f_t(\tilde w_t) v_t\|_2^2].}
			Second, let's handle the gradient:
			 Since $f_t$ is $L$-Lipschitz,
			 we have that 
			 \eqn{f_t(\tilde w_t) \le f_t(w_t) + L \delta \le F + L \delta.}
			 Plugging this inequality into \refeqn{bogdRegretStepTwo}:
			 \eqnl{bogdRegretStepThree}{\sum_{t=1}^T \E[\tilde f_t(w_t) - \tilde f_t(u)]
			 \le \frac{B^2}{2 \eta} + \frac{\eta}{2} \frac{d^2}{\delta^2} (F + L \delta)^2.}
			Finally, we need to relate $\tilde f_t(w_t)$ to $f_t(\tilde w_t)$.
			 Using the Lipschitz property of $f_t$, we have:
			 \eqn{f_t(\tilde w_t) - f_t(u)
			 \le f_t(w_t) - f_t(u) + L \delta
			 \le \tilde f_t(w_t) - \tilde f_t(u) + 3 L \delta.}
			 Plugging this into \refeqn{bogdRegretStepThree} completes the proof.
	Stochastic bandits: upper confidence bound (UCB) \currlecture
		Let us return to the learning from expert advice setting where at each iteration,
		 we are choosing an arm $a_t \sim w_t$ and suffering loss $z_{t,a_t}$.
		 So far, we have assumed that the losses $z_t$ were adversarial.
		 Now, let us now consider the \word{stochastic setting},
		 where $z_t$'s are drawn from an (unknown) distribution.
		 By leveraging the probabilistic structure of the problem,
		 we'll show in this section that we can improve the regret bounds
		 from $O(\sqrt{T})$ to $O(\log T)$.
		We will show an algorithm called upper confidence bound (UCB)
		 \citep{lai1985asymptotically,auer2002finite}
		 that is based on the principle of \emph{optimism in the face of uncertainty}:
		 we maintain upper bounds on the reward of each action and choose the action that maximizes
		 the best possible payoff.
		 The idea is that we either get very low loss (which is great),
		 or we learn something.
		Let us define the notation for the stochastic bandits setting:
			Let $r_1, \dots, r_T$ be an i.i.d.~ sequence of reward vectors,\footnote{In the stochastic bandits literature,
			 we typically maximize reward rather than minimize loss, so think of $r_t = -z_t$.}
			 where each $r_t \in [0, 1]^d$
			Define $\mu \eqdef \E[r_t]$ to be the average rewards ($\mu$ doesn't depend on $t$).
			Define $j^* \eqdef \arg\max_{j=1}^d \mu_j$ be the best action.
			Define $\Delta_j \eqdef \mu_{j^*} - \mu_{j} > 0$
			 be the gap between action $j$ and the best action $j^*$.
			 Let $\Delta \eqdef \min_{j \neq j^*} \Delta_j$ the smallest gap.
			Let $a_1, \dots, a_T \in [d]$ be the sequence of actions taken by the learner.
			Let $S_j(t) \eqdef \{ i \in [t] : a_i = j \}$ be the set of time steps $i$ up to time $t$ where we took action $j$.
			Let $N_j(t) = |S_j(t)|$ be the number of times we took action $j$.
			We are interested in minimizing the expected regret:
			 \eqnl{stochasticRegret}{\E[\Regret]
			 &= \sum_{t=1}^T \E[r_{t,j^*} - r_{t,a_t}] \\
			 &= \sum_{j=1}^d \E[N_j(T)] \Delta_j,
			 }
			 where in the last inequality, we've rewritten the regret to sum over all possible actions;
			 each of the $N_j(T)$ times we take action $j$, we add $\Delta_j$ to the regret.
			 Note that at each time step $t$,
			 $r_t$ is independent of $a_t$, which is a function of the past.
		Now let's design an algorithm for solving the stochastic bandits problem.
		 The way to think about it is that we're just estimating the mean $\mu$ in an online manner.
		 At each time step $t$, we have $N_j(t)$ i.i.d.~draws of action $j$, from which we can form
		 the empirical estimate
		 \eqn{\hat\mu_j(t) = \inv{|S_j(t)|} \sum_{i \in S_j(t)} r_{i,j}.}
		 Provided that $S_j(t)$ grows to infinity,
		 we can estimate the means: $\hat\mu_j(t) \cvP \mu_j$.
		The difference from mean estimation is that we are also trying to \emph{exploit}
		 and will choose actions $j$ that are more likely to have higher reward.
		By Hoeffding's inequality,
		 \eqn{\BP[\hat\mu_j(t) \le \mu_j - \epsilon] \le \exp\p{-2 N_j(t) \epsilon^2}.}
		 In other words, with probability at least $1-\delta$,
		 \eqn{\mu_j \le \hat\mu_j(t) + \sqrt{\frac{\log(1/\delta)}{2 N_j(t)}}.}
		 This motivates the following algorithm:
		\algHeading{ucb}{upper confidence bound (UCB)}
			Play each of the $d$ actions once.
			For each subsequent time step $t = d+1, \dots, T$:
				Choose action
				 \eqn{a_t = \arg\max_{1 \le j \le d} \pc{\hat\mu_j(t-1) + \sqrt{\frac{2 \log t}{N_j(t-1)}}},}
		\theoremHeading{ucb}{regret of upper confidence bound (UCB)}
			The UCB algorithm obtains the following expected regret bound:
			 \eqn{\boxed{\E[\Regret] \le \frac{8 d \log T}{\Delta} + 5 d.}}
		Proof sketch:
			The main idea is to upper bound the number of times an suboptimal action $j \neq j^*$ was taken.
			 This happens when the following event $A$ happens:
			 \eqn{\hat\mu_j + \epsilon_j \ge \hat\mu_{j^*} + \epsilon_{j^*}.}
			FIGURE: [$\hat\mu_j, \mu_j, \hat\mu_{j^*}, \mu_{j^*}$ on a line with $\Delta_j$ gap]
			Recall that the separation between $\mu_j$ and $\mu_{j^*}$ is $\Delta_j$.
			 If the actual values $\hat\mu_j$ and $\hat\mu_{j^*}$ are within $O(\Delta_j)$ of their means,
			 then we're all set.
			The probability of that not happening after $m$ rounds is $e^{-O(m \Delta^2)}$ as given by Hoeffding's inequality.
			 If we wait $m$ rounds and look at the expected number of times $j$ was chosen:
			 \eqn{m + \sum_{t=1}^T t^2 e^{-O(m \Delta_j^2)},}
			 where the $t^2$ comes from taking a sloppy union bound over pairs of time steps
			 (because we don't know which time steps we're comparing so just try all pairs).
			 If we set $m = O\p{\frac{\log T}{\Delta_j^2}}$, then we can make the second term be a constant.
			 Summing the first term over $j$ and multiplying $\Delta_j$ yields the result.
			!comment Skip
				The opposite event is:
				 \eqn{\underbrace_{\hat\mu_j}_{\le \mu_j + \epsilon_j} + \epsilon_j \le \underbrace{\hat\mu_{j^*} + \epsilon_{j^*}}_{\ge \mu_{j^*}}.}
				 So provided that the gap between action $j$ and $j^*$ is large ($\Delta_j \ge 2 \epsilon_j$),
				 then the bad event $A$ occuring must imply either that $j$ looked much better than it actually is
				 or $j^*$ looked a lot worse than it actually is:
				 \eqn{\BP[A] \le \BP[\hat\mu_j \ge \mu_j + \epsilon_j] + \BP[\hat\mu_{j^*} \le \mu_{j^*} - \epsilon_{j^*}].}
				 We need to take a union bound over all time steps twice, one for counting the number of times $A$ occurs
				 and once because we don't know it is true.
			Note that we're being pretty sloppy here.
			 For the full rigorous proof, see \citet{auer2002finite}.
		Interpretation: that we have a logarithmic dependence on $T$,
		 and a necessary linear dependence on the number of actions $d$.
		 There is also a dependence on $\Delta^{-1}$;
		 a smaller $\Delta$ means that it's harder to distinguish the best action from the others.
		!comment Proof of \refthm{ucb}
			Define
				$x_{j,n}$ be the empirical average over $n$ samples of action $j$,
				 so that $\hat\mu_j(t) \eqdistrib x_{j, N_j(t)}$.
				$c_{n,t} \eqdef \sqrt{\frac{2 \log t}{n}}$ to be the radius of the confidence region,
				 so that $a_t = \arg\max_j \hat\mu_j(t-1) + c_{N_j(t-1),t}$.
			If we look at the definition of regret \refeqn{stochasticRegret},
			 we see that it suffices to bound $\E[N_j(T)]$ for $j \neq j^*$,
			 the number of times that we're choosing a suboptimal action.
			First, write the number of times action $j$ was chosen:
			 \eqn{N_j(T)
			 &= 1 + \sum_{t=K}^T \1[a_t = j] \\
			 &\le m + \sum_{t=K}^T \1[a_t = j \wedge N_j(t-1) \ge m],
			 }
			 for any $m$, which we will choose later.
			Second, let us consider the event $a_t = j$.
			 This happens exactly when action $j$ looks better than every other action,
			 and implies that action $j$ looks better than the best action $j^*$:
			 \eqn{[a_t = j] \quad \Rightarrow \quad [\hat\mu_j(t) + c_{j,(t-1)} \ge \mu_{j^*}(t) + c_{j^*,t-1}].}
			 We can therefore continue the bound:
			 \eqn{N_j(T)
			 &\le m + \sum_{t=K}^T \1[\hat\mu_j(t) + c_j(t-1) \ge \mu_{j^*}(t) + c_{j^*}(t-1) \wedge N_j(t-1) \ge m] \\
			 &\le m + \sum_{t=K}^T \1[\max_{m \le s < t} \p{\hat\mu_j(s) + c_j(t-1)} \ge \min_{1 \le s' < t} \p{\mu_{j^*}(s') + c_{j^*}(t-1)} \wedge N_j(t-1) \ge m] \\
			 }
			 If the RHS happens, then at least one of the following must be true:
			 \eqn{
			 \hat\mu_{j^*}(t) & \le \mu_{j^*}(t) - c_{j^*}(t), \\
			 \hat\mu_j(t) & \ge \mu_{j^*}(t) + c_j(t), \\
			 \mu_{j^*}(t) & \le \mu_{j}(t) + 2 c_j(t). \\
			 }
			Third, we can bound each of the
	Stochastic bandits: Thompson sampling \currlecture
		In UCB, our principle was optimism in the face of uncertainty.
		 We now present an alternative method called Thompson sampling, which dates back to \citet{thompson1933likelihood}.
		 The principle is \emph{choose an action with probability proportional to it being optimal},
		 according to a Bayesian model of how the world works.
		Setup
			Prior $p(\theta)$ over parameters $\theta$
			Probability over rewards given actions: $p(r \mid \theta, a)$
		 The Thompson sampling principle leads to a simple, efficient algorithm:
		Algorithm
			For each time $t = 1, \dots, T$:
				Sample a model from the posterior: $\theta_t \sim p(\theta \mid a_{1:t-1}, r_{1:t-1})$
				Choose the best action under that model: $a_t = \arg\max_a p(r \mid \theta_t, a)$.
		Example for Bernoulli rewards
			Assume each reward $r_j \sim \text{Bernoulli}(\mu_j)$ (here the parameters $\theta$ are the means $\mu$).
			For each action $j = 1, \dots, d$
				Let the prior be $\theta_j \sim \text{Beta}(\alpha, \beta)$
				Keep track of the number of successes $s_j$ and failures $f_j$.
			For each time $t = 1, \dots, T$:
				Sample $\theta_j \sim \text{Beta}(\alpha + s_j, \beta + f_j)$ for each $j = 1, \dots, d$
				Choose $a_t = \arg\max_j \theta_j$ and observe reward $r_t$.
				If $r_t = 1$ then increment $s_{a_t}$; else increment $f_{a_t}$.
		 Despite its long history, Thompson sampling was not very popular until recently,
		 where rigorous regret bounds have appeared and people realize that it outperforms UCB in many settings.
		 \citet{agrawal2012analysis} prove the following:
		\theoremHeading{thompson}{regret for Thompson sampling}
			For any $\epsilon > 0$, the expected regret of Thompson sampling us upper bounded by:
			 \eqn{\E[\Regret] \le (1 + \epsilon) \sum_{j : \Delta_j > 0} \frac{\Delta_j \log T}{\KL{\mu_j}{\mu^*}} + O\p{\frac{d}{\epsilon^2}}.}
		!comment Thompson sampling is not perfect.  Here is an example (Ian Osband): suppose we are trying to estimate 
		One strength of Thompson sampling is that it's a principle that allows one to easily generalize the algorithm
		 to structured settings such as reinforcement learning.
		One weakness of Thompson sampling is that it does not choose actions that takes into account the value of information gained
		 (especially important for structured settings).
		 As an example (from Ian Osband), consider $\theta \in \{ e_1, \dots, e_d \}$ be an unknown indicator vector,
		 and suppose $a \in \{ 0, 1 \}^d$ and the reward is is $a \cdot \theta - 0.0001 \|a\|_1$.
		 The optimal strategy would be to do binary search to find $\theta$ in $O(\log d)$ time, but Thompson sampling would just
		 choose one $a$ at a time, which would take $O(d)$ time.
		 More expensive methods such as Gittins index performs the actual dynamic programming
		 and have better guarantees but are computationally more expensive.
	Summary \currlecture
		This concludes our tour of online learning and its extensions to the bandit setting.
		Our measure of success is getting low \textbf{regret},
		 the difference between the learner's cumulative losses and the best \emph{fixed} expert's cumulative losses.
		 In particular, we hope for sublinear regret: $\Regret = o(T)$.
		Without no additional assumptions (even with two experts),
		 any deterministic algorithm must have $\Regret = \Omega(T)$ (fail).
		In the realizable setting (some expert is perfect), the majority algorithm achieves $O(\log d)$ regret (constant in $T$).
		We started with the \textbf{follow the leader (FTL)}, and saw that it worked for quadratic loss functions ($\Regret = O(\log T)$),
		 but failed for linear loss functions ($\Regret = \Omega(T)$).
		Inspecting the regret bounds reveals that in order to get low regret, we need to have a \emph{stable} learner,
		 in the sense that $w_t$ and $w_{t+1}$ should be close (according to some notion of proximity).
		This motivated us to look at \textbf{follow the regularized leader (FTRL)}, where we add a quadratic regularizer,
		 which gave us a regret bound with two terms: (i) a bias-like term (value of regularizer applied to the best expert)
		 and (ii) a variance-like term (sum of the norm of the subgradients).
		 Balancing the two by controlling the amount of regularization (inverse step size)
		 yields $\Regret = O(\sqrt{T})$.
		If our loss functions were non-linear, we could linearize using the subgradient.
		 Coupled with a quadratic regularizer, we get the \textbf{online subgradient descent (OGD)} algorithm.
		 We also showed that it suffices to analyze linear functions,
		 since they result in the most regret.
		We looked at learning with expert advice, and got regret bounds of $O(\sqrt{d T})$, where $d$ is the number of experts.
		 Inspired by the logarithmic dependence on $d$ in the majority algorithm,
		 this motivated us to consider different regularizers.
		The general algorithm that deals with different regularizers is \textbf{online mirror descent (OMD)}.
		 We analyzed this algorithm using Fenchel duality and Bregman divergences, which allowed us
		 to look at arbitrary regularizers through the lens of the mapping between $w_t$ and $\theta_t$.
		 Specializing to learning with expert advice, we get a $O(\sqrt{\log(d) T})$ regret bound with the entropic regularizer,
		 resulting in the \textbf{exponentiated gradient (EG)} algorithm.
		By exploiting properties of exponentiated gradient,
		 we can perform a refined analysis again using \textbf{local norms} to achieve stronger
		 results, matching the $O(\log d)$ regret in the realizable setting.
		In the bandit setting with partial feedback, we get $O(\sqrt{d \log(d) T})$ regret
		 again using local norms to control the size of the now unbounded subgradients.
		 The general principle is to find an estimate of the loss vector $z_t$ and run existing online learning algorithms
		 (which are quite tolerant of noise).
		Finally, we showed that learners with low regret in the online setting directly lead to learners with low \textbf{expected risk}
		 in the batch setting via an online-to-batch conversion.
	References
		Shalev-Shwartz, 2012: Online Learning and Online Convex Optimization (survey paper)
			http://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf
			This is a nice introduction to online learning,
			 on which much of these online learning notes are based.
	!comment Not covered
		Section: Game theory viewpoint of online learning, connection to boosting
		Other references
			Shalev-Shwartz/Singer, 2008: Theory \& Applications of Online Learning (ICML tutorial slides)
				http://ttic.uchicago.edu/~shai/icml08tutorial/OLtutorial.pdf
				Develops regret bounds using duality.
			Kakade/Shalev-Shwartz, 2009: Mind the Duality Gap: Logarithmic regret algorithms for online optimization
				http://research.microsoft.com/en-us/um/people/skakade/papers/ml/mind_the_dual.pdf
			Bubeck/Cesa-Bianchi, 2012: Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems
				http://arxiv.org/pdf/1204.5721.pdf
			Abernethy/Agarwal/Bartlett/Rahklin, 2009: A Stochastic View of Optimal Regret through Minimax Duality
				http://arxiv.org/pdf/0903.5328v1.pdf
			Niu/Recht/Ré/Wright, 2011: Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent
				http://arxiv.org/pdf/1106.5730v2.pdf
			Kakade/Ng, 2004: Online Bounds for Bayesian Algorithms
				http://robotics.stanford.edu/~ang/papers/nips04-onlinebayesian.pdf
			Bottou/Bousquet, 2011: The Tradeoffs of Large Scale Learning
				http://leon.bottou.org/publications/pdf/mloptbook-2011.pdf

!verbatim \newpage
Neural networks (skipped in class)
	Motivation \currlecture
		One major motivating factor for studying neural networks
		 is that they have had a lot of empirical success and attention in recent years.
		 Here is the general recipe:
			Train on large amounts of data (Krizhevsky, 2012: 1 million examples).
			Use a very large neural network (Krizhevsky, 2012: 60 million parameters).
			Running simple stochastic gradient descent
			 (with some (important) tweaks such as step size control, sometimes momentum, dropout),
			 and wait a moderately long period of time (a week).
			Get state-of-the-art results across multiple domains.
				Object recognition (ImageNet): reduce error from 25.7\% to 17.0\% (Krizhevsky, 2012)
				Speech recognition (Switchboard): reduce error from 23\% to 13\% (Microsoft, 2009--2013)
			 Error reductions on these tasks/datasets are significant,
			 since these are large realistic datasets (unlike MNIST)
			 on which many serious people have tried to improve accuracy.
		However, in theory, neural networks are poorly understood:
			The objective function is non-convex.  SGD has no reason to work.
			What about the particular hypothesis class of neural networks makes
			 them perform well across so many tasks?  What types of functions are they good at representing?
		 This makes neural networks an important but challenging area to do new theoretical research.
		Compared to the theory of online learning, uniform convergence, or kernel methods,
		 the theory of neural networks (``why/when do they work?'') is spotty at best.
		 In this lecture, we will attempt lay out the high-level important questions and
		 provide preliminary thrusts towards answering them,
		 which in turn produces a series of more concrete open questions.
		A caveat: there are many theorems that one could prove about neural networks.
		 We will write some of these down.
		 However, most of these theorems only nibble off the corners of the problem,
		 and do not really answer the hard questions.
		 But that's where we are.
		 We will try to point out the interpretation of these results,
		 which is especially important to recognize in this prehistoric stage of understanding.
	Setup \currlecture
		Definition
			Let $x \in \R^d$ be an input.
			A neural network defines a non-linear function on $x$.
			 For concreteness, let us focus on two-layer neural networks (which 
			 means it has one hidden layer).
			Let $\sigma : \R \mapsto \R$ be a non-linear function (called an activation or transfer function).
			 Examples:
				Logistic: $z \mapsto \frac{1}{1 + e^{-z}}$ (maps real numbers monotonically to $[0,1]$)
				Hyperbolic tangent (tanh): $z \mapsto \frac{e^z - e^{-z}}{e^z + e^{-z}}$ (maps real numbers monotonically to $[-1,1]$)
				Rectified linear: $z \mapsto \max(0, z)$ (truncates negative numbers to zero)
			 We will extend $\sigma$ to operate on vectors elementwise:
			 \eqn{\sigma(x) \eqdef [\sigma(x_1), \dots, \sigma(x_d)].}
			An artificial \word{neural network}
			 (not to be confused with and completely different from the thing that's in your brain)
			 can be described by a function $f : \R^d \mapsto \R$
			 which has the following form:
			 \eqnl{neuralNetwork}{f_\theta(x) = \sum_{i=1}^m \alpha_i \sigma(w_i \cdot x + b_i).}
			 Here, the parameters are $\theta = (\alpha_{1:m}, w_{1:m}, b_{1:m})$.
			 In matrix notation:
			 \eqn{f_\theta(x) = \alpha \cdot \sigma(W x + b),}
			 where the $i$-th row of $W$ is $w_i \in \R^d$.
			A useful way to think about a neural network
			 is that the first layer $\sigma(Wx + b)$ computes some non-linear features
			 of the input and the second layer simply performs a linear combination.
			 So you can think of a neural network as parametrizing the features as well as the weights $\alpha$.
			FIGURE: [draw neural network]
		In practice, people use many layers (for example, in speech recognition, seven is not uncommon).
		 A three-layer neural network looks like this:
		 \eqn{f_\theta(x) = \sigma(W_2 \sigma(W_1 x + b_1) + b_2).}
		 People also use convolutional neural networks in which $W$ has additional low-dimensional structure.
		 Recurrent neural networks are good for representing time series.
		 We will not discuss these here and focus on the two-layer neural networks in this lecture.
		Let $\sF = \{ f_\theta \}$ be the class of all neural networks as we range over values of $\theta$.
		We can use $\sF$ for regression or classification in the usual way by minimizing
		 the loss on the training set.  For regression:
		 \eqn{\hat f_\text{ERM} = \arg\min_{f_\theta} \hat L(\theta), \quad \hat L(\theta) \eqdef \inv{n} \sum_{i=1}^n (f_\theta(x^{(i)}) - y^{(i)})^2.}
		The surprising thing about neural networks is that
		 people use stochastic gradient descent (online gradient descent where we choose $i \in \{1, \dots, n\}$ at each step randomly),
		 which converges to $\hat f_\text{SGD}$.
		 Since the objective function $\hat L$ is non-convex, $\fsgd$ will be different from $\ferm$.
		 so there is an additional \word{optimization error} in addition to the usual approximation and estimation errors.
		 The decomposition must be taken with a grain of salt,
		 because having suboptimal optimization effectively restricts the hypothesis class,
		 which actually improves estimation error (a simple example is early stopping).
		 So approximation and optimization error are perhaps difficult to tease apart in
		 methods whose success is tied to an algorithm not just an optimization problem.
		 \Fig{figures.slides/errorDecompOpt}{0.4}{errorDecompOpt}{Cartoon showing error decomposition into approximation, estimation, and optimization errors.}
	Approximation error (universality) \currlecture
		Question: which functions can be represented by two-layer neural networks?
		 Answer: basically all of them.
		We saw that Gaussian kernels were universal (\refthm{universalKernel})
		 in the sense that they are dense in $C_0(\sX)$, the set of all continuous bounded functions on $\sX$.
		Functions defined by (Gaussain) kernels can be represented in terms of the partial kernel evaluations
		 \eqn{f(x) = \sum_{i=1}^n \alpha_i k(x_i, x).}
		 or using Fourier features:
		 \eqn{f(x) = \sum_{i=1}^m \alpha_i e^{-i\inner{w_i}{x}}.}
		 Note that these expressions resemble neural networks \refeqn{neuralNetwork}.
		 All of these are linear combinations of some non-linear basis functions.
		 So we might expect that neural networks would be universal.
		 The following theorem answers affirmatively, which is not hard to show:
		\theoremHeading{nnUniversal}{neural networks are universal}
			Consider $\sX = [0,1]^d$.
			Then the class of two-layer neural networks $\sF$ is dense in $C_0(\sX)$ in the uniform metric:
			 for every $f^* \in C_0(\sX)$, and $\epsilon > 0$,
			 there exists an $f_\theta \in \sF$ such that
			 $\max_{x \in \sX} |f^*(x) - f_\theta(x)| \le \epsilon$.
		Proof of \refthm{nnUniversal}:
			Fix $f^* \in \sF$.  Because $f^*$ is continuous over a compact set ($\sX$),
			 it is uniformly continuous,
			 which means we can find a width $\delta$ such that $|x_1 - x_2| \le \delta$
			 implies $|f(x_1) - f(x_2)| \le \epsilon$.
			Grid $\sX$ into regions $R_j$ which are of the form:
			 $R_j = \{ x : |x - x_j|_\infty \le \delta \}$.
			FIGURE: [1D function]
			Note that $z \mapsto \sigma(C z)$ converges to the step function as $C \to \infty$.
			Let us construct $3d$ hidden units for each $R_j$.
			 For $d=1$, let $R_j = [x_j - \delta, x_j + \delta] = [a_j, b_j]$.
			 \eqn{f^*(x_j) \sigma(C (x - a_j)) + f^*(x_j) \sigma(C (b_j - x)) - f^*(x_j) \sigma(0).}
			 This function is approximately $f^*(x_j)$ on $R_j$ and zero elsewhere,
			 So we we do this for all regions, then we can approximate $f^*$ uniformly well.
		Note that the number of hidden units is exponential in $d$,
		 since the number of regions is $O((\frac{1}{\delta})^d)$.
		 This is just a glorified nearest neighbors.
		 So while this theorem holds,
		 it doesn't really provide much insight into why neural networks work well and generalize.
		Depth: One argument made in favor of deep neural networks is that they more compactly represent the data
		 compared to a shallower one.
		 To get intuition, let us think about networks that perform arithmetic operations
		 on the raw inputs via addition and multiplication.  Such networks define polynomial functions
		 in a compact way, since internal nodes represent factors.
		 For example, the polynomial
		 \eqn{f(x)
		 &= x_1 x_2^2 x_3 + x_1 x_2 x_3 x_4 + x_2^2 x_3^3 + x_2 x_3^3 x_4 \\
		 &= (a + b) (b + c),
		 }
		 where $a = x_1 x_2$, $b = x_2 x_3$, and $c = x_3 x_4$ correspond to internal nodes.
			!comment Results for boolean circuits
				Think about the neural networks as logical circuits
				 (interestingly, that was the first motivation for studying artificial neural networks in the 1940s).
				Positive result: All boolean functions involving at most $T(d)$ operations can be computed by a network
				 with depth $O(T(d))$ and size $O(T(d)^2)$.
				!comment Two-layer circuit of logic gates can represent any boolean function (DNF form)
				!comment d-bit parity circuits of depth 2 have size exponential in $d$ (Yao, 1985)
				Negative result:
				 any depth $q$ circuit computing parity ($\prod_{j=1}^d x_j$ for $x_j \in \{-1,+1\}$) has size at least
				 $\Omega(d 2^{d^{1/(2q)}/2})$ (Andrew Yao, 1985).
				This last result says that some functions (e.g., parity)
				 are difficult to compute depth $q$ circuits
	Generalization bounds \currlecture
		The excess risk $L(\hat h) - L(h^*)$ captures the estimation error (the generalization ability)
		 of an estimator $\hat h$.
		 Recall that the excess risk is controlled by the Rademacher complexity \refeqn{rademacherComplexity}
		 of the function class,
		 so it suffices to study the Rademacher complexity.
		 We will perform the analysis for two-layer neural networks.
		\theoremHeading{rademacherNN}{Rademacher complexity of neural networks}
			Let $x \in \R^d$ be the input vector with $\|x\|_2 \le C_2$.
			Let $w_j \in \R^d$ be the weights connecting to the $j$-th hidden unit, $j = 1, \dots, m$
			Let $\alpha \in \R^m$ be the weights connecting the hidden units to the output
			Let $h : \R \to \R$ be a non-linear activation function with Lipschitz constant 1 such that $h(0) = 0$; examples include
				Hyperbolic tangent: $h(z) = \tanh(z)$
				Rectified linear: $h(z) = \max \{ 0, z \}$
			For each set of weights $(w,\alpha)$, define the predictor:
			 \eqn{f_{w,\alpha}(x) = \sum_{j=1}^m v_j h(w_j \cdot x).}
			Let $\sF$ be the class of prediction functions where the weights are bounded:
			 \eqn{\sF = \{ f_{w,\alpha} : \|\alpha\|_2 \le B_2', \|w_j\|_2 \le B_2 \text{ for } j = 1, \dots, m \}.}
			Then
			 \eqn{R_n(\sF) \le \frac{2 B_2B_2'C_2\sqrt{m}}{\sqrt{n}}.}
		Proof of \refthm{rademacherNN}:
			The key is that the composition properties of Rademacher complexity aligns very nicely
			 with the layer-by-layer compositionality of neural networks.
			The function mapping the input layer to a hidden unit is just a linear function:
			 \eqn{R_n(\{ x \mapsto w \cdot x : \|w\|_2 \le B_2 \}) \le \frac{B_2 C_2}{\sqrt{n}}.}
			Sending each of these through the $1$-Lipschitz non-linearity does not change the upper bound on the complexity:
			 \eqnl{rademacherNNsingle}{R_n(\{ x \mapsto h(w \cdot x) : \|w\|_2 \le B_2 \}) \le \frac{B_2 C_2}{\sqrt{n}}.}
			 For convenience, define $\sG$ as the set of vector-valued functions mapping the input to the hidden activations,
			 as $w$ ranges over different values:
			 \eqn{g(x) = [h(w_1 \cdot x), \dots, h(w_m \cdot x)].}
			Now let us handle the second layer:
			 \eqn{R_n(\sF) \le \E\pb{\sup_{\|\alpha\|_2 \le B_2', g \in \sG} \frac1n \sum_{i=1}^n \sigma_i (\alpha \cdot g(Z_i)).}}
			Applying Cauchy-Schwartz:
			 \eqn{R_n(\sF) \le B_2' \E\pb{\sup_{g \in \sG} \left\|\frac1n \sum_{i=1}^n \sigma_i g(Z_i)\right\|_2}.}
			Using the fact that if $a \in \R^m$ then $\|a\|_2 \le \sqrt{m} \max_{1 \le j \le m} |a_j|$:
			 \eqnl{rademacherNNmaxsup}{R_n(\sF) \le B_2' \sqrt{m} \, \E\pb{\max_{1 \le j \le m} \sup_{\|w_j\|_2 \le B_2} \left|\frac1n \sum_{i=1}^n \sigma_i h(w_j \cdot Z_i) \right|}.}
			This is the same as taking the sup over a single generic $w$ subject to $\|w\|_2 \le B_2$:
			 \eqnl{rademacherNNmaxsup2}{R_n(\sF) \le B_2' \sqrt{m} \, \E\pb{\sup_{\|w\|_2 \le B_2} \left|\frac1n \sum_{i=1}^n \sigma_i h(w \cdot Z_i) \right|}.}
			 Note that the expectation on the RHS is almost the Rademacher complexity of a single unit \refeqn{rademacherNNsingle},
			 but with absolute values.
			 In some parts of the literature,
			 Rademacher complexity is actually defined with absolute values (including the original Bartlett/Mendelson (2002) paper),
			 but the absolute value version is a bit harder to work with in general.
			 For that definition, the Lipschitz composition property still holds with an additional factor of $2$
			 but requires that $h(0) = 0$ (see point 4 of Theorem 12 of Bartlett/Mendelson (2002)).
			 So therefore we can adapt \refeqn{rademacherNNsingle} and plug it into \refeqn{rademacherNNmaxsup2} to obtain:
			 \eqn{R_n(\sF) \le B_2' \sqrt{m} \p{\frac{2 B_2 C_2}{\sqrt{n}}}.}
		Interpretation: the bottom line of this theorem is that neural networks
		 with non-linearities which are smooth (Lipschitz) will generalize
		 regardless of convexity.
	!comment Optimization
		In general, non-convex optimization problems are hard.
		Given a large enough number hidden units, we only have global optima.
		 $\ell(\alpha \cdot \sigma(W x))$
		For a fixed $W$, can solve $\alpha$ exactly
	Approximation error for polynomials \currlecture
		There are two problems with the previous analyses:
			First, we defined approximation error with respect to the set of all bounded continuous functions,
			 which is clearly much too rich of a function class,
			 and as a result we get exponential dependence on the dimension $d$.
			Second, we have not yet said anything about optimization error.
		We will now present part of an ICML paper (Andoni/Panigrahy/Valiant/Zhang),
		 which makes some progress on both points.
		 We will focus on using neural networks to approximate bounded-degree polynomials,
		 which is a reasonable class of functions which itself approximates Lipschitz functions.
		 Further, we will show that choosing weights $W$ \emph{randomly} and optimizing $\alpha$
		 (which is convex) is sufficient.
		Setup
			Monomial $x^J = x^{J_1} \cdots x^{J_d}$ with degrees $J = (J_1, \dots, J_d)$
				Example: $J = (1, 0, 7)$, $x^J = x_1 x_3^7$
			A polynomial is $f(x) = \sum_J b_J x^J$
				Example: $b_{(1, 0, 7)} = 3$ and $b_{(0, 1, 0)} = -5$, $f(x) = 3 x_1 x_3^7 - 5 x_2$
			Degree of polynomial: $\polydeg(f) = \max_{b_J \neq 0} |J|$, $|J| \eqdef \sum_{i=1}^d J_i$
		Inner product structure
			Let $p^*(x)$ is the uniform distribution over $\bbC(R)^d$,
			 where $\bbC(R)$ is set of complex numbers $c$ with norm at most $R$
			 ($|c| = \sqrt{c \overline{c}} \le R$).
			Define the inner product over functions and the associated norm:
			 \eqn{\inner{f}{g}_{p^*} &\eqdef \E_{x \sim p^*}[f(x) \overline{g(x)}] \\
			 \|f\|_{p^*} &\eqdef \sqrt{\inner{f}{f}_{p^*}}.
			 }
		Neural network as an infinite polynomial
			Let $\sigma(z) = \sum_{j \ge 0} a_j z^j$
				Example: for $\sigma(z) = e^z$, $a_j = \inv{j!}$
			For $w \in \bbC^d$, we define the basis function,
			 which can be written as a weighted combination of monomials $x^J$:
			 \eqnl{phiw}{\phi^w(x) \eqdef \sigma(w \cdot x) = \sum_{j \ge 0} a_j \p{\sum_{k=1}^d w_k x_k}^j \eqdef \sum_J a_J w^J x^J.}
			 Here, $w^J = \prod_{k=1}^d w^{J_k}$ is the product of the base weights.
			The neural network is
			 \eqn{f(x) = \sum_{i=1}^m \alpha_i \phi^{w_i}(x).}
			Note that weights $\{w^J\}$ are orthogonal in the following sense:
			 \eqnl{nnwortho}{\E_{w \sim \bbC(R)^d}[w^J \overline{w^{J'}}] = \begin{cases} R^{2|J|} & \text{if $J = J'$} \\ 0 & \text{otherwise}. \end{cases}}
			 This stems from the fact that
			 each $w_j \sim \bbC(R)$ means $w_j = e^{it}$ where $t \sim \text{Uniform}([0, 2\pi])$;
			 and $\E_{t \sim \text{Uniform}([0, 2\pi])}[e^{iat} \overline{e^{ibt}}] = \1[a = b]$.
		\theoremHeading{nnPolyApprox}{neural networks approximate bounded-degree polynomials}
			Choose $w_1, \dots, w_m$ i.i.d.~from $\bbC(1/\sqrt{d})^d$.
			For any polynomial $f^*$ of degree $q$ and norm $\|f^*\| = 1$,
			 exists $\alpha_{1:m}$ with $\|\alpha_{1:m}\|_2 = \sum_{i=1}^m |\alpha_i|^2 = O(d^{2q}/m)$ such that
			 \eqn{\left\|\sum_{i=1}^m \alpha_i \phi^{w_i} - f^*\right\|_{p^*} = O_p\p{\sqrt{\frac{d^{2q}}{m}}}.}
		Proof of \refthm{nnPolyApprox}:
			Let the target function be:
			 \eqn{f^*(x) = \sum_J b_J x^J.}
			Construct an unbiased estimate of $f^*$:
				For any $x \in \bbC^d$ and $J \in \bbN^d$,
				 \eqn{\E_{w \sim \bbC(R)^d}[\phi^w(x) \overline{w^J}] = a_J R^{2|J|} x^J,}
				 which follows from applying \refeqn{nnwortho} to the expansion \refeqn{phiw}
				 and noting that all random coefficients except $w^J$ drop out.
				Define the coefficients
				 \eqn{T(w) = \sum_J \frac{b_J \overline{w^J}}{a_J R^{2|J|}}.}
				Then by construction, we have an unbiased estimate of the target polynomial:
				 \eqn{\E_{w \sim \bbC(R)^d}[T(w) \phi^w(x)] = f^*(x).}
			Now we would like to control the variance of the estimator.
				First, define the error:
				 \eqn{\eta(x) \eqdef \inv{m} \sum_{i=1}^m T(w_i) \phi^{w_i}(x) - f^*(x).}
				We can show that the variance falls as $1/m$ due to independence of $w_1, \dots, w_m$:
				 \eqn{\E_{w_1, \dots, w_m \sim \bbC(R)^d}[\|\eta\|_{p^*}^2] \le \frac{1}{m} \E_{w \sim \bbC(R)^d}[|T(w)|^2 \|\phi^w\|_{p^*}^2].}
				To bound the coefficients:
					Let $a(q) = \min_{|J| \le q} |a_J|$ (property of the non-linear activation function)
					Let $\|f^*\|_1 = \sum_J |b_J|$ (property of the target function)
					Let $q = \polydeg(f^*)$.
					Assume $R \le 1$.
					Then
					 \eqn{|T(w)|
					 &\le \sum_J \left|\frac{b_J \overline{w^J}}{a_J R^{2|J|}}\right| \\
					 &\le \sum_J \left|\frac{b_J}{a_J R^{|J|}}\right| \aside{since $|\overline{w^J}| \le R^{|J|}$} \\
					 &\le \frac{\sum_J |b_J|}{a(q) R^q} \aside{since $|J| \le q, a(q) \le |a_J|$} \\
					 &= \frac{\|f^*\|_1}{a(q) R^q}.
					 }
				Define the following bounds on the variance of the basis functions $\phi^w$
				 and the 1-norm of the target function $f^*$, respectively:
				 \eqn{
				 \beta(q, R) &\eqdef \E_{w \sim \bbC(R)^d}\pb{\frac{\|\phi^w\|_{p^*}^2}{R^{2q}}}, \\
				 \gamma(q) &\eqdef \max_{\|f^*\|_{p^*} = 1} \|f^*\|_1.
				 }
				 Then we have the following result (by plugging quantities in and applying concavity of $\sqrt{\cdot}$):
				 \eqn{\E_{w_1, \dots, w_m \sim \bbC(R)^d}[\|\eta\|_{p^*}] \le \sqrt{\frac{\gamma(q)^2 \beta(q, R)}{a(q)^2 m}}.}
			Finally, we specialize to $\sigma(z) = e^z$ and $p^* = \bbC(1)^d$.
				Claim: $a(q) \ge 1/q!$
					We have $a_j = 1/j!$ and $a_J$ is just $a_j$ times a positive integer stemming from the multiplicities in $J$
					 (for example, if $J = (2, 0, 3)$, then $a_J = 2! 3! a_j \ge a_j$.
				Claim: $\beta(q, R) = O(d^q)$ if $R = O(1/\sqrt{d})$.
					We have that
					 \eqn{\E_{w \sim \bbC(R)^d, x \sim \bbC(1)^d}[\phi^w(x) \overline{\phi^w(x)}]
					 &= \sum_J a_J^2 R^{2|J|} \\
					 &= O(e^{2 \sqrt{d} R}).
					 }
					Finally, $\beta(q, R) = O(e^{2\sqrt{d} R} / R^{2q}) = O(d^q)$ by definition of $R$.
					 Note that we need $R$ to be small to fight the explosion stemming from $e^{2 \sqrt{d}}$.
				Claim: $\gamma(q) = O(\sqrt{d^q})$% because $\|f^*\|_{p^*} = \sum_J |b_J|^2$
				Claim: the coefficients are bounded:
				 \eqn{\sum_{i=1}^m |\alpha_i|^2 = \frac{1}{m^2} \sum_{i=1}^m |T(w_i)|^2 \le \frac{\|f^*\|_1^2}{m a(q)^2 R^{2q}} = O(d^{2q}/m).}
			 See the paper for more details.
	References
		Telgarsky, 2012: Representation Power of Feedforward Neural Networks (slides)
			http://cseweb.ucsd.edu/~dasgupta/254-deep/matus.pdf
		Andoni/Panigrahy/Valiant/Zhang, 2014: Learning Polynomials with Neural Networks
			http://theory.stanford.edu/~valiant/papers/andoni14.pdf
		Livni/Shalev-Shwartz/Shamir, 2014: On the Computational Efficiency of Training Neural Networks
			http://papers.nips.cc/paper/5267-on-the-computational-efficiency-of-training-neural-networks.pdf
		Livni/Shalev-Shwartz/Shamir, 2014: An Algorithm for Training Polynomial Networks
			http://arxiv.org/pdf/1304.7045v2.pdf
		Schmidhuber, 2014: Deep Learning in Neural Networks: An Overview
			http://arxiv.org/pdf/1404.7828v4.pdf

!verbatim \newpage
!verbatim \lecture{18}
Conclusions and outlook
	Review \currlecture
		The goal of this course is to provide a theoretical understanding
		 of why machine learning algorithms work.
		 To undertake this endeavor, we have developed many powerful tools
		 (e.g., convex optimization, uniform convergence)
		 and applied them to the classic machine learning setting:
		 learning a predictor given a training set and applying to unseen test examples.
		To obtain more clarity, it is useful to decompose the error into
		 approximation, estimation, and optimization error:
		 \Fig{figures.slides/errorDecompOpt}{0.4}{errorDecompOpt2}{Cartoon showing error decomposition into approximation, estimation, and optimization errors.}
			Approximation error has to do with how much potential the hypothesis class has
			 (e.g., Gaussian kernel versus linear kernel, large norm versus smaller norm)
			Estimation error has to do with how well you're able to learn,
			 which depends on the complexity of the hypothesis and the amount of data you have.
			Optimization error is how well your algorithm is able to approximate the perfect optimizer
			 (empirical risk minimizer).
		 Sometimes approximation and optimization error are hard to distinguish.
		 For example, in kernel methods, random Fourier features can be viewed
		 as defining a smaller hypothesis space or doing an approximate job optimizing
		 the original kernel-based objective.
		 An algorithm implicity defines a class of hypotheses which are accessible by the algorithm.
		In the batch setting, we wish to study the generalization ability of learning algorithms.
		 The key quantity was excess risk
		 \eqn{L(\hat h) - L(h^*).}
			We could use uniform convergence, which studies
			 \eqn{\sup_{h \in \sH} |L(h) - \hat L(h)|.}
				FIGURE: [empirical and expected risk, convergence]
				Key ideas:
					Concentration: moment generating function of sub-Gaussian random variables
					Measures of complexity of hypothesis class:
					 Rademacher complexity, covering numbers, VC dimension
			We could also use asymptotic statistics to get
			 \eqn{
			 L(\hat\theta) &= L(\theta^*) + (\hat\theta - \theta^*)^\top \nabla L^2(\theta^*) (\hat\theta - \theta^*) + \cdots \\
			 \hat\theta - \theta^* &= -\nabla^2 \hat L(\theta^*) \nabla \hat L(\theta^*) + \cdots,
			 }
			 which vastly simplifies the calculations needed.
		In online learning, nature adversarily chooses convex loss functions $f_1, \dots, f_T$.
		 The online mirror descent learner produces
		 \eqn{w_t = \arg\min_{w \in S} \left\{ \psi(w) + \sum_{i=1}^{t-1} w \cdot z_i \right\},}
		 We analyze the regret:
		 \eqn{\Regret(u) = \sum_{t=1}^T [f_t(w_t) - f_t(u)].}
		 The decomposition into approximation, estimation, and optimization errors
		 is not perfect here.
			Key ideas:
				Convexity allows us to linearize losses to produce an upper bound on regret
				Strongly convex regularizer: tradeoff stability with fitting the data
				$\psi$ allows us to adapt to the geometry of the problem and arrive
				 at algorithsm such as EG
		Results
			Our results for estimation error depend on both the hypothesis class $\sH$ and the number of examples.
			 The exact dependence depends on the structure of the problem, which we summarize here
			 (there are analogues both in the online and the batch settings):
				Classic parametric rate: $1/\sqrt{n}$
				If we have realizability or strong convexity: $1/n$
				Default dependence on dimension: $d$
				If we assume sparsity ($k$ nonzeros): $k \log d$
				In infinite dimensions (RKHS), we depend on the norm of the weight vector: $\|w\|_2$
		But machine learning isn't all about prediction where training and test
		 examples are drawn i.i.d.~from the same distribution.
		 In the following, we will point out a few other directions.
	Changes at test time \currlecture
		Suppose we train a predictor, and deploy it in real-life.
		 Theory says that as long as the test distribution doesn't deviate from training,
		 we have guarantees on the predictor's behavior.
		 But we do not live in a stationary world.
		Example: Google Flu Trends
			Trained on 2003--2008 data, predict flu based on (50M) common queries, got 97\% accuracy.
			In 2009, vastly underestimated (due to swine flu, people searched differently).
			In 2011--2013, overpredicted.
		Online learning doesn't solve this problem
			One might think that online learning doesn't place distribution over the data, so it's robust.
			But the analyses are with respect to an expert which is static,
			 and if the world is changing under you, being static is a pretty lame thing to do.
			 So the bounds do hold, but the statements are quite weak.
		Covariate shift
			Training distribution: $x \sim p^*$
			Test distribution: $x \sim q^*$
				Example: object recognition in different lighting conditions / camera
			Assume $p^*(y \mid x)$ is fixed across both training and test
			Assume lots of unlabeled examples drawn from both $p^*$ and $q^*$
			 which can be used to estimate the marginal distributions.
			Instance weighting (simplest approach)
				Idea: upweight examples that are underrepresented at training time.
				Estimator:
				 \eqn{\hat\theta = \arg\min_{\theta} \hat L(\theta), \quad \hat L(\theta) = \inv{n} \sum_{i=1}^n \red{\hat w(x)} \ell((x,y); \theta).}
				If we had infinite unlabeled data,
				 we could use the weights $\hat w(x) = p^*(x)/q^*(x)$,
				 from which it's easy to check that $\hat\theta$ is an unbiased estimator
				 of the expected risk at test time.
				There are two problems with this:
					First, in practice, we don't have infinite unlabeled data,
					 so we would need to estimate $\hat w(x)$.
					Second, we need to assume that $q^*$ is absolutely continuous with respect to $p^*$
					 (must see all test examples at training time).
					 Otherwise, we have no hope of doing well.
				There are several procedures that address these problems
				 as well as analyses.  For example, one can use asymptotic statistics:
					Shimodaira: Improving predictive inference under covariate shift by weighting the log-likelihood function
						http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.370.4921&rep=rep1&type=pdf
		Domain adaptation / multi-task learning
			In domain adaptation, even $p^*(y \mid x)$ might be different between train and test.
				Example: my email and your email
			Techniques
				The general idea is to solve joint ERM problem where assume that weight vectors are close.
				 Let $W = [w_1, \dots, w_T] \in \R^{d \times T}$ be the matrix of weights for $T$ tasks.
				We can think about performing joint learning where we regularize $W$ using one of the following:
					Assume weight vectors are similar in Euclidean distance: $\sum_{i \neq j} \|w_i - w_j\|^2$
					Assume weight vectors lie in the same low-dimensional subspace: use trace norm $\|W\|_*$
					Assume there exists a sparse set of features that is shared by all tasks: use block norm $\|W\|_{2,1}$,
					 which is the sum of the L2 norms of the rows.
				Neural networks provide a natural and compelling solution:
				 just have all the tasks share the same hidden layer.
			As far as theoretical analyses,
			 the key intuition is that the regularizer reduces the effective hypothesis space
			 from $T$ independent weight vectors to $T$ highly-related weight vectors.
				Maurer, 2006: Bounds for Linear Multi-Task Learning
					http://www.jmlr.org/papers/volume7/maurer06a/maurer06a.pdf
		Example: deep neural networks are non-robust
			Szegedy/Zaremba/Sutskever/Bruna/Erhan/Goodfellow/Fergus, 2014: Intriguing properties of neural networks
				http://arxiv.org/pdf/1312.6199v2.pdf
			This paper shows that one can take a high-accuracy neural network
			 for object recognition, perturb the input by a very small adversarial amount to make the predictor incorrect.
			 The perturbation is inperceptible to the naked eye
			 and is obtained via an optimization problem like this:
			 \eqn{\min_{r \in \R^d} (f(x+r) - y_\text{wrong})^2 + \lambda \|r\|^2,}
			 where $r$ is the perturbation, $f$ is the trained neural network,
			 and $x$ is the input.
			Note that if we had a classifier based on a Gaussian kernel,
			 we couldn't do this, because the Gaussian kernel is smooth.
		Robustness at test time
			At test time, suppose that up to $K$ features can be zeroed out adversarily.
			We can optimize for this using robust optimization.
			 The following paper shows that for classification,
			 the robust optimization version results in a quadratic program.
			Globerson/Roweis, 2006: Nightmare at Test Time: Robust Learning by Feature Deletion
				https://www.cs.nyu.edu/~roweis/papers/robust_icml06.pdf
	Alternative forms of supervision \currlecture
		Active learning (learner chooses examples non i.i.d.)
			Example: learning binary classifiers in 1D
				Suppose data is generated according to $y = \1[x \ge \theta]$ for some unknown $\theta$.
				If we sample i.i.d. from some distribution over $x$,
				 our expected risk falls as $O(1/n)$.
				However, if we actively choose points (using binary search),
				 our expected risk falls as $O(e^{-cn})$, which is exponentially faster.
			Intuition: query examples that we are most uncertain about.
			 In reality, we have noise, and sampling random i.i.d.~is not terrible in the beginning.
			Dasgupta, 2006: Coarse sample complexity bounds for active learning
				http://papers.nips.cc/paper/2943-coarse-sample-complexity-bounds-for-active-learning.pdf
			Bach, 2007: Active learning for misspecified generalized linear models
				http://papers.nips.cc/paper/2961-active-learning-for-misspecified-generalized-linear-models.pdf
		Semi-supervised learning
			Motivation: labeled data is expensive, unlabeled data is cheap
			But how does knowing $p^*(x)$ (from unlabeled data)
			 help you with $p^*(y \mid x)$ (prediction problem).
			 There is no free lunch, so we need to make some assumptions
			 about the relationship between $p^*(x)$ and $p^*(y \mid x)$.
			 For example, in classification,
			 we can assume that the decision boundary defined by $[p^*(y \mid x) \ge \half]$
			 doesn't cut through high-density regions of $p^*$.
			Theory
				$p^*(x)$ + assumptions about relationship
				 defines a compatibility function $\chi(h)$
				Reduced hypothesis class $\sH$ to ones which are compatible $\{ h \in \sH : \chi(h) = 1 \}$
				Unlabeled data helps because we're reducing the hypothesis class
			Balcan/Blum, 2009: A Discriminative Model for Semi-Supervised Learning
				http://www.cs.cmu.edu/~ninamf/papers/ssl-2009.pdf
		!comment Reinforcement learning
			Saw multi-armed bandits; but RL is more intense
			Example: building a robot to move across the room
				State: position of joints
				Actions: what joints to move
				Reward: did the robot move to the other side of the room (or progress towards)
			Challenges
				Partial feedback
				Delayed rewards
			In practice, works when have sufficient immediate rewards
			 and state space is well connected.
			Theory
				Intuition: if state-space is connected, can just explore it,
				 and reduce it to multi-armed bandit
				http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf
				http://arxiv.org/pdf/1210.4843v1.pdf
	!comment Parameter estimation
		Mostly cared about prediction in engineering
		Science: care about understanding the data,
		 estimating the parameters of the model $\log p(x \mid \theta)$
		Exponential families: easy (example: Gaussian), convex optimzation
		Clustering: Gaussian mixture models
		Latent-variable models
			Important: more compact representation of data, causal explanation
			 (mixture models, graphical models)
		Can use EM algorithm, but gets stuck in local optimum
		Goal: derive consistent estimators
		Learning latent structures
			Markov network structures (Chow Liu)
			Learning non-tree Markov network structures (Pradeep)
	Interaction between computation and statistics \currlecture
		This class is mostly about the statistical properties of learning algorithms,
		 for which there is by now quite solid theory for.  One of the most
		 interesting avenues for future research is how computation plays a role in this story.
		 The situation is rather complex.
		Favorable relationship: large optimization error results in smaller effective hypothesis space
		 and actually controls estimation error.
			Early stopping: relate to L2 regularization
			Low-rank kernel approximations
		Unfavorable relationship: good estimators are hard to compute
			Graphical models (partition function): pseudolikelihood
				Suppose $x \in \{0,1\}^d$.
				$p_\theta(x) = W_\theta(x)/Z(\theta)$
				The partition function $Z(\theta) = \sum_{x} W_\theta(x)$ is computationally expensive,
				 taking in the worst case $O(2^d)$ time.
				However, we can maximize the pseudolikelihood
				 $\prod_{i=1}^d p_\theta(x_i \mid x_{-i})$,
				 which takes $O(d)$ time.
				Pseudolikelihood is computationally more efficient,
				 but statistically less efficient.
			Learning latent variable models (e.g., Gaussian mixture models):
				Maximum likelihood is non-convex and hard to compute.
				The method of moments estimator (which works under some assumptions)
				 is easy to compute but statistically less efficient.
			Other constraints on learning: communication, memory, privacy
		Neural networks
			Here, the story is a bit more complex.
			!comment Recurrent neural networks
				Suppose we want to deal with time series
				Maintain a hidden vector $h_t \in R^k$ that summarizes the history
				$h_t = \alpha \cdot \sigma(W [x_t, h_{t-1}])$
				Must be implicitly some locality that enables us to learn
			On one hand, neural networks are difficult to train.
			 People often talk about the vanishing gradient problem in the context of gradient-based optimization,
			 where we are nowhere near a good solution, and yet the gradient is near zero.
			 This happens when the weights $w_j$ are too large, which saturates the sigmoid.
			 To avoid this, careful initialization and step size selection are important.
			On the other hand,
			 if we fully optimized existing neural network models,
			 there is some chance they would just overfit,
			 and the fact that we're not fully optimizing is actually central to their ability to generalize.
			!comment How can we understand the effective hypothesis class?
				Algorithmic stability + effective complexity of hypothesis class?

!verbatim \newpage
!verbatim \appendix
Appendix
	Notation \label{sec:notation}
		!tmpformat N
		In general, we will not be religious about using uppercase letters to denote random variables or bold letters to denote vectors or matrices.
		 The type of the variable should hopefully be clear from context.
		Basic definitions
			$[n] = \{ 1, \dots, n \}$
			For a sequence $v_1, \dots, v_n$:
				Let $v_{i:j} = (v_i, v_{i+1}, \dots, v_{j-1}, v_j)$
				 be the subsequence from $i$ to $j$ inclusive.
				Let $v_{<i} = v_{1:i-1}$.
			$\nabla f$: gradient of a differentiable function $f$
			$\partial f(v)$: set of subgradients of a convex function $f$
			Indicator (one-zero) function:
			 \eqn{\1[\text{condition}] \eqdef \begin{cases} 1 & \text{if condition is true} \\ 0 & \text{otherwise}. \end{cases}}
			Probability simplex:
			 \eqn{\Delta_d \eqdef \left\{ w \in \R^d : w \succeq 0 \text{ and } \sum_{i=1}^d w_i = 1 \right\}.}
			Euclidean projection:
			 \eqn{\Pi_S(w) \eqdef \arg\min_{u \in S} \|u - w\|_2}
			 is the closest point (measured using Euclidean distance) to $w$ that's in $S$.
		We will try to stick with the following conventions:
			$x$: input
			$y$: output
			$z$: input-output pair
			$d$: dimensionality
			$n$: number of examples
			$t$: iteration number
			$T$: total number of iterations
			$f$: (convex) function
			$w$: weight vector
			$\theta$: parameters
			$L$: Lipschitz constant
			$\lambda$: amount of regularization
			$\eta$: step size
			$p^*(x,y)$: true distribution of data
			In general:
				$v^*$ denotes the optimal value of some variable $v$.
				$\hat v$ denotes an empirical estimate of some variable $v$
				 based on training data.
	Linear algebra
		Inner (dot) product: given two vectors $u, v \in \R^n$, the dot product is $u^\top v = u \cdot v = \inner{u}{v} = \sum_{i=1}^n u_i v_i$.
		Outer product: for a vector $v \in \R^n$, let $v^\otimes = v v^\top$
		Positive semidefinite (PSD) matrix:
		 a symmetric square matrix $A \in \R^{n \times n}$ is PSD iff
		 $v^\top A v \ge 0$ for all vectors $v \in \R^n$.
		 Note: in this class, whenever we say a matrix is PSD, we implicitly assume that the matrix is symmetric.
		Eigenvalues: for a PSD matrix $A \in \R^{n \times n}$,
		 let $\lambda_1(A), \dots, \lambda_n(A)$ be the eigenvalues of $A$, sorted in non-increasing order.
		$\tr(A)$: for a square matrix $A \in \R^{n \times n}$, $\tr(A)$ denotes the trace of $A$,
		 the sum of the diagonal entries ($\tr(A) = \sum_{i=1}^n A_{ii}$).
			$\tr(ABC) = \tr(BCA)$, but $\tr(ABC) \neq \tr(BAC)$ in general
			$\tr(A) = \sum_{i=1}^n \lambda_i(A)$
		$\diag(v)$: for a vector $v \in \R^d$, a matrix whose diagonal entries are the components of $v$ and off-diagonal entries are zero.
		Hölder's inequality
			For any real vectors $u, v \in \R^d$,
			 \eqn{|u \cdot v| \le \|u\|_p \|v\|_q,}
			 for $1/p + 1/q = 1$,
			 where $\|u\|_p = (\sum_{j=1}^d |u_j|^p)^{1/p}$ is the $p$-norm.
			For $p = q = 2$, this is the Cauchy-Schwartz inequality.
			Another important case is $p = 1$ and $q = \infty$.
			Note that this result holds in greater generality (for $L_p$ spaces).
	Probability
		In general, we define a background space over which all the random variables are defined
		 and use $\BP[\cdot]$ and $\E[\cdot]$ to reference that space.
		Conditional expectation: If we have two random variables $X$ and $Y$,
		 we will write $\E[F(X, Y) \mid Y]$ to denote the random variable (that is a function of $Y$),
		 rather than $\E_X[F(X, Y)]$ to denote that the expectation is taken over $X$.
		 In general, we condition on the variables that we're not taking an expectation over
		 rather than using subscripts to denote the variables that we are taking an expectation over.
		In the following, assume the following:
			Let $X_1, \dots, X_n$ be real vectors drawn i.i.d. from some distribution with mean $\mu$
			 and covariance matrix $\Sigma$.
			Let $\hat\mu = \inv{n} \sum_{i=1}^n X_i$.
		Convergence in probability
			We say that a sequence of random variables $(Y_n)$ converges in probability to a random variable $Y$
			 (written $Y_n \cvP Y$)
			 if for all $\epsilon > 0$, we have $\lim_{n \to \infty} \BP[|Y_n - Y| \ge \epsilon] = 0$.
			Example: $\hat\mu \cvP \mu$ (weak law of large numbers)
		Convergence in distribution
			We say that a sequence of distributions $(P_n)$ converges in distribution (weak convergence) to a
			 distribution $P$ (written $P_n \cvd P$)
			 if for all bounded continuous functions $f$,
			 $\lim_{n \to \infty} \int f(x) P_n(dx) = \int f(x) P(dx)$.
			When we write $Y_n \cvd P$ for a sequence of random variables $(Y_n)$,
			 we mean that the sequence of distribution of those random variables converges in distribution.
			Example: $\sqrt{n} (\hat\mu - \mu) \cvd \sN(0, \Sigma)$ (central limit theorem)
		Continuous mapping theorem
			This theorem allows us to transfer convergence in probability and convergence in distribution
			 results through continuous transformations.
			If $f : \R^d \to \R^k$ is continuous and $Y_n \cvP Y$, then $f(Y_n) \cvP f(Y)$.
			If $f : \R^d \to \R^k$ is continuous and $Y_n \cvd Y$, then $f(Y_n) \cvd f(Y)$.
			Example: $\|\hat\mu\|_2 \cvP \|\mu\|_2$
		Delta method
			This theorem allows us to transfer asymptotic normality results through smooth transformations.
			If $\nabla f : \R^d \to (\R^d \times \R^k)$ is continuous and $\sqrt{n}(\hat\mu - \mu) \cvd \sN(0, \Sigma)$, then
			 \eqn{\sqrt{n}(f(\hat\mu) - f(\mu)) \cvd \sN(0, \nabla f(\mu)^\top \Sigma \nabla f(\mu)).}
			The key intuition here is a Taylor approximation (valid because $\hat\mu$ is converging to $\mu$):
			 \eqn{f(\hat\mu) = f(\mu) + \nabla f(\mu)^\top (\hat\mu - \mu) + \cdots.}
			 Then apply the continuous mapping theorem.
		Slutsky's theorem
			This result allows us to compose convergence results.
			If $Y_n \cvP c$ and $Z_n \cvd Z$ then $Y_n Z_n \cvd c Z$.
			Example: $\sqrt{n} \hat\mu \cdot (\hat\mu - \mu) \cvd \sN(0, \mu^\top \Sigma \mu)$.
		Notation: $X_n = O_p(f(n))$ if $X_n/f(n)$ is bounded in probability, that is, for every $\epsilon > 0$,
		 there exists $M_\epsilon$ such that for all $n$, $\BP[|X_n/f(n)| > M_\epsilon] < \epsilon$.
			Example: $\sN(0, \Sigma) = O_p(1)$
			Example: $\hat\mu - \mu = O_p\p{\inv{\sqrt{n}}}$
		These results turn complicated things on the LHS into simple things on the RHS.
		 Use your intuitions from real analysis.
		Jensen's inequality
			For any convex function $f : \R^d \to \R$ and random variable $X \in \R^d$,
			 \eqn{f(\E[X]) \le \E[f(X)].}
	Functional analysis
		\definitionHeading{cauchy}{Cauchy sequence}
		 A Cauchy sequence in a metric space $(X, \rho)$ is $(x_i)_{i \ge 1}$ such that 
		 for any $\epsilon > 0$, there exists an integer $n$ such that all $i,j>n$, $\rho(x_i, x_j) < \epsilon$.
		\definitionHeading{complete}{complete metric space}
		 A complete metric space $(X,\rho)$ is one where every Cauchy sequence $(x_i)_{i \ge 1}$ converges to a limit point $x^* \in X$.
		 Intuition: if the elements of the sequence are getting arbitrarily close to each other,
		 they are getting close to some particular point in the space.
		$L^p(\sX)$ is the space of all measurable functions $f$ with finite $p$-norm:
		 \eqn{\|f\|_p \eqdef \p{\int |f(x)|^p dx}^{1/p} < \infty.}
		Example: if $\sX = \R$, $f(x) = 1$ is not in $L^p$ for any $p < \infty$.
		$L^2(\sX)$ is the set of all square integrable functions.
		$L^\infty(\sX)$ is the set of all bounded functions.
!verbatim \newpage
!verbatim \bibliographystyle{apalike}
!verbatim \bibliography{refdb/all}
